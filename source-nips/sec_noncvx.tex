% !TEX ROOT=./main.tex



\section{Non-convex convergence rate}

{\centering
\begin{tabular}{l} 
Algorithm 1 MKL-SGD \\
\hline 1: Initialize $\boldsymbol{w}_{\mathbf{0}}$ \\
2: Given samples $D=\left(\boldsymbol{x}_{\boldsymbol{t}}, y_{t}\right)_{t=1}^{\infty}$ \\
3: for $t=1, \ldots$ do \\
4: Choose a set $S_{t}$ of $k$ samples \\
5: Select $i_{t}=\arg \min _{i \in S_{t}} f_{i}\left(\boldsymbol{w}_{t}\right)$ \\
6: Update $\boldsymbol{w}_{t+1}=\boldsymbol{w}_{t}-\eta_t \nabla f_{i_{t}}\left(\boldsymbol{w}_{t}\right)$ \\
7: end for \\
8: Return $\boldsymbol{w}_{t}$ \\
\hline
\end{tabular}}
In the algorithm, we require the step size satisfies
\begin{equation}
\sum_{t=1}^{\infty} \eta_t = \infty \text{ and } \sum_{t=1}^{\infty}\eta_t^2 < \infty
\label{eq:stepsize}
\end{equation}


\subsection{Problem Setup and Notation}
In this subsection, we analyze the convergence rate of MKL-SGD for Non-convex objective. Formally, 
we study the problem of 
 $$\min _{\boldsymbol{w}} F(\boldsymbol{w}),$$
we denote the objective function on clean dataset as $F(\vw) = \frac{1}{\|G\|} \sum_{i \in G} f_i(\vw)$ and
the objective function on noisy dataset $F_a(\vw) =\frac{1}{N} \sum_{i=1}^N f_i(\vw)$, where the
$N$ data point includes both clean data and abnormal data. 
We use $p_i(\vw_t)$ denote the probability that sample $i$ will be used for computing
gradient using MKL-SGD.

\begin{assumption}[Gradient purity]
The expected gradient of MKL-SGD in noisy dataset move in the same direction as the 
expected gradient in clean dataset.
$\langle \grad F(\vw_t),\sum_{i=1}^N p_i(\vw_t)\grad f_i(\vw_t) - \grad F(\vw_t) \rangle \geq 0$
\label{ass:gp}
\end{assumption}

\begin{remark}
	The Assumption~\ref{ass:gp} is relatively weak in terms of gradient norm as it tolerates the
	where noisy gradient is persistent, while it has strict requirement on the noisy gradient moves
	the object toward the correct direction with sufficient distance.
\end{remark}
In the noiseless case, we have $ \sum_{i=1}^N p_i(\vw_t)\grad f_i(\vw_t) = \grad F(\vw_t)$, the 
assumption is satisfied and the results reduce to regular non-convex objective convergence 
results. 

\begin{assumption}[L-smooth]
	$F(\mathbf{v}) \leq F(\mathbf{w})+\langle \mathbf{v}-\mathbf{w}, \nabla F(\mathbf{w}) \rangle +\frac{L}{2}\|\mathbf{v}-\mathbf{w}\|_{2}^{2}$.
	\label{ass:lsmooth}
\end{assumption}

\begin{assumption}[Bounded gradient norm]
	The expected square of gradient norm is bounded $\EE \|\grad F_a(\vw_t)\|_2^2  \leq G$: 
	\label{ass:bgn}
\end{assumption}





\begin{theorem}
	With assumption~\ref{ass:gp}, \ref{ass:lsmooth}, \ref{ass:bgn}, the MKL-SGD is running with
	step size $\eta_t$ satisfying \eq{\ref{eq:stepsize}}, then, with $A_T =\sum_{t=0}^T\eta_t $, 
	\begin{equation}
		\EE \frac{1}{A_T}\sum_{t=0}^T \eta_t\|\grad F(\vw_t)\|_2^2 \rightarrow 0
	\end{equation}
\end{theorem}
\begin{proof}
$\vw_{t+1} = \vw_t - \eta_t \grad f_i(\vw_t)$
\begin{align*}
	F(\vw_{t+1}) - F(\vw_{t}) & \leq \langle \grad F(\vw_t), \vw_{t+1} - \vw_t \rangle + \frac{L}{2}\|\vw_{t+1} - \vw_t\|_2^2  \\
                              &  = - \eta_t \langle \grad F(\vw_t), \grad f_i(\vw_t)\rangle  + \frac{L}{2}\|\vw_{t+1} - \vw_t\|_2^2  \\
                              & \leq - \eta_t \langle \grad F(\vw_t), \grad f_i(\vw_t)\rangle  + \frac{\eta_t^2 L}{2}\|\grad f_i(\vw_t)\|_2^2 
\end{align*}
Take expectation w.r.t $i$, condition on $\vw_t$, 
\begin{align*}
	\EE[F(\vw_{t+1})] - F(\vw_{t}) & \leq - \eta_t \langle \grad F(\vw_t),\sum_{i=1}^N p_i(\vw_t)\grad f_i(\vw_t) \rangle + \sum_{i=1}^N p_i(\vw_t) \frac{\eta_t^2 L}{2}\|\grad f_i(\vw_t)\|_2^2 \\
	&  \leq - \eta_t \langle \grad F(\vw_t),\sum_{i=1}^N p_i(\vw_t)\grad f_i(\vw_t) - \grad F(\vw_t) \rangle - \eta_t \|\grad F(\vw_t)\|_2^2 +  \frac{\eta_t^2 LG}{2}\\
\end{align*}
Take total expectation: 
\begin{align*}
	\eta_t\EE \|\grad F(\vw_t)\|_2^2 &\leq \EE[F(\vw_{t})] - \EE[F(\vw_{t+1})] + \EE[- \eta_t \langle \grad F(\vw_t),\sum_{i=1}^N p_i(\vw_t)\grad f_i(\vw_t) - \grad F(\vw_t) \rangle] + \frac{\eta_t^2 LG}{2}
\end{align*}
Summing both sides from $t = 0$ to $T$
\begin{align*}
	\sum_{t=0}^T \eta_t\EE \|\grad F(\vw_t)\|_2^2 &\leq \EE[F(\vw_{1})] - F_{\inf} -\sum_{t=0}^T \eta_t \EE[ \langle \grad F(\vw_t),\sum_{i=1}^N p_i(\vw_t)\grad f_i(\vw_t) - \grad F(\vw_t) \rangle] + \sum_{t=0}^T \frac{\eta_t^2 LG}{2} \\
	& \leq \EE[F(\vw_{1})] - F_{\inf} + \sum_{t=0}^T \frac{\eta_t^2 LG}{2} 
\end{align*}
The learning rate satisfies\eq{\ref{eq:stepsize}}, therefore, $\EE \frac{1}{A_T}\sum_{t=0}^T \eta_t \|\grad F(\vw_t)\|_2^2 \rightarrow 0$
	
\end{proof}

