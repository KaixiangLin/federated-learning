% !TEX ROOT=./main.tex

To organize our proofs more effectively and highlight the significance of our results compared to prior works, 
we first state the following key lemmas used in proofs of main results and defer their proofs to later. 
\begin{lemma}[\textbf{One step progress, strongly convex}] Let $\overline{\mathbf{w}}_{t}=\sum_{k=1}^{N}p_{k}\mathbf{w}_{t}^{k}$, and
suppose our functions satisfy Assumptions~\ref{ass:lsmooth},\ref{ass:stroncvx},\ref{ass:boundedvariance},\ref{ass:subgrad2}, and set step size $\alpha_{t}=\frac{4}{\mu(\gamma+t)}$
	with $\gamma=\max\{32\kappa,E\}$ and $\kappa=\frac{L}{\mu}$, then the updates of FedAvg with full participation satisfy
	\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & \leq(1-\mu\alpha_{t})\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}\\
	+& \alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}+6E^{2}L\alpha_{t}^{3}G^{2}.
	\end{align*}
\label{lem:scvxoner}
\end{lemma}
We emphasize that the above lemma is the key step that allows us to obtain a bound that improves on the convergence result of~\cite{li2019convergence} with linear speedup. Its proof will make use of the following two results. 
\begin{lemma}[\textbf{Bounding gradient variance (Lemma 2 \cite{li2019convergence})} ]
Given Assumption~\ref{ass:boundedvariance}, the upper bound of gradient variance is given as follows,
\begin{align*}
	\mathbb{E}\|\vg_{t}-\ov{g}_{t}\|^{2} \leq \sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}.
	\end{align*}
\label{lem:bgv}
\end{lemma}

\begin{lemma}[\textbf{Bounding the divergence of $\vw_t^k$ (Lemma 3 \cite{li2019convergence})} ]
Given Assumption~\ref{ass:subgrad2}, and assume that $\alpha_t$ is non-increasing and $\alpha_t \leq 2\alpha_{t+E}$ for all $t\geq 0$, we have
	\begin{align*}
	\mathbb{E}\left[\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2} \right]\leq 4E^{2}\alpha_{t}^{2}G^{2}.
	\end{align*}
\label{lem:bdw}
\end{lemma}

We now restate Theorem~\ref{th:scvx_sgd} from the main text and then prove it using Lemma~\ref{lem:scvxoner}.
\begin{thm}
	Let $\overline{\mathbf{w}}_{T}=\sum_{k=1}^{N}p_{k}\mathbf{w}_{T}^{k}$ in FedAvg,
	$\nu_{\max}=\max_{k}Np_{k}$, and set decaying learning rates $\alpha_{t}=\frac{4}{\mu(\gamma+t)}$
	with $\gamma=\max\{32\kappa,E\}$ and $\kappa=\frac{L}{\mu}$. Then
	under Assumptions~\ref{ass:lsmooth},\ref{ass:stroncvx},\ref{ass:boundedvariance},\ref{ass:subgrad2} with full device participation, 
	\begin{align*}
	\mathbb{E}F(\overline{\mathbf{w}}_{T})-F^{\ast}=\mathcal{O}\left(\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}}\right)
	\end{align*}
	and with partial device participation with at most $K$ sampled devices
	at each communication round, 
	\begin{align*}
	\mathbb{E}F(\overline{\mathbf{w}}_{T})-F^{\ast}=\mathcal{O}\left(\frac{\kappa E^{2}G^{2}/\mu}{KT}+\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}}\right)
	\end{align*}
\end{thm}

\begin{proof}
	% The proof builds on ideas from~\cite{li2019convergence}. The first step is to observe that the $L$-smoothness of $F$ provides
	% the upper bound
	% \begin{align*}
	% \mathbb{E}(F(\ov{w}_{t}))-F^{\ast} & \leq\frac{L}{2}\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}
	% \end{align*}
	% and bound $\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}$. 
	The road map of the proof for full device participation contains three steps. First, we establish a recursive relationship between $\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2}$
	and $\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}$, upper bounding the progress of FedAvg from step $t$ to step $t+1$. 
	Second, we show that $\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}=\mathcal{O}(\frac{\nu_{max}^{2}\sigma^{2}/\mu}{tN}+\frac{E^{2}LG^{2}/\mu^2}{t^{2}})$ by induction using the recursive relationship from the previous step. 
	Third, we use the property of $L$-smoothness to bound the optimality gap by $\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}$. 
	
	By Lemma~\ref{lem:scvxoner}, we have the following upper bound for the one step progress: 
	\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & \leq(1-\mu\alpha_{t})\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}\\
	& +\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}+6E^{2}L\alpha_{t}^{3}G^{2}.
	\end{align*}
	We show next that $\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}=O(\frac{\nu_{max}^{2}\sigma^{2}/\mu}{tN}+\frac{E^{2}LG^{2}/\mu^2}{t^{2}})$ using induction. 
	To simplify the presentation, we denote $C\equiv6E^{2}LG^{2}$ and $D\equiv\frac{1}{N}\nu_{max}^{2}\sigma^{2}$.
	Suppose that we have the bound $\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}\leq b\cdot(\alpha_{t}D+\alpha_{t}^{2}C)$
	for some constant $b$ and learning rates $\alpha_{t}$. Then the one step progress from Lemma~\ref{lem:scvxoner} becomes:
	\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & \leq (b(1-\mu\alpha_{t})+\alpha_{t})\alpha_{t}D\\
	& +(b(1-\mu\alpha_{t})+\alpha_{t})\alpha_{t}^{2}C
	\end{align*}
	To establish the result at step $t+1$, it remains to choose $\alpha_{t}$ and $b$ such that $(b(1-\mu\alpha_{t})+\alpha_{t})\alpha_{t}\leq b\alpha_{t+1}$
	and $(b(1-\mu\alpha_{t})+\alpha_{t})\alpha_{t}^{2}\leq b\alpha_{t+1}^{2}$.
	%Recall that we require $\alpha_{t_{0}}\leq2\alpha_{t}$ for any $t-t_{0}\leq E-1$, and $L\alpha_{t}\leq\frac{1}{8}$. 
	If we let $\alpha_{t}=\frac{4}{\mu(t+\gamma)}$
	where $\gamma=\max\{E,32\kappa\}$ (choice of $\gamma$ required to guarantee the one step progress) and set $b=\frac{4}{\mu}$, we have:
	\begin{align*}
	(b(1-\mu\alpha_{t})+\alpha_{t})\alpha_{t}  &=\left(b(1-\frac{4}{t+\gamma})+\frac{4}{\mu(t+\gamma)}\right)\frac{4}{\mu(t+\gamma)}\\
	% & =\left(b\frac{t+\gamma-4}{t+\gamma}+\frac{4}{\mu(t+\gamma)}\right)\frac{4}{\mu(t+\gamma)}\\
	% & =b(\frac{t+\gamma-3}{t+\gamma})\frac{4}{\mu(t+\gamma)}\\
	% & \leq b(\frac{t+\gamma-1}{t+\gamma})\frac{4}{\mu(t+\gamma)}\\
	 &\leq b\frac{4}{\mu(t+\gamma+1)}=b\alpha_{t+1}\\
	 (b(1-\mu\alpha_{t})+\alpha_{t})\alpha_{t}^{2} &=b(\frac{t+\gamma-2}{t+\gamma})\frac{16}{\mu^{2}(t+\gamma)^{2}} \\
	 & \leq b\frac{16}{\mu^{2}(t+\gamma+1)^{2}}=b\alpha_{t+1}^{2}
	\end{align*}
	where we have used the following inequalities:
	\begin{align*}
	\frac{t+\gamma-1}{(t+\gamma)^{2}} \leq\frac{1}{(t+\gamma+1)},\hspace{0.2em} 
	\frac{t+\gamma-2}{(t+\gamma)^{3}} \leq\frac{1}{(t+\gamma+1)^{2}}, \hspace{0.2em}  \forall\ \gamma\geq1
	\end{align*}
	Thus we have established the result at step $t+1$ assuming the result is correct at step $t$:
	\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & \leq b\cdot(\alpha_{t+1}D+\alpha_{t+1}^{2}C)
	\end{align*}
	% for our choice of $\alpha_{t}$ and $b$. Now to ensure 
	At step $t=0$, we can ensure the following inequality by scaling $b$ with $c\|\vw_{0}-\vw^{\ast}\|^{2}$ for a sufficiently large constant
	$c$:
	\begin{align*}
	\|\vw_{0}-\vw^{\ast}\|^{2} \leq b\cdot(\alpha_{0}D+\alpha_{0}^{2}C) =b\cdot(\frac{4}{\mu\gamma}D+\frac{16}{\mu^{2}\gamma^{2}}C)
	\end{align*}
	It follows that 
	\begin{align}
	\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2} & \leq c\|\vw_{0}-\vw^{\ast}\|^{2}\frac{4}{\mu}(D\alpha_{t}+C\alpha_{t}^{2})
	\label{eq:th1:1}
	\end{align}
	for all $t\geq0$. 
	
	Finally, the $L$-smoothness of $F$ implies 
	\begin{align*}
	& \mathbb{E}(F(\ov{w}_{T}))-F^{\ast} \leq \frac{L}{2}\mathbb{E}\|\ov{w}_{T}-\vw^{\ast}\|^{2}\\ 
	\leq &\frac{L}{2}c\|\vw_{0}-\vw^{\ast}\|^{2}\frac{4}{\mu}(D\alpha_{T}+C\alpha_{T}^{2})\\
	=& 2c\|\vw_{0}-\vw^{\ast}\|^{2}\kappa(D\alpha_{T}+C\alpha_{T}^{2})\\
	\leq &2c\|\vw_{0}-\vw^{\ast}\|^{2}\kappa\left[\frac{4}{\mu(T+\gamma)}\cdot\frac{1}{N}\nu_{max}^{2}\sigma^{2}+6E^{2}LG^{2}\cdot(\frac{4}{\mu(T+\gamma)})^{2}\right]\\
	=& \mathcal{O}(\frac{\kappa}{\mu}\frac{1}{N}\nu_{max}^{2}\sigma^{2}\cdot\frac{1}{T}+\frac{\kappa^{2}}{\mu}E^{2}G^{2}\cdot\frac{1}{T^{2}})
	\end{align*}
	where in the first line, we use the property of $L$-smooth function (see Lemma~\ref{lem:lsmooth}), and in the second line, we use the conclusion in \eq{\ref{eq:th1:1}}. 
	
	With partial participation, the update at each communication round
	is now given by weighted averages over a subset of sampled devices. When $t+1\notin\mathcal{I}_{E}$,
	$\ov{v}_{t+1}=\ov{w}_{t+1}$, while when $t+1\in\mathcal{I}_{E}$,
	we have $\mathbb{E}\ov{w}_{t+1}=\ov{v}_{t+1}$ by design
	of the sampling schemes~(\cite{li2019convergence}, Lemma 4), so that 
	\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & =\mathbb{E}\|\ov{w}_{t+1}-\ov{v}_{t+1}+\ov{v}_{t+1}-\vw^{\ast}\|^{2}\\
	& =\mathbb{E}\|\ov{w}_{t+1}-\ov{v}_{t+1}\|^{2}+\mathbb{E}\|\ov{v}_{t+1}-\vw^{\ast}\|^{2}
	\end{align*}
	This in particular implies $\mathbb{E}\|\ov{v}_{t}-\vw^{\ast}\|^{2}\leq \mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}$ for all $t$.
	Since $\ov{v}_t =\sum_{k=1}^{N} p_{k} \mathbf{v}_{t}^{k}$ always averages over all devices, the full participation one step progress result Lemma~\ref{lem:scvxoner} applied to $\ov{v}_t$  implies
	\begin{align*}
	   \mathbb{E}\|\ov{v}_{t+1}-\vw^{\ast}\|^{2} & \leq\mathbb{E}(1-\mu\alpha_{t})\|\ov{v}_{t}-\vw^{\ast}\|^{2}\\
	   & +6E^{2}L\alpha_{t}^{3}G^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}\\ &
	   \leq\mathbb{E}(1-\mu\alpha_{t})\|\ov{w}_{t}-\vw^{\ast}\|^{2}\\
	   & +6E^{2}L\alpha_{t}^{3}G^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}
	\end{align*}
	
	The bound for $\mathbb{E}\|\ov{w}_{t+1}-\ov{v}_{t+1}\|^{2}$ for the two sampling schemes we consider is provided in \eq{\ref{eq:partialsample}}, and applying it we can write the one step progress for partial participation as
	\begin{align}\label{eqn:scvx-partial-oner}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & \leq(1-\mu\alpha_{t})\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}\nonumber\\
	&+\frac{4}{K}\alpha_{t}^{2}E^{2}G^{2}+6E^{2}L\alpha_{t}^{3}G^{2},
	\end{align}
	and the same arguments using induction and $L$-smoothness as the full participation case implies 
	\begin{align*}
	\mathbb{E}F(\ov{w}_{T})-F^{\ast}=\mathcal{O}(\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}
	+\frac{\kappa E^{2}G^{2}/\mu}{KT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}})
	\end{align*}
	
	\begin{comment}
	\begin{proof}
	With partial participation, $2E\sum_{\tau=1}^{E-1}\alpha_{t-\tau}^{2}\sum_{k=1}^{N}p_{k}\left(\|\nabla F_{k}(\ov{w}_{t-\tau},\mathbf{\xi}_{t-\tau}^{k})\|^{2}+l^{2}\|w_{t-\tau}^{k}-\ov{w}_{t-\tau}\|^{2}\right)$
	\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & \leq\mathbb{E}(1-\mu\alpha_{t})\|\ov{w}_{t}-\vw^{\ast}\|^{2}+5E^{2}L\alpha_{t}^{3}G^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}+\frac{1}{K}\sum_{k}p^{k}\|\vw_{t}^{k}-\ov{w}_{t}\|^{2}\\
	& \leq\mathbb{E}(1-\mu\alpha_{t})\|\ov{w}_{t}-\vw^{\ast}\|^{2}+5E^{2}L\alpha_{t}^{3}G^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}+\frac{4}{K}\alpha_{t}^{2}E^{2}G^{2}\\
	& \leq\mathbb{E}(1-\mu\alpha_{t})(1-\mu\alpha_{t-1})\cdots(1-\mu\alpha_{t-E})\|\ov{w}_{t-E}-\vw^{\ast}\|^{2}=O(\frac{1}{t-E}\sigma^{2}+E^{2}LG^{2}\frac{1}{(t-E)^{2}}+\frac{E^{2}G^{2}}{K}\frac{1}{(t-E)^{3/2}})\\
	& +(\alpha_{t}^{3}+(1-\mu\alpha_{t-1})\alpha_{t-1}^{3}+(1-\mu\alpha_{t-1})(1-\mu\alpha_{t-2})\alpha_{t-2}^{3}+\cdots+(1-\mu\alpha_{t-1})\cdots(1-\mu\alpha_{t-E})\alpha_{t-E}^{3}E^{2}LG^{2}\\
	& +(\alpha_{t}^{2}+(1-\mu\alpha_{t-1})\alpha_{t-1}^{2}+(1-\mu\alpha_{t-1})(1-\mu\alpha_{t-2})\alpha_{t-2}^{2}+\cdots+(1-\mu\alpha_{t-1})\cdots(1-\mu\alpha_{t-E})\alpha_{t-E}^{2}\sigma^{2}\\
	& +\frac{4}{K}EG^{2}(\alpha_{t}^{2}+\alpha_{t-1}^{2}+\cdots+\alpha_{t-E}^{2})
	\end{align*}
	
	so need ot check 
	\begin{align*}
	(1-\mu\alpha_{t})(1-\mu\alpha_{t-1})\cdots(1-\mu\alpha_{t-E+1})b\alpha_{t-E+1}^{\beta}+\alpha_{t}^{2} & \leq\alpha_{t+1}^{\beta}\\
	\end{align*}
	
	Alternatively, show that 
	\begin{align*}
	\frac{1}{K}\sum_{k}p^{k}\|\vw_{t}^{k}-\ov{w}_{t}\|^{2} & =O(\alpha_{t}^{2}G^{2}/K)\\
	\sum_{k}p^{k}\|\vw_{t}^{k}-\ov{w}_{t}\|^{2}\le\sum_{k}p^{k}\|\vw_{t}^{k}-\ov{w}_{t_{0}}\|^{2} & \leq\sum_{k}p^{k}\|-\alpha_{t_{0}}g_{t_{0},k}-\alpha_{t_{0}+1}g_{t_{0}+1,k}-\cdots-\alpha_{t-1}\vg_{t-1,k}\|^{2}\\
	& =\sum_{k}p^{k}\|\alpha_{t_{0}}g_{t_{0},k}+\alpha_{t_{0}+1}g_{t_{0}+1,k}+\cdots+\alpha_{t-1}\vg_{t-1,k}\|^{2}\\
	& =\sum_{k}p^{k}\|\alpha_{t_{0}}\nabla F_{k}(\ov{w}_{t_{0}},\mathbf{\xi}_{t_{0}}^{k})+\alpha_{t_{0}}\nabla F_{k}(\ov{w}_{t_{0}}-\alpha_{t_{0}+1}\nabla F_{k}(\ov{w}_{t_{0}},\mathbf{\xi}_{t_{0}}^{k}),\mathbf{\xi}_{t_{0}+1}^{k})+\cdots+\alpha_{t-1}\vg_{t-1,k}\|^{2}
	\end{align*}
	The intuition is to approximate with the sequence starting at $w^{\ast}$,
	i.e. 
	\begin{align*}
	\sum_{k}p^{k}\|\alpha_{t_{0}}\nabla F_{k}(\vw^{\ast},\mathbf{\xi}_{t_{0}}^{k})+\alpha_{t_{0}+1}\nabla F_{k}(\vw^{\ast}-\alpha_{t_{0}+1}\nabla F_{k}(\vw^{\ast},\mathbf{\xi}_{t_{0}}^{k}),\mathbf{\xi}_{t_{0}+1}^{k})+\cdots+\alpha_{t-1}\vg_{t-1,k}\|^{2} & \approx\\
	\end{align*}
	since $\ov{w}_{t_{0}}\approx w^{\ast}+\frac{1}{t_{0}}$. We
	can take 
	\begin{align*}
	g_{t_{0},k} & =\nabla F_{k}(\ov{w}_{t_{0}},\xi)\approx\nabla F_{k}(\ov{w}_{t_{0}},\xi)+\\
	\end{align*}
	
	\begin{align*}
	\alpha_{t}^{2}+(1-\mu\alpha_{t-1})\alpha_{t-1}^{2}+(1-\mu\alpha_{t})(1-\mu\alpha_{t-1})b\alpha_{t-1} & \le\alpha_{t+1}\\
	\alpha_{t}^{2}+(1-\mu\alpha_{t-1})\alpha_{t-1}^{2}+(1-\mu\alpha_{t-1})(1-\mu\alpha_{t-2})\alpha_{t-2}^{2}+(1-\mu\alpha_{t})(1-\mu\alpha_{t-1})(1-\mu\alpha_{t-2})b\alpha_{t-2} & \le\alpha_{t+1}\\
	\dots
	\end{align*}
	recall 
	\begin{align*}
	(b(1-\mu\alpha_{t})+\alpha_{t})\alpha_{t} & \leq b\alpha_{t+1}
	\end{align*}
	
	\begin{align*}
	(1-\mu\alpha_{t-1})\alpha_{t-1}^{2}+(1-\mu\alpha_{t})(1-\mu\alpha_{t-1})b\alpha_{t-1} & \leq(1-\mu\alpha_{t})\alpha_{t-1}^{2}+(1-\mu\alpha_{t})(1-\mu\alpha_{t-1})b\alpha_{t-1}\\
	& \le(1-\mu\alpha_{t})\alpha_{t-1}(\alpha_{t-1}+(1-\mu\alpha_{t-1})b)\\
	& \leq(1-\mu\alpha_{t})b\alpha_{t}
	\end{align*}
	so that 
	\begin{align*}
	(1-\mu\alpha_{t})\beta\alpha_{t}+\alpha_{t}^{2} & \leq b\alpha_{t+1}\\
	\end{align*}
	\begin{align*}
	(1-\mu\alpha_{t-1})\alpha_{t-1}^{2}+(1-\mu\alpha_{t})(1-\mu\alpha_{t-1})b\alpha_{t-1} & \leq\alpha_{t}^{2}+(1-\mu\alpha_{t-1})\alpha_{t-1}(\alpha_{t-1}+b(1-\mu\alpha_{t}))\le\\
	(\alpha_{t-1}+b(1-\mu\alpha_{t}))\alpha_{t}\leq
	\end{align*}
	\end{proof}
	\end{comment}
\end{proof}

\subsubsection{Deferred proofs of key lemmas}
Here we first rewrite the proofs of lemmas \ref{lem:bgv} and  \ref{lem:bdw} from~\cite{li2019convergence} with slight modifications for the consistency and completeness of this work, since later we will use modified versions of these results in the convergence proof for Nesterov accelerated FedAvg.
\begin{proof}[Proof of lemma~\ref{lem:bgv}]
	\begin{align*}
	\mathbb{E}\|\vg_{t}-\ov{g}_{t}\|^{2} & =\mathbb{E}\|\vg_{t}-\mathbb{E}\vg_{t}\|^{2}=\sum_{k=1}^{N}p_{k}^{2}\|\vg_{t,k}-\mathbb{E}\vg_{t,k}\|^{2}\leq \sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}
	\end{align*}
\end{proof}


\begin{proof}[Proof of lemma~\ref{lem:bdw}]
	Now we bound $\mathbb{E}\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}$ following \cite{li2019convergence}.
	Since communication is done every $E$ steps, for any $t\geq0$, we
	can find a $t_{0}\leq t$ such that $t-t_{0}\leq E-1$ and $\vw_{t_{0}}^{k}=\ov{w}_{t_{0}}$for
	all $k$. Moreover, using $\alpha_{t}$ is non-increasing and $\alpha_{t_{0}}\leq2\alpha{}_{t}$
	for any $t-t_{0}\leq E-1$, we have 
		\begin{align*}
	& \mathbb{E}\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}
= \mathbb{E}\sum_{k=1}^{N}p_{k}\|\vw_{t}^{k}-\ov{w}_{t_{0}}-(\ov{w}_{t}-\ov{w}_{t_{0}})\|^{2}\\
&\leq \mathbb{E}\sum_{k=1}^{N}p_{k}\|\vw_{t}^{k}-\ov{w}_{t_{0}}\|^{2}
= \mathbb{E}\sum_{k=1}^{N}p_{k}\|\vw_{t}^{k}-\vw_{t_{0}}^{k}\|^{2}\\
	&= \mathbb{E}\sum_{k=1}^{N}p_{k}\|-\sum_{i=t_{0}}^{t-1}\alpha_{i}\vg_{i,k}\|^{2}
	\leq  2\sum_{k=1}^{N}p_{k}\mathbb{E}\sum_{i=t_{0}}^{t-1}E\alpha_{i}^{2}\|\vg_{i,k}\|^{2}\\
	&\leq  2\sum_{k=1}^{N}p_{k}E^{2}\alpha_{t_{0}}^{2}G^{2}
	\leq  4E^{2}\alpha_{t}^{2}G^{2}
	\end{align*}
\end{proof}
Based on the results of Lemma~\ref{lem:bgv},~\ref{lem:bdw}, we now prove the upper bound of one step SGD progress. This 
proof improves on the previous work~\cite{li2019convergence} and is the first to reveal the linear speedup of convergence of FedAvg. 
\begin{proof}[Proof of lemma~\ref{lem:scvxoner}]
	We have 
	\begin{align*}
	&\|\ov{w}_{t+1}-\vw^{\ast}\|^{2}  =\|(\ov{w}_{t}-\alpha_{t}\vg_{t})-\vw^{\ast}\|^{2}\\ &=\|(\ov{w}_{t}-\alpha_{t}\ov{g}_{t}-\vw^{\ast})-\alpha_{t}(\vg_{t}-\ov{g}_{t})\|^{2} = \underbrace{\|\ov{w}_{t}-\vw^{\ast}-\alpha_{t}\ov{g}_{t}\|^{2}}_{A_1} \\ &+\underbrace{2\alpha_{t}\langle\ov{w}_{t}-\vw^{\ast}-\alpha_{t}\ov{g}_{t},\ov{g}_{t}-\vg_{t}\rangle}_{A_2} + \underbrace{\alpha_{t}^{2}\|\vg_{t}-\ov{g}_{t}\|^{2}}_{A_3}
	\end{align*}
	where we denote: 
	\begin{align*}
	A_{1} & =\|\ov{w}_{t}-\vw^{\ast}-\alpha_{t}\ov{g}_{t}\|^{2}\\
	A_{2} & =2\alpha_{t}\langle\ov{w}_{t}-\vw^{\ast}-\alpha_{t}\ov{g}_{t},\ov{g}_{t}-\vg_{t}\rangle\\
	A_{3} & =\alpha_{t}^{2}\|\vg_{t}-\ov{g}_{t}\|^{2}
	\end{align*}
	By definition of $\vg_{t}$ and $\ov{g}_{t}$ (see \eq{\ref{eq:gradient}}), we have $\mathbb{E}A_{2}=0$.
	For $A_{3}$, we have the following upper bound (see Lemma~\ref{lem:bgv}):
	\begin{align*}
	\alpha_{t}^{2}\mathbb{E}\|\vg_{t}-\ov{g}_{t}\|^{2} \leq\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}
	\end{align*} 
	
	Next we bound $A_{1}$: 
	\begin{align*}
	&\|\ov{w}_{t}-\vw^{\ast}-\alpha_{t}\ov{g}_{t}\|^{2} \\ &=\|\ov{w}_{t}-\vw^{\ast}\|^{2}+2\langle\ov{w}_{t}-\vw^{\ast},-\alpha_{t}\ov{g}_{t}\rangle+\|\alpha_{t}\ov{g}_{t}\|^{2}
	\end{align*}
	and we will show that the third term $\|\alpha_{t}\ov{g}_{t}\|^{2}$
	can be canceled by an upper bound of the second term, which is one of our major improvements compared to prior works~\cite{li2019convergence}.
	\begin{comment}
	The last term is straightforward to bound by the convexity of $\|\cdot\|^{2}$
	and $L$-smoothness of $F_{k}$,
	\begin{align*}
	\alpha_{t}^{2}\|\ov{g}_{t}\|^{2} & \leq\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}\|\nabla F_{k}(\vw_{t}^{k})\|^{2}\leq2L\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}(F_{k}(\vw_{t}^{k})-F_{k}^{\ast})
	\end{align*}
	or 
	\begin{align*}
	\alpha_{t}^{2}\|\ov{g}_{t}\|^{2} & \leq\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}\|\nabla F_{k}(\vw_{t}^{k})\|^{2}\leq\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}\mathbb{E}\|\nabla F_{k}(\vw_{t}^{k},\mathbf{\xi}_{t}^{k})\|^{2}\leq\alpha_{t}^{2}G^{2}
	\end{align*}
	\end{comment}
	The upper bound of second term can be derived as follows, 	using the strong convexity and $L$-smoothness of $F_{k}$:
	\begin{align*}
	 -&2\alpha_{t}\langle\ov{w}_{t}-\vw^{\ast},\ov{g}_{t}\rangle= -2\alpha_{t}\sum_{k=1}^{N}p_{k}\langle\ov{w}_{t}-\vw^{\ast},\nabla F_{k}(\vw_{t}^{k})\rangle\\
	= &-2\alpha_{t}\sum_{k=1}^{N}p_{k}\langle\ov{w}_{t}-\vw_{t}^{k},\nabla F_{k}(\vw_{t}^{k})\rangle\\
	-&2\alpha_{t}\sum_{k=1}^{N}p_{k}\langle \vw_{t}^{k}-\vw^{\ast},\nabla F_{k}(\vw_{t}^{k})\rangle\\
	\leq&-2\alpha_{t}\sum_{k=1}^{N}p_{k}\langle\ov{w}_{t}-\vw_{t}^{k},\nabla F_{k}(\vw_{t}^{k})\rangle\\
	+&2\alpha_{t}\sum_{k=1}^{N}p_{k}(F_{k}(\vw^{\ast})-F_{k}(\vw_{t}^{k}))-\alpha_{t}\mu\sum_{k=1}^{N}p_{k}\|\vw_{t}^{k}-\vw^{\ast}\|^{2}\\
	\leq & 2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw_{t}^{k})-F_{k}(\ov{w}_{t})+\frac{L}{2}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}+F_{k}(\vw^{\ast})-F_{k}(\vw_{t}^{k})\right]-\alpha_{t}\mu\|\sum_{k=1}^{N}p_{k}\vw_{t}^{k}-\vw^{\ast}\|^{2}\\
	= &\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}+2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]\\
	-&\alpha_{t}\mu\|\ov{w}_{t}-\vw^{\ast}\|^{2}
	\end{align*}
	We record the bound we have obtained so far, as it will also be used in the proof for convex case: 
	\begin{align*}
	 &\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2}
	\leq  \mathbb{E}(1-\mu\alpha_{t})\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}\\
	 +&2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}+\alpha_{t}^{2}\|\ov{g}_{t}\|^{2} \numberthis \label{eq:common one step}
	\end{align*}
	For the term $2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]$, which is negative, we can ignore it, but this
	yields a suboptimal bound that fails to provide the desired linear
	speedup. Instead, we upper bound it using the following derivation:
	\begin{align*}
	& 2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]\\
	\leq& 2\alpha_{t}\left[F(\ov{w}_{t+1})-F(\ov{w}_{t})\right]\\
	\leq& 2\alpha_{t}\mathbb{E}\langle\nabla F(\ov{w}_{t}),\ov{w}_{t+1}-\ov{w}_{t}\rangle+\alpha_{t}L\mathbb{E}\|\ov{w}_{t+1}-\ov{w}_{t}\|^{2}\\
	=&-2\alpha_{t}^{2}\mathbb{E}\langle\nabla F(\ov{w}_{t}),\ov{g}_{t}\rangle+\alpha_{t}^{3}L\mathbb{E}\|\vg_{t}\|^{2}\\
	=&-\alpha_{t}^{2}\left[\|\nabla F(\ov{w}_{t})\|^{2}+\|\ov{g}_{t}\|^{2}-\|\nabla F(\ov{w}_{t})-\ov{g}_t\|^{2}\right]+\alpha_{t}^{3}L\mathbb{E}\|\vg_{t}\|^{2}\\
	\leq&-\alpha_{t}^{2}\left[\|\nabla F(\ov{w}_{t})\|^{2}+\|\ov{g}_{t}\|^{2}-L^{2}\sum_{k}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}\right]+\alpha_{t}^{3}L\mathbb{E}\|\vg_{t}\|^{2}\\
	\leq&-\alpha_{t}^{2}\|\ov{g}_{t}\|^{2}+\alpha_{t}^{2}L^{2}\sum_{k}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}+\alpha_{t}^{3}L\mathbb{E}\|\vg_{t}\|^{2}-\alpha_{t}^{2}\|\nabla F(\ov{w}_{t})\|^{2}
	\end{align*}
	where we have used the smoothness of $F$ twice. 
	
	Note that the term $-\alpha_{t}^{2}\|\ov{g}_{t}\|^{2}$ exactly
	cancels the $\alpha_{t}^{2}\|\ov{g}_{t}\|^{2}$ in the bound in \eq{\ref{eq:common one step}}, so that plugging in the bound for $-2\alpha_{t}\langle\ov{w}_{t}-\vw^{\ast},\ov{g}_{t}\rangle$,
	we have so far proved 
	\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & \leq\mathbb{E}(1-\mu\alpha_{t})\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}\\
	& +\alpha_{t}^{2}L^{2}\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}+\alpha_{t}^{3}L\mathbb{E}\|\vg_{t}\|^{2}-\alpha_{t}^{2}\|\nabla F(\ov{w}_{t})\|^{2} \numberthis \label{eq:common recursion}
	\end{align*}
	Under Assumption~\ref{ass:subgrad2}, we have $\mathbb{E}\|\vg_{t}\|^{2}\leq G^{2}$. Furthermore, we can check that our choice of $\alpha_t$ satisfies $\alpha_t$ is non-increasing and $\alpha_t \leq 2\alpha_{t+E}$, so we may plug in the bound $\mathbb{E}\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2} \leq 4E^{2}\alpha_{t}^{2}G^{2}$ to the above inequality (see Lemma~\ref{lem:bdw}).
	
	\begin{comment}
	Alternatively, we can also write
	\begin{align*}
	\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2} & =\sum_{k=1}^{N}p_{k}\|\ov{w}_{t-1}-\alpha_{t-1}\vg_{t-1}-\vw_{t-1}^{k}+\alpha_{t-1}\vg_{t-1,k}\|^{2}\\
	& \leq2\sum_{k=1}^{N}p_{k}\left(\|\ov{w}_{t-1}-\vw_{t-1}^{k}\|^{2}+\|\alpha_{t-1}\vg_{t-1}-\alpha_{t-1}\vg_{t-1,k}\|^{2}\right)
	\end{align*}
	and write 
	\begin{align*}
	\sum_{k}p_{k}\|\vg_{t-1,k}-\vg_{t-1}\|^{2}\leq\sum_{k}p_{k}\|\vg_{t-1,k}\|^{2} & =\sum_{k}p_{k}\|\vg_{t-1,k}-\nabla F_{k}(\vw_{t-1}^{k})+\nabla F_{k}(\vw_{t-1}^{k})\|^{2}\\
	& \leq2\sum_{k}p_{k}\|\vg_{t-1,k}-\nabla F_{k}(\vw_{t-1}^{k})\|^{2}+2\sum_{k}p_{k}\|\nabla F_{k}(\vw_{t-1}^{k})\|^{2}\\
	& \leq2\sum_{k}p_{k}\sigma_{k}^{2}+2\sum_{k}p_{k}\|\nabla F_{k}(\vw_{t-1}^{k})-\nabla F_{k}(\ov{w}_{t-1})+\nabla F_{k}(\ov{w}_{t-1})\|^{2}\\
	& \leq2\sum_{k}p_{k}\sigma_{k}^{2}+4\sum_{k}p_{k}\left(\|\nabla F_{k}(\vw_{t-1}^{k})-\nabla F_{k}(\ov{w}_{t-1})\|^{2}+\|\nabla F_{k}(\ov{w}_{t-1})\|^{2}\right)\\
	& \leq2\sigma^{2}+4\sum_{k}p_{k}\left(L^{2}\|\ov{w}_{t-1}-\vw_{t-1}^{k}\|^{2}+\|\nabla F_{k}(\ov{w}_{t-1})\|^{2}\right)
	\end{align*}
	so that 
	\begin{align*}
	\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2} & \leq2(1+4L^{2}\alpha_{t-1}^{2})\sum_{k=1}^{N}p_{k}\|\ov{w}_{t-1}-\vw_{t-1}^{k}\|^{2}+4\sigma^{2}\alpha_{t-1}^{2}+8\alpha_{t-1}^{2}\sum_{k}p_{k}\|\nabla F_{k}(\ov{w}_{t-1})\|^{2}
	\end{align*}
	and taking expectation, we have 
	\begin{align*}
	\mathbb{E}\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2} & \leq2(1+4L^{2}\alpha_{t-1}^{2})\mathbb{E}\sum_{k=1}^{N}p_{k}\|\ov{w}_{t-1}-\vw_{t-1}^{k}\|^{2}+4\alpha_{t-1}^{2}(\sigma^{2}+2G^{2})
	\end{align*}
	Using this recusive relation at most $E$ times to arrive at a communication
	round and that $\alpha_{t}L\leq1/8$ for all $t$, we have 
	\begin{align*}
	\mathbb{E}\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}\leq E\frac{1}{L^{2}}(\sigma^{2}+2G^{2})
	\end{align*}
	which gives 
	\end{comment}
	Therefore, we can conclude that, with $\nu_{max}:=N\cdot\max_{k}p_{k}$ and $\nu_{min}:=N\cdot\min_{k}p_{k}$, 
	\begin{align*}
	& \mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2}\\
	\leq& \mathbb{E}(1-\mu\alpha_{t})\|\ov{w}_{t}-\vw^{\ast}\|^{2}+4E^{2}L\alpha_{t}^{3}G^{2}+4E^{2}L^{2}\alpha_{t}^{4}G^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}+\alpha_{t}^{3}LG^{2}\\
	= &\mathbb{E}(1-\mu\alpha_{t})\|\ov{w}_{t}-\vw^{\ast}\|^{2}+4E^{2}L\alpha_{t}^{3}G^{2}+4E^{2}L^{2}\alpha_{t}^{4}G^{2}+\alpha_{t}^{2}\frac{1}{N^{2}}\sum_{k=1}^{N}(p_{k}N)^{2}\sigma_{k}^{2}+\alpha_{t}^{3}LG^{2}\\
	\leq &\mathbb{E}(1-\mu\alpha_{t})\|\ov{w}_{t}-\vw^{\ast}\|^{2}+4E^{2}L\alpha_{t}^{3}G^{2}+4E^{2}L^{2}\alpha_{t}^{4}G^{2}+\alpha_{t}^{2}\frac{1}{N^{2}}\nu_{max}^{2}\sum_{k=1}^{N}\sigma_{k}^{2}+\alpha_{t}^{3}LG^{2}\\
	\leq &\mathbb{E}(1-\mu\alpha_{t})\|\ov{w}_{t}-\vw^{\ast}\|^{2}+6E^{2}L\alpha_{t}^{3}G^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}
	\end{align*}
	where in the last inequality we use $\sigma^2=\sum_{k=1}^{N}p_k\sigma_{k}^{2}$, and that by construction $\alpha_{t}$
	satisfies $L\alpha_{t}\leq\frac{1}{8}$. 

\end{proof}


One may ask whether the dependence on $E$ in the term $\frac{\kappa E^{2}G^{2}/\mu}{KT}$
can be removed, or equivalently whether $\sum_{k}p_{k}\|\mathbf{w}_{t}^{k}-\overline{\mathbf{w}}_{t}\|^{2}=\mathcal{O}(1/T^{2})$
can be independent of $E$. We provide a simple counterexample that
shows that this is not possible in general. 
\begin{proposition} \label{prop:tight}
	There exists a dataset such that if $E=\mathcal{O}(T^{\beta})$ for
	any $\beta>0$ then $\sum_{k}p_{k}\|\mathbf{w}_{t}^{k}-\overline{\mathbf{w}}_{t}\|^{2}=\Omega(\frac{1}{T^{2-2\beta}})$
	.
\end{proposition}
\begin{proof}
	Suppose that we have an even number of devices and each $F_{k}(\mathbf{w})=\frac{1}{n_{k}}\sum_{j=1}^{n_{k}}(\mathbf{x}_{k}^{j}-\mathbf{w})^{2}$
	contains data points $\mathbf{x}_{k}^{j}=\mathbf{w}^{\ast,k}$, with
	$n_{k}\equiv n$. Moreover, the $\mathbf{w}{}^{\ast,k}$'s come in
	pairs around the origin. As a result, the global objective $F$ is
	minimized at $\mathbf{w}^{\ast}=0$. Moreover, if we start from $\overline{\mathbf{w}}_{0}=0$,
	then by design of the dataset the updates in local steps exactly cancel
	each other at each iteration, resulting in $\overline{\mathbf{w}}_{t}=0$
	for all $t$. On the other hand, if $E=T^{\beta}$, then starting
	from any $t=\mathcal{O}(T)$ with constant step size $\mathcal{O}(\frac{1}{T})$,
	after $E$ iterations of local steps, the local parameters are updated
	towards $\mathbf{w}^{\ast,k}$ with $\|\mathbf{w}_{t+E}^{k}\|^{2}=\Omega((T^{\beta}\cdot\frac{1}{T})^{2})=\Omega(\frac{1}{T^{2-2\beta}})$.
	This implies that 
	\begin{align*}
	\sum_{k}p_{k}\|\mathbf{w}_{t+E}^{k}-\overline{\mathbf{w}}_{t+E}\|^{2} & =\sum_{k}p_{k}\|\mathbf{w}_{t+E}^{k}\|^{2}\\
	& =\Omega(\frac{1}{T^{2-2\beta}})
	\end{align*}
	which is at a slower rate than $\frac{1}{T^{2}}$ for any $\beta>0$.
	Thus the sampling variance $\mathbb{E}\|\overline{\mathbf{w}}_{t+1}-\overline{\mathbf{v}}_{t+1}\|^{2}=\Omega(\sum_{k}p_{k}\mathbb{E}\|\mathbf{w}_{t+1}^{k}-\overline{\mathbf{w}}_{t+1}\|^{2})$
	decays at a slower rate than $\frac{1}{T^{2}}$, resulting in a convergence
	rate slower than $\mathcal{O}(\frac{1}{T})$ with partial participation. 
\end{proof}