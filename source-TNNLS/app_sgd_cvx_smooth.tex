% !TEX ROOT=./supp-main.tex
In this section we provide the proof of the convergence result for FedAvg with convex and smooth objectives. The key step is a one step progress result analogous to that in the strongly convex case, and their proofs share identical components as well. 
\begin{lemma} [\textbf{One step progress, convex case}]
Let $\overline{\mathbf{w}}_{t}=\sum_{k=1}^{N}p_{k}\mathbf{w}_{t}^{k}$ in FedAvg. Under assumptions~\ref{ass:lsmooth},\ref{ass:boundedvariance},\ref{ass:subgrad2}, the following bound holds for all $t$:
\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2}+\alpha_{t}(F(\ov{w}_{t})-F(\vw^{\ast})) & \leq \mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+6\alpha_{t}^{3}E^{2}LG^{2}
	\end{align*}
	\label{lem:cvxoner}
\end{lemma}
\begin{proof}
    The first part of the proof follows directly from \eq{\ref{eq:common one step}} in the proof of Lemma \ref{lem:scvxoner}. Setting $\mu=0$ in \eq{\ref{eq:common one step}} (since we are in the convex setting instead of strongly convex), we obtain 
    \begin{align*}
	\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & \leq\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2} \\ 
	& +2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]+\alpha_{t}^{2}\|\ov{g}_{t}\|^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}
	\end{align*}
	The difference of this bound with that in the strongly convex case
	is that we no longer have a contraction factor of $1-\mu\alpha_t$ in front of $\|\ov{w}_{t}-\vw^{\ast}\|^{2}$.
	In the strongly convex case, we were able to cancel $\alpha_{t}^{2}\|\ov{g}_{t}\|^{2}$
	with $2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]$
	and obtain only lower order terms. In the convex case, we use a different
	strategy and preserve $\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]$
	in order to obtain the desired optimality gap. 
	
	We have
	\begin{align*}
	\|\ov{g}_{t}\|^{2} & =\|\sum_{k}p_{k}\nabla F_{k}(\vw_{t}^{k})\|^{2}\\
	& =\|\sum_{k}p_{k}\nabla F_{k}(\vw_{t}^{k})-\sum_{k}p_{k}\nabla F_{k}(\ov{w}_{t})+\sum_{k}p_{k}\nabla F_{k}(\ov{w}_{t})\|^{2}\\
	& \leq2\|\sum_{k}p_{k}\nabla F_{k}(\vw_{t}^{k})-\sum_{k}p_{k}\nabla F_{k}(\ov{w}_{t})\|^{2}+2\|\sum_{k}p_{k}\nabla F_{k}(\ov{w}_{t})\|^{2}\\
	& \leq2L^{2}\sum_{k}p_{k}\|\vw_{t}^{k}-\ov{w}_{t}\|^{2}+2\|\sum_{k}p_{k}\nabla F_{k}(\ov{w}_{t})\|^{2}\\
	& =2L^{2}\sum_{k}p_{k}\|\vw_{t}^{k}-\ov{w}_{t}\|^{2}+2\|\nabla F(\ov{w}_{t})\|^{2}
	\end{align*}
	using $\nabla F(\vw^{\ast})=0$. Now using the $L$ smoothness of $F$,
	we have $\|\nabla F(\ov{w}_{t})\|^{2}\leq2L(F(\ov{w}_{t})-F(\vw^{\ast}))$,
	so that 
	\begin{align*}
	& \|\ov{w}_{t+1}-\vw^{\ast}\|^{2}\\
	\leq & \|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}+2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]\\
	& +2\alpha_{t}^{2}L^{2}\sum_{k}p_{k}\|\vw_{t}^{k}-\ov{w}_{t}\|^{2}+4\alpha_{t}^{2}L(F(\ov{w}_{t})-F(\vw^{\ast}))+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}\\
	= & \|\ov{w}_{t}-\vw^{\ast}\|^{2}+(2\alpha_{t}^{2}L^{2}+\alpha_{t}L)\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}+\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right] \\ 
	 & +\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}
	 +\alpha_{t}(1-4\alpha_{t}L)(F(\vw^{\ast})-F(\ov{w}_{t}))
	\end{align*}
	Since $F(\vw^{\ast})\leq F(\ov{w}_{t})$, as long as $4\alpha_{t}L\leq1$,
	we can ignore the last term, and rearrange the inequality to obtain
	\begin{align*}
	& \|\ov{w}_{t+1}-\vw^{\ast}\|^{2}+\alpha_{t}(F(\ov{w}_{t})-F(\vw^{\ast}))\\
 \leq & \|\ov{w}_{t}-\vw^{\ast}\|^{2}+(2\alpha_{t}^{2}L^{2}+\alpha_{t}L)\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}\\
	\leq & \|\ov{w}_{t}-\vw^{\ast}\|^{2}+\frac{3}{2}\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}
	\end{align*}
	
	The same argument as before yields $\mathbb{E}\sum_{k=1}^{N}p_{k}\|\ov{w}_{t}-\vw_{t}^{k}\|^{2}\leq4E^{2}\alpha_{t}^{2}G^{2}$
	which gives 
	\begin{align*}
	\|\ov{w}_{t+1}-\vw^{\ast}\|^{2}+\alpha_{t}(F(\ov{w}_{t})-F(\vw^{\ast})) & \leq\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}+6\alpha_{t}^{3}E^{2}LG^{2}\\
	& \leq\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+6\alpha_{t}^{3}E^{2}LG^{2}
	\end{align*}
\end{proof}
With the one step progress result, we can now prove the convergence result in the convex setting, which we restate below.
\begin{thm}
	Under assumptions~\ref{ass:lsmooth},\ref{ass:boundedvariance},\ref{ass:subgrad2} and constant learning
	rate $\alpha_{t}=\mathcal{O}(\sqrt{\frac{N}{T}})$, FedAvg satisfies
	\begin{align*}
	\min_{t\leq T}F(\overline{\mathbf{w}}_{t})-F(\mathbf{w}^{\ast}) & =\mathcal{O}\left(\frac{\nu_{\max}\sigma^{2}}{\sqrt{NT}}+\frac{NE^{2}LG^{2}}{T}\right)
	\end{align*}
	with full participation, and with partial device participation with $K$ sampled devices at
	each communication round and learning rate $\alpha_{t}=\mathcal{O}(\sqrt{\frac{K}{T}})$,
	\begin{align*}
	\min_{t\leq T}F(\overline{\mathbf{w}}_{t})-F(\mathbf{w}^{\ast}) & =\mathcal{O}\left(\frac{\nu_{\max}\sigma^{2}}{\sqrt{KT}}+\frac{E^{2}G^{2}}{\sqrt{KT}}+\frac{KE^{2}LG^{2}}{T}\right)
	\end{align*}
\end{thm}

\begin{proof}
	We first prove the bound for full participation. Applying Lemma~\ref{lem:cvxoner}, we have
	\begin{align*}
	\|\ov{w}_{t+1}-\vw^{\ast}\|^{2}+\alpha_{t}(F(\ov{w}_{t})-F(\vw^{\ast})) & \leq\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+6\alpha_{t}^{3}E^{2}LG^{2}
	\end{align*}
	Summing the inequalities from $t=0$ to $t=T$, we obtain 
	\begin{align*}
	\sum_{t=0}^{T}\alpha_{t}(F(\ov{w}_{t})-F(\vw^{\ast})) & \leq\|\vw_{0}-\vw^{\ast}\|^{2}+\sum_{t=0}^{T}\alpha_{t}^{2}\cdot\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+\sum_{t=0}^{T}\alpha_{t}^{3}\cdot6E^{2}LG^{2}
	\end{align*}
	so that
	\begin{align*}
	\min_{t\leq T}F(\ov{w}_{t})-F(\vw^{\ast}) & \leq\frac{1}{\sum_{t=0}^{T}\alpha_{t}}\left(\|\vw_{0}-\vw^{\ast}\|^{2}+\sum_{t=0}^{T}\alpha_{t}^{2}\cdot\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+\sum_{t=0}^{T}\alpha_{t}^{3}\cdot6E^{2}LG^{2}\right)
	\end{align*}
	
	By setting the constant learning rate $\alpha_{t}\equiv\sqrt{\frac{N}{T}}$,
	we have 
	
\begin{align*}
\min_{t\leq T}F(\ov{w}_{t})-F(\vw^{\ast}) & \leq\frac{1}{\sqrt{NT}}\cdot\|\vw_{0}-\vw^{\ast}\|^{2}+\frac{1}{\sqrt{NT}}T\cdot\frac{N}{T}\cdot\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+\frac{1}{\sqrt{NT}}T(\sqrt{\frac{N}{T}})^{3}6E^{2}LG^{2}\\
& \leq\frac{1}{\sqrt{NT}}\cdot\|\vw_{0}-\vw^{\ast}\|^{2}+\frac{1}{\sqrt{NT}}T\cdot\frac{N}{T}\cdot\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+\frac{N}{T}6E^{2}LG^{2}\\
& =(\|\vw_{0}-\vw^{\ast}\|^{2}+\nu_{\max}^{2}\sigma^{2})\frac{1}{\sqrt{NT}}+\frac{N}{T}6E^{2}LG^{2}\\
& =\mathcal{O}(\frac{\nu_{\max}^{2}\sigma^{2}}{\sqrt{NT}}+\frac{NE^{2}LG^{2}}{T})
\end{align*}
	
	{\color{red}For partial participation, the one step progress bound in Lemma \ref{lem:cvxoner} is updated in a similar manner as the strongly convex case in \eqref{eqn:scvx-partial-oner} to incorporate the sampling variance. More precisely, with partial participation,  
	\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} & =\mathbb{E}\|\ov{w}_{t+1}-\ov{v}_{t+1}+\ov{v}_{t+1}-\vw^{\ast}\|^{2}\\
	& =\mathbb{E}\|\ov{w}_{t+1}-\ov{v}_{t+1}\|^{2}+\mathbb{E}\|\ov{v}_{t+1}-\vw^{\ast}\|^{2},
	\end{align*}
	where $\mathbb{E}\ov{w}_{t+1}=\ov{v}_{t+1}$ for all $t$, by the unbiasedness of our sampling schemes. Since $\ov{v}_t =\sum_{k=1}^{N} p_{k} \mathbf{v}_{t}^{k}$ always averages over all devices, the full participation one step progress bound in Lemma~\ref{lem:cvxoner} applied to $\ov{v}_t$ implies
	\begin{align*}
	\mathbb{E}\|\ov{v}_{t+1}-\vw^{\ast}\|^{2}+\alpha_{t}(F(\ov{v}_{t})-F(\vw^{\ast})) & \leq\mathbb{E}\|\ov{v}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+6\alpha_{t}^{3}E^{2}LG^{2}\\ &
	   \leq\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+6\alpha_{t}^{3}E^{2}LG^{2}
	\end{align*}
	
	The bound for $\mathbb{E}\|\ov{w}_{t+1}-\ov{v}_{t+1}\|^{2}$ for the two sampling schemes we consider is provided in \eq{\ref{eq:partialsample}}, and applying it to the above bound we can write the one step progress for partial participation as
	\begin{align*}
	\mathbb{E}\|\ov{w}_{t+1}-\vw^{\ast}\|^{2} + \alpha_{t}(F(\ov{w}_{t})-F(\vw^{\ast})) & \leq\mathbb{E}\|\ov{w}_{t}-\vw^{\ast}\|^{2}+\alpha_{t}^{2}(\frac{1}{N}\nu_{max}^{2}\sigma^{2}+C)+6E^{2}L\alpha_{t}^{3}G^{2},
	\end{align*}
	where $C=\frac{4}{K}E^{2}G^{2}$ or $\frac{N-K}{N-1}\frac{4}{K}E^{2}G^{2}$ depending on the sampling scheme.
	}
	
	Summing up the one-step progress over $t$, 
	\begin{align*}
	\min_{t\leq T}F(\ov{w}_{t})-F(\vw^{\ast}) & \leq\frac{1}{\sum_{t=0}^{T}\alpha_{t}}\left(\|\vw_{0}-\vw^{\ast}\|^{2}+\sum_{t=0}^{T}\alpha_{t}^{2}\cdot(\frac{1}{N}\nu_{\max}\sigma^{2}+C)+\sum_{t=0}^{T}\alpha_{t}^{3}\cdot6E^{2}LG^{2}\right),
	\end{align*}
	so that with $\alpha_{t}=\sqrt{\frac{K}{T}}$, we have 
	\begin{align*}
	\min_{t\leq T}F(\ov{w}_{t})-F(\vw^{\ast}) & =\mathcal{O}(\frac{\nu_{\max}\sigma^{2}}{\sqrt{KT}}+\frac{E^{2}G^{2}}{\sqrt{KT}}+\frac{KE^{2}LG^{2}}{T}).
	\end{align*}
\end{proof}