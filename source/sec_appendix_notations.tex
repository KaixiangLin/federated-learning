% !TEX ROOT=./main.tex

\section{Additional Notations}
In this section, we introduce additional notations that are used throughout
the proof. Following common practice~\cite{stich2018local,li2019convergence}, we define two virtual sequences $\overline{\mathbf{v}}_{t}$ and $\overline{\mathbf{w}}_{t}$. For full device participation and $t \notin \cI_E$,
$\ov{v}_t = \ov{w}_t =\sum_{k=1}^{N} p_{k} \mathbf{v}_{t}^{k}$. For partial participation, $t \in \cI_E$, $\ov{w}_t \neq \ov{v}_t$ since $\ov{v}_t=\sum_{k=1}^{N} p_{k} \mathbf{v}_{t}^{k}$ while $\ov{w}_t=\sum_{k\in \cS_t}\mathbf{w}_{t}^{k}$. However, we can
set unbiased sampling strategy such that $ \EE_{\cS_t} \ov{w}_t = \ov{v}_t$.
$\overline{\mathbf{v}}_{t+1}$ is one-step SGD from $\overline{\mathbf{w}}_{t}$. 
\begin{align}
\overline{\mathbf{v}}_{t+1}=\overline{\mathbf{w}}_{t}-\eta_{t} \mathbf{g}_{t}	\label{eq:vbar}
\end{align}
% $t+1 \in \cI_E$, we can fetch $\overline{\vw}_{t+1}$, we can communicate $\overline{\vw}_{t+1}$ to all devices.
where $\vg_{t} = \sum_{k=1}^{N} p_{k} \vg_{t,k} $ is one-step stochastic gradient, averaged over all devices. 
\begin{align}
\vg_{t,k} \left\{\begin{array}{ll} 
 = \nabla F_{k}\left(\mathbf{w}_{t}^{k},\xi_{t}^{k} \right)  &  \text{smooth}\\
 \in \partial F_{k}\left(w_{t}^{k}, \xi_{t}^{k}\right)  & \text{non-smooth}
 \end{array}\right.
\end{align}
Similarly, we denote the expected one-step gradient $\ov{g}_{t}= \EE_{\xi_t}[\vg_t] = \sum_{k=1}^{N} p_{k} \EE_{\xi_{t}^{k}} \vg_{t,k}$, where
\begin{align}
\EE_{\xi_{t}^{k}} \vg_{t,k}  \left\{\begin{array}{ll} 
 = \nabla F_{k}\left(\mathbf{w}_{t}^{k}\right)  &  \text{smooth}\\
 \in \partial F_{k}\left(w_{t}^{k}\right)  & \text{non-smooth}
 \end{array}\right.
\end{align}
and $\xi_t = \{\xi_t^k\}_{k=1}^N$ denotes random samples at all devices at time step $t$. 
