% !TEX ROOT=./main.tex



\section{Linear Speedup Analysis of FedAvg}
\label{sec:sgd}

In this section, we provide convergence analyses of FedAvg with local
SGD updates in the general setting with heterogeneous data and partial
participation. We show that for strongly convex and smooth objectives,
the convergence of the optimality gap of averaged parameters across
devices is $\mathcal{O}(1/NT)$ where $N$ is the number of active
devices during each communication round, while for convex and smooth
objectives, the rate is $\mathcal{O}(1/\sqrt{NT})$. 

\subsection{Strongly Convex and Smooth Objectives}

We first show that FedAvg with local SGD updates has $\mathcal{O}(1/NT)$
convergence rate for $\mu$-strongly convex and $L$-smooth objectives.
The result improves on the $\mathcal{O}(1/T)$ result of \cite{li2019convergence}
with a linear speedup in the number of devices $N$. Moreover, it
implies a distinction in communication efficiency that guarantees
this linear speedup for FedAvg with full and partial device participations.
With full participation, $E$ can be chosen as large as $\mathcal{O}(\sqrt{T/N})$
without degrading the linear speedup in the number of workers. On
the other hand, with partial participation, $E$ must be $\mathcal{O}(1)$. 
\begin{theorem}
	\label{thm:SGD_scvx}Let $\overline{\mathbf{w}}_{T}=\sum_{k=1}^{N}p_{k}\mathbf{w}_{T}^{k}$
	be the average of local parameters of FedAvg with learning rates $\alpha_{t}=\frac{1}{4\mu(\gamma+t)}$,
	where $\gamma=\max\{32\kappa,E\}$ and $\kappa=\frac{L}{\mu}$. Then
	with full device participation, 
	\begin{align*}
	\mathbb{E}F(\overline{\mathbf{w}}_{T})-F^{\ast}=\mathcal{O}\left(\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}}\right)
	\end{align*}
	and with partial device participation with $K$ sampled devices at
	each communication round, 
	\begin{align*}
	\mathbb{E}F(\overline{\mathbf{w}}_{T})-F^{\ast}\leq\mathcal{O}\left(\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}+\frac{\kappa E^{2}G^{2}/\mu}{KT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}}\right)
	\end{align*}
	where $\nu_{\max}=\max_{k}Np_{k}$.
\end{theorem}
%
\textbf{Linear speedup. }We first compare our bound with that in \cite{li2019convergence},
which is $\mathcal{O}(\frac{1}{NT}\nu_{\max}^{2}\sigma^{2}\kappa/\mu+\frac{\kappa E^{2}G^{2}/\mu}{KT}+\frac{E^{2}G^{2}}{T}\kappa/\mu)$.
Because the third term $\frac{E^{2}G^{2}}{T}\kappa/\mu$ is also $\mathcal{O}(1/T)$
without a dependence on $N$, for any choice of $E$ their bound cannot
achieve linear speedup. The improvement of our bound comes from the
term $\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}}$, which now is $\mathcal{O}(1/T^{2})$.
As a result, all leading terms scale with $1/N$ in the full device
particiaption setting, and with $1/K$ in the partial participation
setting. This implies that in both settings, there is a linear speedup
in the number of active workers during the communication round. To
our knowledge, this is the first result that explicitly demonstrates
linear speedup in the number of active workers in the setting with
both non i.i.d. data and partial participation. \\
\textbf{Communication Complexity.} Our bound implies a distinction
in the choice of $E$ between the full and partial participation settings.
With full participation there is linear speedup $\mathcal{O}(1/N)$
as long as $E=\mathcal{O}(\sqrt{T/N})$. This corresponds to a communication
complexity of $\mathcal{O}(\sqrt{NT})$. In contrast, the bound in
\cite{li2019convergence} does not allow $E$ to scale with $\sqrt{T}$,
even for full participation. On the other hand, with partial participation,
the term $\frac{\kappa E^{2}G^{2}/\mu}{KT}$ is also a leading term,
and so $E$ must be $\mathcal{O}(1)$. In this setting, our bound
still yields a linear speedup in $K$, which is not the case for previous
analyses. \\
\textbf{Comparison with related works.} \cite{haddadpour2019convergence}
also proves a linear speedup $\mathcal{O}(1/NT)$ result for strongly
convex and smooth objectives in the heterogeneous data with partial
participation setting, with $\mathcal{O}(N^{1/3}T^{2/3})$ communication
complexity. However, their results are derived based on the more restrictive
bounded gradient diversity assumption. As discussed before, if $\frac{\sum_{k}p_{k}\|\nabla F_{k}(\mathbf{w})\|_{2}^{2}}{\|\sum_{k}p_{k}\nabla F_{k}(\mathbf{w})\|_{2}^{2}}$
is bounded, then $\nabla F_{k}(\mathbf{w}^{\ast})\equiv0$ for all
$k$. In this overparameterized setting, we show in Section~\ref{sec:overparameterized}
that the convergence is in fact exponential, thus improving on the
$\mathcal{O}(1/NT)$ rate in \cite{haddadpour2019convergence} with
better communication complexity. 

\subsection{Convex Smooth Objectives}
Next we provide linear speedup analyses of FedAvg with convex and
smooth objectives and show that the optimality gap is $\mathcal{O}(1/\sqrt{NT})$
where $N$ is the number of participating devices. This result complements
the strongly convex case in the previous part, as well as the nonconvex
smooth setting in \cite{jiang2018linear,yu2019parallel,haddadpour2019convergence},
where $\mathcal{O}(1/\sqrt{NT})$ results are given in terms of averaged
gradient norm. 
\begin{theorem}
	\label{thm:SGD_cvx}With full device participation and constant learning
	rate $\alpha_{t}=\mathcal{O}(\sqrt{\frac{N}{T}})$, 
	\begin{align*}
	\min_{t\leq T}F(\overline{\mathbf{w}}_{t})-F(\mathbf{w}^{\ast}) & =\mathcal{O}\left(\frac{\nu_{\max}^{2}\sigma^{2}}{\sqrt{NT}}+\frac{NE^{2}LG^{2}}{T}\right)
	\end{align*}
	and with partial device participation with $K$ sampled devices at
	each communication round and learning rate $\alpha_{t}=\mathcal{O}(\sqrt{\frac{K}{T}})$,
	\begin{align*}
	\min_{t\leq T}F(\overline{\mathbf{w}}_{t})-F(\mathbf{w}^{\ast}) & =\mathcal{O}\left(\frac{\nu_{\max}^{2}\sigma^{2}}{\sqrt{KT}}+\frac{E^{2}G^{2}}{\sqrt{KT}}+\frac{KE^{2}LG^{2}}{T}\right)
	\end{align*}
\end{theorem}
%
\textbf{Choice of $E$ and linear speedup. }With full participation,
as long as $E=\mathcal{O}(T^{1/4}/N^{3/4})$, the convergence
rate is $\mathcal{O}(1/\sqrt{NT})$ with $\mathcal{O}(N^{3/4}T^{3/4})$
communication rounds. In the partial participation setting, $E$ must
be $O(1)$ in order to achieve linear speedup of $\mathcal{O}(1/\sqrt{KT})$.
Our result again demonstrates the difference in communication complexities
between full and partial participation, and is to our knowledge the
first result on linear speedup in the general federated learning setting
with both heterogeneous data and partial participation. \\
\textbf{Learning rate. }The learning rate now depends on the final
horizon $T$ of the convergence statement, whereas in the strongly
convex case the learning rate decays as $\mathcal{O}(1/t)$. Such
a requirement $\alpha_{t}=\mathcal{O}(\sqrt{N/T})$ is also present
in the works \cite{haddadpour2019convergence,yu2019parallel} on nonconvex
problems with $\mathcal{O}(1/\sqrt{NT})$ linear speedup convergence results. 