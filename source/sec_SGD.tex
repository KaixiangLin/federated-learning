% !TEX ROOT=./main.tex



\section{Linear Speedup Analysis of FedAvg}
\label{sec:sgd}

In this section, we provide convergence analyses of FedAvg for convex objectives in the general setting with both heterogeneous data and partial
participation. We show that for strongly convex and smooth objectives,
the convergence of the optimality gap of averaged parameters across
devices is $\mathcal{O}(1/NT)$, while for convex and smooth
objectives, the rate is $\mathcal{O}(1/\sqrt{NT})$. 
Detailed proofs are deferred to Appendix Section~\ref{sec:app:fedavg}.

\subsection{Strongly Convex and Smooth Objectives}

We first show that FedAvg has $\mathcal{O}(1/NT)$ convergence rate
for $\mu$-strongly convex and $L$-smooth objectives. The result
improves on the $\mathcal{O}(1/T)$ rate of \cite{li2019convergence}
with a linear speedup in the number of devices $N$. Moreover, it
implies a distinction in communication efficiency that guarantees
this linear speedup for FedAvg with full and partial device participations.
With full participation, $E$ can be chosen as large as $\mathcal{O}(\sqrt{T/N})$
without degrading the linear speedup in the number of workers. On
the other hand, with partial participation, $E$ must be $\mathcal{O}(1)$
to guarantee $\mathcal{O}(1/NT)$ convergence.
\begin{theorem}
	\label{thm:SGD_scvx}Let $\overline{\mathbf{w}}_{T}=\sum_{k=1}^{N}p_{k}\mathbf{w}_{T}^{k}$,
	$\nu_{\max}=\max_{k}Np_{k}$, and set decaying learning rates $\alpha_{t}=\frac{1}{4\mu(\gamma+t)}$
	with $\gamma=\max\{32\kappa,E\}$ and $\kappa=\frac{L}{\mu}$. Then
	under Assumptions~\ref{ass:lsmooth},\ref{ass:stroncvx},\ref{ass:boundedvariance},\ref{ass:subgrad2} with full device participation, 
	\begin{align*}
	\mathbb{E}F(\overline{\mathbf{w}}_{T})-F^{\ast}=\mathcal{O}\left(\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}}\right)
	\end{align*}
	and with partial device participation with at most $K$ sampled devices
	at each communication round, 
	\begin{align*}
	\mathbb{E}F(\overline{\mathbf{w}}_{T})-F^{\ast}=\mathcal{O}\left(\frac{\kappa E^{2}G^{2}/\mu}{KT}+\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}}\right)
	\end{align*}
\end{theorem}
%
\textbf{Linear speedup. }We first compare our bound with that in \cite{li2019convergence},
which is $\mathcal{O}(\frac{1}{NT}+\frac{E^{2}}{KT}+\frac{E^{2}G^{2}}{T})$.
Because the term $\frac{E^{2}G^{2}}{T}$ is also $\mathcal{O}(1/T)$
without a dependence on $N$, for any choice of $E$ their bound cannot
achieve linear speedup. The improvement of our bound comes from the
term $\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}}$, which now is $\mathcal{O}(E^{2}/T^{2})$.
As a result, all leading terms scale with $1/N$ in the full device
particiaption setting, and with $1/K$ in the partial participation
setting. This implies that in both settings, there is a \emph{linear
	speedup} in the number of active workers during the communication
round. \\
% To our knowledge, this is the first result that explicitly demonstrates
% linear speedup in the number of active workers in the setting with
% both non i.i.d. data and partial participation. \\
\textbf{Communication Complexity.} Our bound implies a distinction
in the choice of $E$ between the full and partial participation settings.
With full participation there is linear speedup $\mathcal{O}(1/NT)$
as long as $E=\mathcal{O}(\sqrt{T/N})$ since then $\mathcal{O}(E^{2}/T^{2})=\mathcal{O}(1/NT)$
matches the leading term. This corresponds to a communication complexity
of $T/E=\mathcal{O}(\sqrt{NT})$. In contrast, the bound in \cite{li2019convergence}
does not allow $E$ to scale with $\sqrt{T}$ to preserve $\mathcal{O}(1/T)$
rate, even for full participation. On the other hand, with partial
participation, $\frac{\kappa E^{2}G^{2}/\mu}{KT}$ is also a leading
term, and so $E$ must be $\mathcal{O}(1)$. In this case, our bound
still yields a linear speedup in $K$, which is also confirmed by
experiments. The $E=\mathcal{O}(1)$ requirement in partial participation
likely cannot be removed for our sampling schemes, as the sampling
variance is $\Omega(E^{2}/T^{2})$ and the dependence on $E$ is tight.  \\
\textbf{Comparison with related works.} To better understand the significance of the obtained bound, we compare our rates to the best-known results in related settings.~\cite{haddadpour2019convergence} proves a linear speedup $\mathcal{O}(1/NT)$ result for strongly convex and smooth objectives, with $\mathcal{O}(N^{1/3}T^{2/3})$ communication complexity with i.i.d. data and partial participation. However, their results build on the bounded gradient diversity assumption, which implies the existence of $\mathbf{w}^*$ that minimizes all local objectives (see discussions in Section~\ref{sec:assumptions}), effectively removing system heterogeneity. The bound in \cite{koloskova2020unified} matches our bound in the full participation case, but their framework excludes partial particiation (see their Proposition 1).

\begin{comment}
	In this overparameterized setting, we prove a geometric convergence rate (see Section~\ref{sec:overparameterized}), 
	thus improving on the rate in \cite{haddadpour2019convergence} with
	better communication complexity. 
\end{comment}

\subsection{Convex Smooth Objectives}
Next we provide linear speedup analyses of FedAvg with convex and
smooth objectives and show that the optimality gap is $\mathcal{O}(1/\sqrt{NT})$. 
This result complements the strongly convex case in the previous part, as well as the non-convex
smooth setting in \cite{jiang2018linear,yu2019parallel,haddadpour2019convergence},
where $\mathcal{O}(1/\sqrt{NT})$ results are given in terms of averaged
gradient norm. 
\begin{theorem}
	\label{thm:SGD_cvx}Under assumptions~\ref{ass:lsmooth},\ref{ass:boundedvariance},\ref{ass:subgrad2} and constant learning
	rate $\alpha_{t}=\mathcal{O}(\sqrt{\frac{N}{T}})$, 
	\begin{align*}
	\min_{t\leq T}F(\overline{\mathbf{w}}_{t})-F(\mathbf{w}^{\ast}) & =\mathcal{O}\left(\frac{\nu_{\max}^{2}\sigma^{2}}{\sqrt{NT}}+\frac{NE^{2}LG^{2}}{T}\right)
	\end{align*}
	with full participation, and with partial device participation with $K$ sampled devices at
	each communication round and learning rate $\alpha_{t}=\mathcal{O}(\sqrt{\frac{K}{T}})$,
	\begin{align*}
	\min_{t\leq T}F(\overline{\mathbf{w}}_{t})-F(\mathbf{w}^{\ast}) & =\mathcal{O}\left(\frac{\nu_{\max}^{2}\sigma^{2}}{\sqrt{KT}}+\frac{E^{2}G^{2}}{\sqrt{KT}}+\frac{KE^{2}LG^{2}}{T}\right)
	\end{align*}
\end{theorem}
%
\textbf{Choice of $E$ and linear speedup. }With full participation,
as long as $E=\mathcal{O}(T^{1/4}/N^{3/4})$, the convergence
rate is $\mathcal{O}(1/\sqrt{NT})$ with $\mathcal{O}(N^{3/4}T^{3/4})$
communication rounds. In the partial participation setting, $E$ must
be $O(1)$ in order to achieve linear speedup of $\mathcal{O}(1/\sqrt{KT})$.
Our result again demonstrates the difference in communication complexities
between full and partial participation, and is to our knowledge the
first result on linear speedup in the general federated learning setting
with both heterogeneous data and partial participation for convex objectives.
\begin{comment}
\textbf{Learning rate. }The learning rate now depends on the final
horizon $T$ of the convergence statement, whereas in the strongly
convex case the learning rate decays as $\mathcal{O}(1/t)$. Such
a requirement $\alpha_{t}=\mathcal{O}(\sqrt{N/T})$ is also present
in the works \cite{haddadpour2019convergence,yu2019parallel} on non-convex
problems with $\mathcal{O}(1/\sqrt{NT})$ linear speedup convergence results. 
\end{comment}