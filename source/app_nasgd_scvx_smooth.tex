We show that FedAvg with Accelerated SGD has $O(1/T)$ rate under
$\mu$-strong convexity and $L$-smoothness. The proof follows the
framework of the ICLR paper. The FedAv algorithm with Nesterov Accelerated
SGD (NASGD) follows the updates
\begin{align*}
y_{t+1}^{k} & =w_{t}^{k}-\alpha_{t}g_{t,k}\\
w_{t+1}^{k} & =\begin{cases}
y_{t+1}^{k}+\beta_{t}(y_{t+1}^{k}-y_{t}^{k}) & \text{if }t+1\notin\mathcal{I}_{E}\\
\sum_{k=1}^{N}p_{k}\left[y_{t+1}^{k}+\beta_{t}(y_{t+1}^{k}-y_{t}^{k})\right] & \text{if }t+1\in\mathcal{I}_{E}
\end{cases}
\end{align*}
where 
\begin{align*}
g_{t,k} & :=\nabla F_{k}(w_{t}^{k},\xi_{t}^{k})
\end{align*}
is the stochastic gradient. 

Define the virtual sequences $\overline{y}_{t}=\sum_{k=1}^{N}p_{k}y_{t}^{k}$,
$\overline{w}_{t}=\sum_{k=1}^{N}p_{k}w_{t}^{k}$, and $\overline{g}_{t}=\sum_{k=1}^{N}p_{k}\mathbb{E}g_{t,k}$.
We have $\mathbb{E}g_{t}=\overline{g}_{t}$ and $\overline{y}_{t+1}=\overline{w}_{t}-\alpha_{t}g_{t}$,
and $\overline{w}_{t+1}=\overline{y}_{t+1}+\beta_{t}(\overline{y}_{t+1}-\overline{y}_{t})$. 

\begin{theorem}
	Assume that $F_{k}$ is $L$-smooth and $\mu$-strongly convex for
	all $k$, and let $\nu_{\max}=\max_{k}\frac{1}{N}p_{k}$. Let $\kappa=\frac{L}{\mu}$,
	$\gamma=\max\{32\kappa,E\}$ where $E$ is the communication delay,
	and diminishing learning rates $\alpha_{t}=\frac{9}{\mu}\frac{1}{t+\gamma}$
	and $\beta_{t-1}=\frac{9}{14\max\{\mu,1\}}\frac{1}{t+\gamma}$. Let
	$\overline{y}_{T}=\sum_{k=1}^{N}p_{k}y_{T}^{k}$ be the average of
	local parameters at an arbitrary time $T$. 
	
	With full device participation, 
	\begin{align*}
	\mathbb{E}F(\overline{y}_{T})-F^{\ast}=O(\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}})
	\end{align*}
	and with partial device participation with $K$ sampled devices at
	each communication round, 
	\begin{align*}
	\mathbb{E}F(\overline{y}_{T})-F^{\ast}\leq O(\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}+\frac{\kappa E^{2}G^{2}/\mu}{KT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}})
	\end{align*}
\end{theorem}
%
%
\textbf{}%
\begin{comment}
This implies that $E$ canot be chosen $O(T^{\beta})$ for any $\beta>0$
without degrading the performance. This should be checked in experiments,
whether with partial participation if the communication round is set
to scale with $T$, the convergence deteriorates. This is in constrast
with the full participation case, where $E=O(\sqrt{\frac{T}{N}})$
is allowed. \textbf{If we can confirm that full participation allows
linear speedup with $\nu=N\cdot\max_{k}p_{k}\approx1$ and $E=O(\sqrt{\frac{T}{N}})$,
whereas partial participation only allows $E=O(1)$, then this would
an interesting phenomenon that is not reported by previous studies!}

The convergence result implies that with fixed $E$, as long as \$N\$
satisfies \$E=O(\textbackslash sqrt\{T/N\})\$, there is linear speedup.
When \$N\$ exceeds an upper bound, however, linear speedup may fail
to happen, and \$T/N\$ may remain constant, i.e. the number of iterations
required for convergence may increase with \$N\$. 
\end{comment}

\begin{proof}
	The proof again uses the $L$-smoothness of $F$ to bound 
	\begin{align*}
	\mathbb{E}(F(\overline{y}_{t}))-F^{\ast} & =\mathbb{E}(F(\overline{y}_{t})-F(w^{\ast}))\\
	& \leq\frac{L}{2}\mathbb{E}\|\overline{y}_{t}-w^{\ast}\|^{2}
	\end{align*}
	
	Our main step is to prove the bound 
	\begin{align*}
	\mathbb{E}\|\overline{y}_{t+1}-w^{\ast}\|^{2} & \leq(1-\mu\alpha_{t})\mathbb{E}\|\overline{y}_{t}-w^{\ast}\|^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}+5E^{2}L\alpha_{t}^{3}G^{2}
	\end{align*}
	
	We have 
	\begin{align*}
	\|\overline{y}_{t+1}-w^{\ast}\|^{2} & =\|(\overline{w}_{t}-\alpha_{t}g_{t})-w^{\ast}\|^{2}\\
	& =\|(\overline{w}_{t}-\alpha_{t}\overline{g}_{t}-w^{\ast})-\alpha_{t}(g_{t}-\overline{g}_{t})\|^{2}\\
	& =A_{1}+A_{2}+A_{3}
	\end{align*}
	where 
	\begin{align*}
	A_{1} & =\|\overline{w}_{t}-w^{\ast}-\alpha_{t}\overline{g}_{t}\|^{2}\\
	A_{2} & =2\alpha_{t}\langle\overline{w}_{t}-w^{\ast}-\alpha_{t}\overline{g}_{t},\overline{g}_{t}-g_{t}\rangle\\
	A_{3} & =\alpha_{t}^{2}\|g_{t}-\overline{g}_{t}\|^{2}
	\end{align*}
	$\mathbb{E}A_{2}=0$ by definition of $g_{t}$ and $\overline{g}_{t}$,
	while for $A_{3}$ we have
	\begin{align*}
	\alpha_{t}^{2}\mathbb{E}\|g_{t}-\overline{g}_{t}\|^{2} & =\alpha_{t}^{2}\mathbb{E}\|g_{t}-\mathbb{E}g_{t}\|^{2}=\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\|g_{t,k}-\mathbb{E}g_{t,k}\|^{2}\leq\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}
	\end{align*}
	again by Jensen's inequality and using the independence of $g_{t,k},g_{t,k'}$. 
	
	Next we bound $A_{1}$: 
	\begin{align*}
	\|\overline{w}_{t}-w^{\ast}-\alpha_{t}\overline{g}_{t}\|^{2} & =\|\overline{w}_{t}-w^{\ast}\|^{2}+2\langle\overline{w}_{t}-w^{\ast},-\alpha_{t}\overline{g}_{t}\rangle+\|\alpha_{t}\overline{g}_{t}\|^{2}
	\end{align*}
	Same as the SGD case, 
	\begin{align*}
	-2\alpha_{t}\langle\overline{w}_{t}-w^{\ast},\overline{g}_{t}\rangle+\|\alpha_{t}\overline{g}_{t}\|^{2} & \leq\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+\alpha_{t}^{2}L^{2}\sum_{k}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+\alpha_{t}^{3}L\mathbb{E}\|g_{t}\|^{2}-\alpha_{t}\mu\|\overline{w}_{t}-w^{\ast}\|^{2}
	\end{align*}
	so that 
	\begin{align*}
	\|\overline{w}_{t}-w^{\ast}-\alpha_{t}\overline{g}_{t}\|^{2} & \leq(1-\alpha_{t}\mu)\|\overline{w}_{t}-w^{\ast}\|^{2}+\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+\alpha_{t}^{2}L^{2}\sum_{k}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+\alpha_{t}^{3}L\mathbb{E}\|g_{t}\|^{2}
	\end{align*}
	
	Different from the SGD case, we have 
	\begin{align*}
	\|\overline{w}_{t}-w^{\ast}\|^{2} & =\|\overline{y}_{t}+\beta_{t-1}(\overline{y}_{t}-\overline{y}_{t-1})-w^{\ast}\|^{2}\\
	& =\|(1+\beta_{t-1})(\overline{y}_{t}-w^{\ast})-\beta_{t-1}(\overline{y}_{t-1}-w^{\ast})\|^{2}\\
	& =(1+\beta_{t-1})^{2}\|\overline{y}_{t}-w^{\ast}\|^{2}-2\beta_{t-1}(1+\beta_{t-1})\langle\overline{y}_{t}-w^{\ast},\overline{y}_{t-1}-w^{\ast}\rangle+\beta_{t-1}^{2}\|(\overline{y}_{t-1}-w^{\ast})\|^{2}\\
	& \leq(1+\beta_{t-1})^{2}\|\overline{y}_{t}-w^{\ast}\|^{2}+2\beta_{t-1}(1+\beta_{t-1})\|\overline{y}_{t}-w^{\ast}\|\cdot\|\overline{y}_{t-1}-w^{\ast}\|+\beta_{t-1}^{2}\|(\overline{y}_{t-1}-w^{\ast})\|^{2}
	\end{align*}
	which gives 
	\begin{align*}
	\|\overline{y}_{t+1}-w^{\ast}\|^{2} & \leq(1-\alpha_{t}\mu)(1+\beta_{t-1})^{2}\|\overline{y}_{t}-w^{\ast}\|^{2}+2(1-\alpha_{t}\mu)\beta_{t-1}(1+\beta_{t-1})\|\overline{y}_{t}-w^{\ast}\|\cdot\|\overline{y}_{t-1}-w^{\ast}\|+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}\\
	& +\beta_{t-1}^{2}(1-\alpha_{t}\mu)\|(\overline{y}_{t-1}-w^{\ast})\|^{2}+\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+\alpha_{t}^{2}L^{2}\sum_{k}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+\alpha_{t}^{3}LG^{2}
	\end{align*}
	and we will using this recursive relation to obtain the desired bound. 
	
	First we bound $\mathbb{E}\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}$.
	Since communication is done every $E$ steps, for any $t\geq0$, we
	can find a $t_{0}\leq t$ such that $t-t_{0}\leq E-1$ and $w_{t_{0}}^{k}=\overline{w}_{t_{0}}$for
	all $k$. Moreover, using $\alpha_{t}$ is non-increasing and $\alpha_{t_{0}}\leq2\alpha{}_{t}$
	for any $t-t_{0}\leq E-1$, we have 
	\begin{align*}
	\mathbb{E}\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2} & =\mathbb{E}\sum_{k=1}^{N}p_{k}\|w_{t}^{k}-\overline{w}_{t_{0}}-(\overline{w}_{t}-\overline{w}_{t_{0}})\|^{2}\\
	& \leq\mathbb{E}\sum_{k=1}^{N}p_{k}\|w_{t}^{k}-\overline{w}_{t_{0}}\|^{2}\\
	& =\mathbb{E}\sum_{k=1}^{N}p_{k}\|w_{t}^{k}-w_{t_{0}}^{k}\|^{2}\\
	& =\mathbb{E}\sum_{k=1}^{N}p_{k}\|\sum_{i=t_{0}}^{t-1}\beta_{i}(y_{i+1}^{k}-y_{i}^{k})-\sum_{i=t_{0}}^{t-1}\alpha_{i}g_{i,k}\|^{2}\\
	& \leq2\sum_{k=1}^{N}p_{k}\mathbb{E}\sum_{i=t_{0}}^{t-1}(E-1)\alpha_{i}^{2}\|g_{i,k}\|^{2}+2\sum_{k=1}^{N}p_{k}\mathbb{E}\sum_{i=t_{0}}^{t-1}(E-1)\beta_{i}^{2}\|(y_{i+1}^{k}-y_{i}^{k})\|^{2}\\
	& \leq2\sum_{k=1}^{N}p_{k}\mathbb{E}\sum_{i=t_{0}}^{t-1}(E-1)\alpha_{i}^{2}(\|g_{i,k}\|^{2}+\|(y_{i+1}^{k}-y_{i}^{k})\|^{2})\\
	& \leq4\sum_{k=1}^{N}p_{k}\mathbb{E}\sum_{i=t_{0}}^{t-1}(E-1)\alpha_{i}^{2}G^{2}\\
	& \leq4(E-1)^{2}\alpha_{t_{0}}^{2}G^{2}\leq16(E-1)^{2}\alpha_{t}^{2}G^{2}
	\end{align*}
	where we have used $\mathbb{E}\|\overline{y}_{t+1}-\overline{y}_{t}\|^{2}\leq G^{2}$.
	To see this identity for appropriate $\alpha_{t},\beta_{t}$, note
	the recursion 
	\begin{align*}
	y_{t+1}^{k}-y_{t}^{k} & =w_{t}^{k}-w_{t-1}^{k}-(\alpha_{t}g_{t,k}-\alpha_{t-1}g_{t-1,k})\\
	w_{t+1}^{k}-w_{t}^{k} & =-\alpha_{t}g_{t,k}+\beta_{t}(y_{t+1}^{k}-y_{t}^{k})
	\end{align*}
	so that 
	\begin{align*}
	y_{t+1}^{k}-y_{t}^{k} & =-\alpha_{t-1}g_{t-1,k}+\beta_{t-1}(y_{t}^{k}-y_{t-1}^{k})-(\alpha_{t}g_{t,k}-\alpha_{t-1}g_{t-1,k})\\
	& =\beta_{t-1}(y_{t}^{k}-y_{t-1}^{k})-\alpha_{t}g_{t,k}
	\end{align*}
	Since the identity $y_{t+1}^{k}-y_{t}^{k}=\beta_{t-1}(y_{t}^{k}-y_{t-1}^{k})-\alpha_{t}g_{t,k}$
	implies 
	\begin{align*}
	\mathbb{E}\|y_{t+1}^{k}-y_{t}^{k}\|^{2} & \leq2\beta_{t-1}^{2}\mathbb{E}\|y_{t}^{k}-y_{t-1}^{k}\|^{2}+2\alpha_{t}^{2}G^{2}
	\end{align*}
	as long as $\alpha_{t},\beta_{t}$ satisfy $2\beta_{t-1}^{2}+2\alpha_{t}^{2}\leq1$,
	we can guarantee that $\mathbb{E}\|y_{t}^{k}-y_{t-1}^{k}\|^{2}\leq G^{2}$
	for all $k$. This together with Jensen's inequality gives $\mathbb{E}\|\overline{y}_{t}-\overline{y}_{t-1}\|^{2}\leq G^{2}$
	for all $t$. 
	
	Using the bound on $\mathbb{E}\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}$,
	we can conclude that, with $\nu_{max}:=N\cdot\max_{k}p_{k}$ and $\nu_{min}:=N\cdot\min_{k}p_{k}$, 
	
	\begin{align*}
	\mathbb{E}\|\overline{y}_{t+1}-w^{\ast}\|^{2} & \leq\mathbb{E}(1-\mu\alpha_{t})(1+\beta_{t-1})^{2}\|\overline{y}_{t}-w^{\ast}\|^{2}+16E^{2}L\alpha_{t}^{3}G^{2}+16E^{2}L^{2}\alpha_{t}^{4}G^{2}+\alpha_{t}^{3}LG^{2}+(1-\alpha_{t}\mu)\beta_{t-1}^{2}\|(\overline{y}_{t-1}-w^{\ast})\|^{2}\\
	& +\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}+2\beta_{t-1}(1+\beta_{t-1})(1-\alpha_{t}\mu)\|\overline{y}_{t}-w^{\ast}\|\cdot\|\overline{y}_{t-1}-w^{\ast}\|\\
	& \leq\mathbb{E}(1-\mu\alpha_{t})(1+\beta_{t-1})^{2}\|\overline{y}_{t}-w^{\ast}\|^{2}+20E^{2}L\alpha_{t}^{3}G^{2}+(1-\alpha_{t}\mu)\beta_{t-1}^{2}\|(\overline{y}_{t-1}-w^{\ast})\|^{2}\\
	& +\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}+2\beta_{t-1}(1+\beta_{t-1})(1-\alpha_{t}\mu)\|\overline{y}_{t}-w^{\ast}\|\cdot\|\overline{y}_{t-1}-w^{\ast}\|
	\end{align*}
	where $\sigma^{2}=\max_{k}\sigma_{k}^{2}$, and $\alpha_{t}$ satisfies
	$L\alpha_{t}\leq\frac{1}{8}$. We show next that $\mathbb{E}\|\overline{y}_{t}-w^{\ast}\|^{2}=O(\frac{1}{tN}+\frac{1}{t^{2}})$
	by induction.
	
	Assume that we have shown 
	\begin{align*}
	\mathbb{E}\|\overline{y}_{t}-w^{\ast}\|^{2} & \leq b(C\alpha_{t}^{2}+D\alpha_{t})
	\end{align*}
	where $C=20E^{2}LG^{2}$, $D=\frac{1}{N}\nu_{max}^{2}\sigma^{2}$,
	and $b$ is to be chosen later. For step sizes we choose $\alpha_{t}=\frac{9}{\mu}\frac{1}{t+\gamma}$
	and $\beta_{t-1}=\frac{9}{14\mu}\frac{1}{t+\gamma}$ if $\mu>1$ and
	$\frac{9}{14}\frac{1}{t+\gamma}$ if $\mu<1$, so that $\beta_{t-1}\leq\alpha_{t}$
	and $(1-\mu\alpha_{t})(1+14\beta_{t-1})^{2}\leq(1-\mu\alpha_{t})(1+\mu\alpha_{t})$.
	Moreover, $\|\overline{y}_{t-1}-w^{\ast}\|\leq2\|\overline{y}_{t}-w^{\ast}\|$.
	Recall that we require $\alpha_{t_{0}}\leq\alpha_{t}$ for any $t-t_{0}\leq E-1$,
	and $L\alpha_{t}\leq\frac{1}{8}$. 
	
	Then the bound for $\mathbb{E}\|\overline{y}_{t+1}-w^{\ast}\|^{2}$
	can be further simplified with 
	\begin{align*}
	2\beta_{t-1}(1+\beta_{t-1})(1-\alpha_{t}\mu)\|\overline{y}_{t}-w^{\ast}\|\cdot\|\overline{y}_{t-1}-w^{\ast}\| & \leq4\beta_{t-1}(1+\beta_{t-1})(1-\alpha_{t}\mu)\|\overline{y}_{t}-w^{\ast}\|^{2}
	\end{align*}
	and 
	\begin{align*}
	(1-\alpha_{t}\mu)\beta_{t-1}^{2}\|(\overline{y}_{t-1}-w^{\ast})\|^{2} & \leq4(1-\alpha_{t}\mu)\beta_{t-1}^{2}\|(\overline{y}_{t}-w^{\ast})\|^{2}
	\end{align*}
	Thus 
	\begin{align*}
	\mathbb{E}\|\overline{y}_{t+1}-w^{\ast}\|^{2} & \leq\mathbb{E}(1-\mu\alpha_{t})((1+\beta_{t-1})^{2}+4\beta_{t-1}(1+\beta_{t-1})+4\beta_{t-1}^{2})\|(\overline{y}_{t}-w^{\ast})\|^{2}\\
	& +20E^{2}L\alpha_{t}^{3}G^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}\\
	& \leq\mathbb{E}(1-\mu\alpha_{t})(1+14\beta_{t-1})\|(\overline{y}_{t}-w^{\ast})\|^{2}+20E^{2}L\alpha_{t}^{3}G^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{max}^{2}\sigma^{2}\\
	& \leq b(1-\mu\alpha_{t})^{2}(C\alpha_{t}^{2}+D\alpha_{t})+C\alpha_{t}^{3}+D\alpha_{t}^{2}\\
	& =(b(1-\mu\alpha_{t})^{2}+\alpha_{t})\alpha_{t}^{2}C+(b(1-\mu\alpha_{t})^{2}+\alpha_{t})\alpha_{t}D
	\end{align*}
	and so it remains to choose $b$ such that 
	\begin{align*}
	(b(1-\mu\alpha_{t})^{2}+\alpha_{t})\alpha_{t} & \leq b\alpha_{t+1}\\
	(b(1-\mu\alpha_{t})^{2}+\alpha_{t})\alpha_{t}^{2} & \leq b\alpha_{t+1}^{2}
	\end{align*}
	
	We have 
	\begin{align*}
	(b(1-\mu\alpha_{t})^{2}+\alpha_{t})\alpha_{t} & =(b(1-\frac{4}{t+\gamma})^{2}+\frac{4}{\mu(t+\gamma)})\frac{4}{\mu(t+\gamma)}\\
	& =(b\frac{(t+\gamma-4)^{2}}{(t+\gamma)^{2}}+\frac{4}{\mu(t+\gamma)})\frac{4}{\mu(t+\gamma)}\\
	& =b(\frac{(t+\gamma-4)^{2}+t+\gamma}{(t+\gamma)^{2}})\frac{4}{\mu(t+\gamma)}\\
	& =b\frac{4}{\mu(t+\gamma+1)}
	\end{align*}
	where we have used 
	\begin{align*}
	\frac{(t+\gamma-4)^{2}+t+\gamma}{(t+\gamma)^{2}}\frac{1}{t+\gamma} & \leq\frac{1}{t+\gamma+1}
	\end{align*}
	Similarly $(b(1-\mu\alpha_{t})^{2}+\alpha_{t})\alpha_{t}^{2}\leq b\alpha_{t+1}^{2}$.
	It follows that $\mathbb{E}\|\overline{y}_{t}-w^{\ast}\|^{2}\leq b(C\alpha_{t}^{2}+D\alpha_{t})$
	for all $t$. Thus 
	\begin{align*}
	\mathbb{E}(F(\overline{y}_{T}))-F^{\ast} & =O(\frac{\kappa}{\mu}\frac{1}{N}\nu_{max}^{2}\sigma^{2}\cdot\frac{1}{T}+\frac{\kappa}{\mu^{2}}E^{2}LG^{2}\cdot\frac{1}{T^{2}})
	\end{align*}
	
	With partial participation, the update at each communication round
	is now given by averages over a subset of sampled devices. The key
	is to bound 
	\begin{align*}
	\mathbb{E}\|\overline{w}_{t}-\overline{y}_{t}\|^{2} & =\frac{1}{K}\sum_{k}p^{k}\|w_{t}^{k}-\sum_{k}p_{k}w_{t}^{k}\|^{2}
	\end{align*}
	This can be bounded by the same argument as before and yield 
	\begin{align*}
	\mathbb{E}\|\overline{w}_{t}-\overline{y}_{t}\|^{2} & \leq\frac{4}{K}\alpha_{t}^{2}E^{2}G^{2}
	\end{align*}
	which results in the bound 
	\begin{align*}
	\mathbb{E}F(\overline{w}_{T})-F^{\ast}=O(\frac{\kappa\nu_{\max}^{2}\sigma^{2}/\mu}{NT}+\frac{\kappa E^{2}G^{2}/\mu}{KT}+\frac{\kappa^{2}E^{2}G^{2}/\mu}{T^{2}})
	\end{align*}
\end{proof}