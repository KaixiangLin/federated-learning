
\begin{theorem}
	Suppose $F_{k}$ is $L$-smooth and convex for all $k$, and let $\nu_{\max}=\max_{k}\frac{1}{N}p_{k}$.
	Set learning rate $\alpha_{t}=O(\sqrt{\frac{N}{T}})$.
	
	With full device participation, 
	\begin{align*}
	\min_{t\leq T}F(\overline{w}_{t})-F(w^{\ast}) & =O(\sqrt{\frac{\nu_{\max}^{2}\sigma^{2}}{NT}+\frac{E^{2}LG^{2}}{T^{4/3}}})
	\end{align*}
	and with partial device participation with $K$ sampled devices at
	each communication round, 
	\begin{align*}
	\min_{t\leq T}F(\overline{w}_{t})-F(w^{\ast}) & =O(\sqrt{\frac{\nu_{\max}^{2}\sigma^{2}}{NT}+\frac{E^{2}G^{2}}{KT}+\frac{E^{2}LG^{2}}{T^{4/3}}})
	\end{align*}
\end{theorem}

\begin{proof}
	We again start by bounding the term
	\begin{align*}
	\|\overline{w}_{t+1}-w^{\ast}\|^{2} & =\|(\overline{w}_{t}-\alpha_{t}g_{t})-w^{\ast}\|^{2}\\
	& =\|(\overline{w}_{t}-\alpha_{t}\overline{g}_{t}-w^{\ast})-\alpha_{t}(g_{t}-\overline{g}_{t})\|^{2}\\
	& =A_{1}+A_{2}+A_{3}
	\end{align*}
	where 
	\begin{align*}
	A_{1} & =\|\overline{w}_{t}-w^{\ast}-\alpha_{t}\overline{g}_{t}\|^{2}\\
	A_{2} & =2\alpha_{t}\langle\overline{w}_{t}-w^{\ast}-\alpha_{t}\overline{g}_{t},\overline{g}_{t}-g_{t}\rangle\\
	A_{3} & =\alpha_{t}^{2}\|g_{t}-\overline{g}_{t}\|^{2}
	\end{align*}
	$\mathbb{E}A_{2}=0$ by definition of $g_{t}$ and $\overline{g}_{t}$,
	while for $A_{3}$ we have
	\begin{align*}
	\alpha_{t}^{2}\mathbb{E}\|g_{t}-\overline{g}_{t}\|^{2} & =\alpha_{t}^{2}\mathbb{E}\|g_{t}-\mathbb{E}g_{t}\|^{2}=\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\|g_{t,k}-\mathbb{E}g_{t,k}\|^{2}\leq\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}
	\end{align*}
	again by Jensen's inequality and using the independence of $g_{t,k},g_{t,k'}$. 
	
	Next we bound $A_{1}$: 
	\begin{align*}
	\|\overline{w}_{t}-w^{\ast}-\alpha_{t}\overline{g}_{t}\|^{2} & =\|\overline{w}_{t}-w^{\ast}\|^{2}+2\langle\overline{w}_{t}-w^{\ast},-\alpha_{t}\overline{g}_{t}\rangle+\|\alpha_{t}\overline{g}_{t}\|^{2}
	\end{align*}
	Using the convexity and $L$-smoothness of $F_{k}$, 
	\begin{align*}
	-2\alpha_{t}\langle\overline{w}_{t}-w^{\ast},\overline{g}_{t}\rangle & =-2\alpha_{t}\sum_{k=1}^{N}p_{k}\langle\overline{w}_{t}-w^{\ast},\nabla F_{k}(w_{t}^{k})\rangle\\
	& =-2\alpha_{t}\sum_{k=1}^{N}p_{k}\langle\overline{w}_{t}-w_{t}^{k},\nabla F_{k}(w_{t}^{k})\rangle-2\alpha_{t}\sum_{k=1}^{N}p_{k}\langle w_{t}^{k}-w^{\ast},\nabla F_{k}(w_{t}^{k})\rangle\\
	& \leq-2\alpha_{t}\sum_{k=1}^{N}p_{k}\langle\overline{w}_{t}-w_{t}^{k},\nabla F_{k}(w_{t}^{k})\rangle+2\alpha_{t}\sum_{k=1}^{N}p_{k}(F_{k}(w^{\ast})-F_{k}(w_{t}^{k}))\\
	& \leq2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(w_{t}^{k})-F_{k}(\overline{w}_{t})+\frac{L}{2}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+F_{k}(w^{\ast})-F_{k}(w_{t}^{k})\right]\\
	& =\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(w^{\ast})-F_{k}(\overline{w}_{t})\right]
	\end{align*}
	which results in 
	\begin{align*}
	\|\overline{w}_{t+1}-w^{\ast}\|^{2} & \leq\|\overline{w}_{t}-w^{\ast}\|^{2}+\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(w^{\ast})-F_{k}(\overline{w}_{t})\right]+\alpha_{t}^{2}\|\overline{g}_{t}\|^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}
	\end{align*}
	
	The difference of this bound with that in the strongly convex case
	is that we no longer have a contraction factor in front of $\|\overline{w}_{t}-w^{\ast}\|^{2}$.
	In the strongly convex case, we were able to cancel $\alpha_{t}^{2}\|\overline{g}_{t}\|^{2}$
	with $2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(w^{\ast})-F_{k}(\overline{w}_{t})\right]$
	and obtain only lower order terms. In the convex case, we use a different
	strategy and preserve $\sum_{k=1}^{N}p_{k}\left[F_{k}(w^{\ast})-F_{k}(\overline{w}_{t})\right]$
	in order to obtain a telescoping sum. 
	
	We have
	\begin{align*}
	\|\overline{g}_{t}\|^{2} & =\|\sum_{k}p_{k}\nabla F_{k}(w_{t}^{k})\|^{2}\\
	& =\|\sum_{k}p_{k}\nabla F_{k}(w_{t}^{k})-\sum_{k}p_{k}\nabla F_{k}(\overline{w}_{t})+\sum_{k}p_{k}\nabla F_{k}(\overline{w}_{t})\|^{2}\\
	& \leq2\|\sum_{k}p_{k}\nabla F_{k}(w_{t}^{k})-\sum_{k}p_{k}\nabla F_{k}(\overline{w}_{t})\|^{2}+2\|\sum_{k}p_{k}\nabla F_{k}(\overline{w}_{t})\|^{2}\\
	& \leq2L^{2}\sum_{k}p_{k}\|w_{t}^{k}-\overline{w}_{t}\|^{2}+2\|\sum_{k}p_{k}\nabla F_{k}(\overline{w}_{t})\|^{2}\\
	& =2L^{2}\sum_{k}p_{k}\|w_{t}^{k}-\overline{w}_{t}\|^{2}+2\|\nabla F(\overline{w}_{t})\|^{2}
	\end{align*}
	using $\nabla F(w^{\ast})=0$. Now using the $L$ smoothness of $F$,
	we have $\|\nabla F(\overline{w}_{t})\|^{2}\leq2L(F(\overline{w}_{t})-F(w^{\ast}))$,
	so that 
	\begin{align*}
	\|\overline{w}_{t+1}-w^{\ast}\|^{2} & \leq\|\overline{w}_{t}-w^{\ast}\|^{2}+\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(w^{\ast})-F_{k}(\overline{w}_{t})\right]\\
	& +2\alpha_{t}^{2}L^{2}\sum_{k}p_{k}\|w_{t}^{k}-\overline{w}_{t}\|^{2}+4\alpha_{t}^{2}L(F(\overline{w}_{t})-F(w^{\ast}))+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}\\
	& =\|\overline{w}_{t}-w^{\ast}\|^{2}+(2\alpha_{t}^{2}L^{2}+\alpha_{t}L)\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(w^{\ast})-F_{k}(\overline{w}_{t})\right]+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}\\
	& +\alpha_{t}(1-4\alpha_{t}L)(F(w^{\ast})-F(\overline{w}_{t}))
	\end{align*}
	Since $F(w^{\ast})\leq F(\overline{w}_{t})$, as long as $4\alpha_{t}L\leq1$,
	we can ignore the last term, and rearrange the inequality to obtain
	\begin{align*}
	\|\overline{w}_{t+1}-w^{\ast}\|^{2}+\alpha_{t}(F(\overline{w}_{t})-F(w^{\ast})) & \leq\|\overline{w}_{t}-w^{\ast}\|^{2}+(2\alpha_{t}^{2}L^{2}+\alpha_{t}L)\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}\\
	& \leq\|\overline{w}_{t}-w^{\ast}\|^{2}+\frac{3}{2}\alpha_{t}L\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}
	\end{align*}
	
	The same argument as before yields $\mathbb{E}\sum_{k=1}^{N}p_{k}\|\overline{w}_{t}-w_{t}^{k}\|^{2}\leq4E^{2}\alpha_{t}^{2}G^{2}$
	which gives 
	\begin{align*}
	\|\overline{w}_{t+1}-w^{\ast}\|^{2}+\alpha_{t}(F(\overline{w}_{t})-F(w^{\ast})) & \leq\|\overline{w}_{t}-w^{\ast}\|^{2}+\alpha_{t}^{2}\sum_{k=1}^{N}p_{k}^{2}\sigma_{k}^{2}+6\alpha_{t}^{3}E^{2}LG^{2}\\
	& \leq\|\overline{w}_{t}-w^{\ast}\|^{2}+\alpha_{t}^{2}\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+6\alpha_{t}^{3}E^{2}LG^{2}
	\end{align*}
	Summing the inequalities from $t=0$ to $t=T$, we obtain 
	\begin{align*}
	\sum_{t=0}^{T}\alpha_{t}(F(\overline{w}_{t})-F(w^{\ast})) & \leq\|w_{0}-w^{\ast}\|^{2}+\sum_{t=0}^{T}\alpha_{t}^{2}\cdot\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+\sum_{t=0}^{T}\alpha_{t}^{3}\cdot6E^{2}LG^{2}
	\end{align*}
	so that
	\begin{align*}
	\min_{t\leq T}F(\overline{w}_{t})-F(w^{\ast}) & \leq\frac{1}{\sum_{t=0}^{T}\alpha_{t}}\left(\|w_{0}-w^{\ast}\|^{2}+\sum_{t=0}^{T}\alpha_{t}^{2}\cdot\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+\sum_{t=0}^{T}\alpha_{t}^{3}\cdot6E^{2}LG^{2}\right)
	\end{align*}
	
	By setting the constant learning rate $\alpha_{t}\equiv\sqrt{\frac{\|w_{0}-w^{\ast}\|^{2}}{\frac{1}{N}\nu_{\max}^{2}\sigma^{2}T+6E^{2}LG^{2}T^{2/3}}}=O(\sqrt{\frac{N}{T}})$,
	we have 
	
	\begin{align*}
	\min_{t\leq T}F(\overline{w}_{t})-F(w^{\ast}) & \leq\frac{1}{T\sqrt{\frac{\|w_{0}-w^{\ast}\|^{2}}{\frac{1}{N}\nu_{\max}^{2}\sigma^{2}T+6E^{2}LG^{2}T^{2/3}}}}\cdot\left(\|w_{0}-w^{\ast}\|^{2}+T\frac{\|w_{0}-w^{\ast}\|^{2}}{\frac{1}{N}\nu_{\max}^{2}\sigma^{2}T}\frac{1}{N}\nu_{\max}^{2}\sigma^{2}+6E^{2}LG^{2}T\frac{\|w_{0}-w^{\ast}\|^{2}}{6E^{2}LG^{2}T}\right)\\
	& =\frac{3\|w_{0}-w^{\ast}\|^{2}}{T\sqrt{\frac{\|w_{0}-w^{\ast}\|^{2}}{\frac{1}{N}\nu_{\max}^{2}\sigma^{2}T+6E^{2}LG^{2}T^{2/3}}}}=3\|w_{0}-w^{\ast}\|\sqrt{(\frac{\nu_{\max}^{2}\sigma^{2}}{NT}+\frac{6E^{2}LG^{2}}{T^{4/3}})}
	\end{align*}
	
	Similarly, for partial participation, we just need to replace $\frac{\nu_{\max}^{2}\sigma^{2}}{NT}$
	with $\frac{\nu_{\max}^{2}\sigma^{2}}{NT}+C\frac{E^{2}G^{2}}{KT}$,
	so that we have 
	\begin{align*}
	\min_{t\leq T}F(\overline{w}_{t})-F(w^{\ast}) & =O(\sqrt{\frac{\nu_{\max}^{2}\sigma^{2}}{NT}+\frac{E^{2}G^{2}}{KT}+\frac{E^{2}LG^{2}}{T^{4/3}}})
	\end{align*}
\end{proof}