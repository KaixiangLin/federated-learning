{\color{blue}\textbf{Reviewer 3:}} We thank the reviewer for the valuable feedback. Your concerns are addressed as follows: \\
\textbf{\textit{1.Comparison with SCAFFOLD.}}: 
\begin{itemize}
	\item The analyses in SCAFFOLD do not imply $E=\cO(\sqrt{T})$ but in fact also require $E=O(1)$ under partial participation (sampling). To see this, it is easier to examine Theorem V in their paper, which is stated in terms of optimality gaps, but otherwise equivalent to their Theorem I. Notice that when $S<N$, the second term in $M^2$ gives rise to a term $O(1/R)$ in the bound for the strongly convex case (ignoring constants), and since $R=T/E$ in our notation, this term is $O(E/T)$. Therefore, in order to achieve a $O(1/T)$ convergence rate for the optimality gap, it must be the case that $E=O(1)$ as well. Similarly for the general convex case. Thus in terms of communication complexity, our results imply the same requirements for both full and partial participation as that in SCAFFOLD. 
	\item Furthermore, the sampling procedure analyzed in our paper is different from that in SCAFFOLD, as we allow the sampling probability to scale with device specific weights and sample with replacement, whereas in SCAFFOLD the sampling is uniform without replacement. 
	\item  Under our sampling schemes, we explicitly analyze the contribution of sampling variance to the optimality gap, and in Lemma 10 of our paper in the Appendix, we provided a problem instance that lower bounds the sampling variance, showing that $E=O(1)$ cannot be improved in general when there is partial participation with sampling.
\end{itemize}

\textbf{\textit{2. The convergence of Nestrov Accelerated FedAvg.}}
As there has not been any linear speedup analysis of Nesterov accelerated FedAvg for convex problems, our result is a first in this regard and completes the picture. Our analyses of FedAvg and Nesterov FedAvg are also unified, highlighting the common elements and distinctions for the two algorithms, which has not been done by previous studies. 

\textbf{\textit{3. The geometrical rates of overparametrized linear regression.}}
The geometric rates in Theorem 5 are for general overparameterized strongly convex objectives rather than just linear regression, and as SCAFFOLD is a variance reduction based algorithm, our result, which is on the FedAvg algorithm, is not directly comparable to the geometric convergence result in SCAFFOLD.