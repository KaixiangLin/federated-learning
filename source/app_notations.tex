% !TEX ROOT=./main.tex

\section{Additional Notations and Bounds for Sampling Schemes}
\label{sec:app:notations}
In this section, we introduce additional notations that are used throughout
the proofs. Following common practice, e.g.~\cite{stich2018local,li2019convergence}, we define two virtual sequences $\overline{\mathbf{v}}_{t}=\sum_{k=1}^{N} p_{k} \mathbf{v}_{t}^{k}$ and $\overline{\mathbf{w}}_{t}=\sum_{k=1}^{N} p_{k} \mathbf{w}_{t}^{k}$, where we recall the FedAvg updates from \eqref{eq:fedavg updates}:
\begin{align*}
\vv_{t+1}^{k} & =\vw_{t}^{k}-\alpha_{t}\vg_{t,k}, \hspace{1em}
\mathbf{w}_{t+1}^{k} =\left\{
\begin{array}{ll}
\mathbf{v}_{t+1}^{k} & \text { if } t+1 \notin \mathcal{I}_{E}, \\ 
\sum_{k \in \cS_{t+1}} q_k \mathbf{v}_{t+1}^{k} & \text { if } t+1 \in \mathcal{I}_{E}.
\end{array}\right.
\end{align*}
The following observations apply to FedAvg updates, while Nesterov accelerated FedAvg requires modifications. For full device participation or partial participation with $t \notin \cI_E$, note that
$\ov{v}_t = \ov{w}_t =\sum_{k=1}^{N} p_{k} \mathbf{v}_{t}^{k}$. For partial participation with $t \in \cI_E$, $\ov{w}_t \neq \ov{v}_t$ since $\ov{v}_t=\sum_{k=1}^{N} p_{k} \mathbf{v}_{t}^{k}$ while $\ov{w}_t=\sum_{k\in \cS_t}q_k\mathbf{w}_{t}^{k}$. However, we can
use unbiased sampling strategies such that $ \EE_{\cS_t} \ov{w}_t = \ov{v}_t$.
Note that $\overline{\mathbf{v}}_{t+1}$ is one-step SGD from $\overline{\mathbf{w}}_{t}$. 
\begin{align}
\overline{\mathbf{v}}_{t+1}=\overline{\mathbf{w}}_{t}-\alpha_{t} \mathbf{g}_{t},	\label{eq:vbar}
\end{align}
% $t+1 \in \cI_E$, we can fetch $\overline{\vw}_{t+1}$, we can communicate $\overline{\vw}_{t+1}$ to all devices.
where $\vg_{t} = \sum_{k=1}^{N} p_{k} \vg_{t,k} $ is the one-step stochastic gradient averaged over all devices. 
\begin{align*}
\vg_{t,k} = \nabla F_{k}\left(\mathbf{w}_{t}^{k},\mathbf{\xi}_{t}^{k} \right),   
\end{align*}
% \begin{align}
% \vg_{t,k} \left\{\begin{array}{ll} 
%  = \nabla F_{k}\left(\mathbf{w}_{t}^{k},\mathbf{\xi}_{t}^{k} \right)  &  \text{smooth}\\
%  \in \partial F_{k}\left(\vw_{t}^{k}, \mathbf{\xi}_{t}^{k}\right)  & \text{non-smooth} \lkxcom{remove non-smooth}
%  \end{array}\right.
% \end{align}
Similarly, we denote the expected one-step gradient $\ov{g}_{t}= \EE_{\xi_t}[\vg_t] = \sum_{k=1}^{N} p_{k} \EE_{\mathbf{\xi}_{t}^{k}} \vg_{t,k}$, where
\begin{align}
\EE_{\mathbf{\xi}_{t}^{k}} \vg_{t,k}  = \nabla F_{k}\left(\mathbf{w}_{t}^{k}\right), 
\label{eq:gradient}
\end{align}
% \begin{align}
% \EE_{\mathbf{\xi}_{t}^{k}} \vg_{t,k}  \left\{\begin{array}{ll} 
%  = \nabla F_{k}\left(\mathbf{w}_{t}^{k}\right)  &  \text{smooth}\\
%  \in \partial F_{k}\left(\vw_{t}^{k}\right)  & \text{non-smooth}
%  \end{array}\right.
% \end{align}
and $\mathbf{\xi}_t = \{\mathbf{\xi}_t^k\}_{k=1}^N$ denotes random samples at all devices at time step $t$. \\
Since in this work we also consider the case of partial participation, the sampling strategy to approximate the system heterogeneity can also affect the convergence. Here we
follow the prior works~\cite{li2019convergence} and~\cite{li2018federated} and consider two types of sampling
schemes that guarantee $ \EE_{\cS_t} \ov{w}_t = \ov{v}_t$. 
The sampling scheme I establishes $\cS_{t+1}$ by \emph{i.i.d.} sampling the devices according to probabilities $p_k$ with replacement, and setting $q_k=\frac{1}{K}$.
In this case the upper bound of expected square norm of $\ov{w}_{t+1} - \ov{v}_{t+1}$ is given by~\cite[Lemma 5]{li2019convergence}:
\begin{align}
\EE_{\cS_{t+1}}\left\|\ov{w}_{t+1} - \ov{v}_{t+1}\right\|^2	\leq \frac{4}{K} \alpha_t^2 E^2G^2.
\end{align}
The sampling scheme II establishes $\cS_{t+1}$ by uniformly sampling all devices without
replacement and setting $q_k=p_k\frac{N}{K}$, in which case we have
\begin{align}
\EE_{\cS_{t+1}}\left\|\ov{w}_{t+1} - \ov{v}_{t+1}\right\|^2	\leq \frac{4(N - K)}{K(N-1)} \alpha_t^2 E^2G^2.
\end{align}
We summarize these upper bounds as follows: 
\begin{align}
	\EE_{\cS_{t+1}}\left\|\ov{w}_{t+1} - \ov{v}_{t+1}\right\|^2 \leq  \frac{4}{K} \alpha_t^2 E^2G^2.
	\label{eq:partialsample}
\end{align}
and this bound will be used in the convergence proof of the partial participation result.