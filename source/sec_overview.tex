% !TEX ROOT=./main.tex


\begin{table}[h!]
\centering
\small
	\begin{tabular}{|c|c|c|c|c|}\hline
		paper         &  Cvx Non-smth & Cvx L-smth & Strongly Cvx Non-smth& Strongly Cvx L-smth \\ \hline
	Accelerated	SGD   &        &    &       &      \\\hline
	SGD    &    $\cO(\frac{1}{\sqrt{T}})$  &   $\cO(\frac{1}{\sqrt{T}})$  &  $\cO(\frac{1}{T})$  & $\cO(\frac{1}{T})$\cite{li2019convergence,haddadpour2019convergence}      \\\hline
	\end{tabular}
	\caption{Summarize of related work on federated learning, heterogeneous data.}
\end{table}


\section{Stochastic Gradient decent}

% 	Stochastic gradient of device $k$ at time step $t$, at point $\vw_t^k$: $$\vg_{t,k} \coloneqq \vg_{t,k}(w_t^k)$$
% 	$$ \vg_{t, k} = \grad F_{k}\left(w_{t}^{k}, \xi_{t}^{k}\right) $$
% 	$$\EE \vg_{t, k} = \grad F_{k}\left(w_{t}^{k}\right)$$
%   One-step stochastic subgradient of all devices.
% $$\vg_{t}=\sum_{k=1}^{N} p_{k} \vg_{t, k}\left(w_{t}^{k}\right) $$
% \begin{align}
% 	\EE \vg_{t}= \EE \sum_{k=1}^{N} p_{k} \vg_{t, k}\left(w_{t}^{k}\right) \coloneqq \sum_{k=1}^{N} p_{k} \EE \vg_{t, k}
% 	\label{eq:egtsgd}
% \end{align}



\subsection{Stochastic gradient descent}
\subsubsection{Convex}
\input{sec_convexsmoothsgd}

\subsubsection{Strongly Convex}
\cite{li2019convergence}

\subsection{Stochastic Subgradient methods}

\subsubsection{Convex}
\input{sec_cvxnonsm}
\subsubsection{Strongly Convex}
\input{sec_sgdscvxnonsmth}



\section{Accelerated methods}
\subsection{stochastic gradient descent}
\subsubsection{Convex}
\subsubsection{Strongly Convex}
\input{sec_nasgd_scvx_smooth}

\subsection{Stochastic Subgradient methods}

\subsubsection{Convex}

\subsubsection{Strongly Convex}

\input{sec_nagcvxnonsmth}







