% !TEX ROOT=./main.tex

\section{Conclusions}
This paper provides a comprehensive analysis of the convergence rate of FedAvg
and its accelerated variants. We show that both accelerated FedAvg and FedAvg
can achieve speedup when the number of nodes increases, i.e., $O(1/\sqrt{NT})$
convergence for convex smooth problems and $O(1/NT)$ convergence for strongly 
convex problems. Furthermore, we show FedAvg can achieve exponential 
convergence for overparameterized strongly convex smooth problems, and we propose Mass accelerated Fedavg, which show improved convergence rate over
FedAvg on the linear regression problem. Last but not least, we empirically
verify the linear speedup of FedAvg and Nesterov accelerated FedAvg for strongly convex, convex smooth, and linear regression problems. The empirical results are well-aligned with our theories. 