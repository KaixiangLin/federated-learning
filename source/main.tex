\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[nonatbib]{neurips_2020}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsthm}     % for theorems
\usepackage{algorithm,algorithmic}
\usepackage{mathtools}
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{amssymb}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\usepackage{balance}
\usepackage{comment}
\newcommand{\eq}[1]{{Eq~(#1)}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newtheorem{theorem}{Theorem}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}{Remark}
\newcommand{\lkxcom}[1]{{\color{red}{#1}}}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\ov}[1]{{\overline{\mathbf{#1}}}}
\input{bmacros}
\newtheorem*{assumption*}{Assumption}

\title{Federated Learning's Blessing:\\
FedAvg has Linear Speedup}

\author{}

\begin{document}

\maketitle

\begin{abstract}
Federated learning learns
a model jointly from a set of distributed workers without sharing their private
data. The characteristics of non-\emph{i.i.d.} data
across the network and low device participation bring significant challenges
in understanding the convergence of federated learning algorithms. Prior work
analyzed the convergence with restrained settings,
%either \emph{i.i.d.} data, or with full device participation, 
and often with suboptimal rates. 
In this work, we provide a
comprehensive convergence analysis on Federated Averaging (FedAvg),
a widely used framework in which SGD or Nesterov SGD updates are used for general convex problems with arbitrarily
heterogeneous data. 
We first show that the convergence rates using both updates enjoy a linear speedup w.r.t. the number of participating workers for all convex smooth problems. 
In addition, in the overparamterized setting, we show that the convergence of FedAvg is in fact exponential, also with a speedup factor linear in the number of workers. 
Furthermore, we propose a new accelerated federated learning algorithm that further improves this convergence rate with provable guarantees in the linear regression setting. 
Empirical studies of the algorithms in various settings have supported our theoretical results.
\end{abstract}

\input{sec_intro}
%\input{sec_relatedwork}
\input{sec_problem}
\input{sec_SGD}
\input{sec_Nesterov}
\input{sec_overparameterized}
\input{sec_exp}

\input{sec_conclude}


\newpage
\input{sec_impact}
\bibliographystyle{unsrt}
\bibliography{ref.bib}

\appendix
\input{sec_appendix}
\end{document}



\iffalse

\endif