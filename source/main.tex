\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[nonatbib]{neurips_2020}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsthm}     % for theorems
\usepackage{algorithm,algorithmic}
\usepackage{mathtools}
\usepackage{amsmath}
\allowdisplaybreaks
\usepackage{amssymb}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\usepackage{balance}
\usepackage{comment}
\newcommand{\eq}[1]{{Eq~(#1)}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newtheorem{theorem}{Theorem}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}{Remark}
\newcommand{\lkxcom}[1]{{\color{red}{#1}}}

\newcommand{\ov}[1]{{\overline{\mathbf{#1}}}}
\input{bmacros}
\newtheorem*{assumption*}{Assumption}

\title{The Blessing of FedAvg's Linear Speedup in Federated Learning}

\author{}

\begin{document}

\maketitle

\begin{abstract}
Federated learning is a distributed learning paradigm that enables learning
a model jointly over a large number of workers without sharing the private
data on local workers. The characteristics of non-identically distributed data
across the network and low device participation brings significant challenges
in analyzing the convergence of federated learning algorithms. Prior works
have either analyzed the convergence with i.i.d. data, or with full device
participation, and often with suboptimal rates. In this work, we provide a
comprehensive analysis of the convergence rate of Federated Averaging (FedAvg)
with SGD and Nesterov SGD updates for general convex problems with arbitrarily
heterogeneous data. Firstly we show that the convergence rates enjoy a linear
speedup in the number of participating workers for all convex smooth problems
for both algorithms.  Furthermore, in the overparamterized setting, we prove that the convergence under FedAvg is in fact exponential, also with a speedup factor linear in the number of workers. We also propose a new accelerated federated learning algorithm that further improves this convergence rate with provable guarantees in the linear regression setting. Empirical evaluation of the algorithms in various settings match our theoretical results.
\end{abstract}

\input{sec_intro}
\input{sec_problem}
\input{sec_strongly_convex_smooth}
\input{sec_convex_smooth}
\input{sec_overparameterized}
\input{sec_exp}



\input{sec_impact}
\newpage
\bibliographystyle{plain}
\bibliography{ref.bib}

\appendix
\input{sec_appendix}
\end{document}
