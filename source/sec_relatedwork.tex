% !TEX ROOT=./main.tex

\section{Related Works}

% related work on convergence rate
Federated learning (FL) was originally proposed
in~\cite{mcmahan2016communication} for learning a single, global statistical
model from the isolated data stored in a massive number of devices.  The
empirical success of FL~\cite{chen2018federated,47586} has attracted much attention to the rigorous theoretical understanding of the leading algorithm in the field: Federated Averaging (FedAvg) and its accelerated variants~\cite{liu2019accelerating,haddadpour2019convergence,khaled2019first,li2019convergence,huo2020faster,yu2019linear,yu2019parallel}.

% Non-convex linear speedup
In particular, we focus on the convergence analysis of FedAvg, which
has been discussed in various settings. 
In~\cite{yu2019parallel,wang2019adaptive}, the authors provides convergence
rate of FedAvg for non-convex problem, given all the devices are active at
each communication round. Recently, \citep{huo2020faster} provide 
convergence guarantee for Nesterov accelerated FedAvg for non-convex 
setting in partial participation setting. Although the empirical
results is improved over FedAvg, the accelerated version admitting a similar 
convergence rate as FedAvg. In this work, we provide an novel accelerated
FedAvg with improved convergence rate under a popular overparameterized 
setting. In addition, \cite{yu2019linear} provides $O(1/\sqrt{NT})$ 
convergence (i.e., linear speedup) of FedAvg for non-convex problem in the full participation setting. 
For convex smooth problems, \cite{khaled2019first} prove the 
convergence rate of local GD (a full batch version of FedAvg with full participation) on heterogeneous data. 
Concurrently, \cite{li2019convergence} firstly provide exponential convergence of FedAvg for strongly convex problem,
under the more practical setting of partial participation and Non-IID data,
while their results did not achieve linear speedup. 
\cite{stich2018local} prove linear speedup convergence of FedAvg for strongly convex problem, while they assume full participation and i.i.d. data.
Comparing to prior work, our results provide the linear speedup on strongly convex problem in the practical partial participation setting and heterogeneous data. In \cite{liu2019accelerating}, the authors provide convergence rate of accelerated FedAvg for stronlgy convex problem, while it is considered in full participation setting. To the best of our knowledge,
we provide the first linear speedup convergence results for 
accelerated FedAvg in practical setting.









