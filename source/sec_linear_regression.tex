% !TEX ROOT=./main.tex



\section{Exponential Convergence of FedAvg in the Overparameterized Setting}

In contrast to the gradient descent setting, in the stochastic gradient
setting, Nesterov update is known to fail to accelerate over stochastic
gradient. Thus in general we cannot hope to obtain acceleration results
for the FedAvg algorithm with Nesterov updates. However, in special
problem settings, there have been a number of works showing that accelerated
stochastic gradient descent methods achieves better rates than stochastic
gradient descent. One such case is the so-called interpolation setting,
where the objective has an optimal value of 0, e.g. due to overparameterization.
The works of Belkin et.al have shown that under interpolation, SGD
achieves exponential convergence under constant step-size, thanks
to the automatic variance reduction property that is not available
in generic optimization problems. In this section, we extend their
result to the federated learning problem and prove the exponential
convergence of SGD for a class of convex objectives in the interpolation
setting.

In the non-federated learning setting, {[}TODO: CITE{]} have provided
negative results on the acceleration of Nesterov and Heavy Ball updates
over plain SGD for certain problems. In {[}Liu\&Belkin{]}, the authors
show that in the interpolation setting, stochastic Nesterov update
is not able to achieve accleration over vanilla SGD. They introduce
the MaSS algorithm, which is a variant of the Nesterov acceleration,
where in the update a non-negative multiple of the gradient is added
to the Nesterov parameter update to correct for the ``over-descent''
of the Nesterov update. They show that the MaSS algorithm is able
to achieve acceleration over the exponential convergence of SGD both
theoretically and empirically.

The next natural question is then whether similar conclusions can
be drawn in the federated learning setting. We demonstrate that this
is possible by extending the results of Belkin et al. to show that
for the federated learning problem in the interpolation setting, Nesterov
SGD cannot in general achieve acceleration over plain SGD. We then
adapt the MaSS algorithm to the federated learning setting, and prove
that for certain class of convex objectives with 0 global objective
value, the FedAvg algorithm with MaSS updates achieves exponential
convergence and acceleration of convergence rate over SGD. 

We first show that FedAvg with SGD has exponential convergence for
linear regression when the global objective has 0 as its global minimum.

\subsection{Exponential Convergence of FedAvg with SGD}

In Belkin et al., the authors show that in the interpolation setting,
SGD achieves exponential convergence with linear speedup batch size
that is below some critical threshold. In this section, we extend
their result to the federated learning setting and show that similar
exponential convergence with linear speedup in the number of workers
hold using FedAvg with SGD updates holds. 

We consider the standard setting of empirical risk minimization where
$\ell(w;\xi_{k,j})$ is non-negative, $l$-smooth, and convex, and
as before, each $F_{k}(w)=\frac{1}{n_{k}}\sum_{j=1}^{n_{k}}\ell(w;\xi_{k,j})$
is $L$-smooth and $\mu$-strongly convex. Note that $l\geq L$. This
setting includes many important problems in practice, such as linear
regression with a full rank sample covariance matrix. In the interpolation
or overparameterized setting, we assume that there exists $w^{\ast}\in\arg\min_{w}\sum_{k=1}^{N}p_{k}F_{k}(w)$
such that $\ell(w^{\ast};\xi_{k,j})=0$ for all $\xi_{k,j}$. We first
present a general result on the exponential convergence with linear
speedup in the number of workers, before giving a sharper analysis
for linear regression problems. 
\begin{theorem}
	For the overparameterized setting with general strongly convex and
	smooth objectives, FedAvg with local SGD updates and communication
	every $E$ iterations with constant step size $\overline{\alpha}=\frac{1}{2E}\frac{N}{l\nu_{\max}+L(N-\nu_{\min})}$
	gives the exponential convergence guarantee 
	\begin{align*}
	\mathbb{E}F(\overline{w}_{t}) & \leq\frac{L}{2}(1-\mu\overline{\alpha})^{t}\|w_{0}-w^{\ast}\|^{2}=O(\exp(-\frac{\mu}{2E}\frac{N}{l\nu_{\max}+L(N-\nu_{\min})}t)\cdot\|w_{0}-w^{\ast}\|^{2})
	\end{align*}
\end{theorem}
%
We see that when $l>L$, the speedup factor is on the order of $\frac{N}{E(l\nu_{\max}+L(N-\nu_{\min}))}/\frac{1}{l\nu_{\max}+L(1-\nu_{\min})}\approx\frac{Nl}{E(l+L(N-1))}=O(N/E)$
for $N\leq\frac{l}{L}+1$, i.e. FedAvg with $N$ workers and communication
every $E$ iterations provide an exponential convergence speedup factor
of $O(N/E)$, for $N\leq\frac{l}{L}+1$.

Next we show that specialized to the overparameterized linear regression
setting, the exponential convergence bound can be further sharpened,
with a similar linear speedup in the number of workers. In this setting,
we have 
\begin{align*}
F(w) & =\sum_{k=1}^{N}p_{k}F_{k}(w)\\
F_{k}(w) & =\frac{1}{2n_{k}}\sum_{j=1}^{n_{k}}(w^{T}x_{k,j}-z_{k,j})^{2}
\end{align*}
and there exists $w^{\ast}$ such that $F_{k}(w^{\ast})\equiv0$
for all $k$. 
\begin{theorem}
	(FedAvg with SGD, Linear Regression) For the overparamterized linear
	regression problem, FedAvg with local SGD updates and communication
	every $E$ iterations with constant step size $\overline{\alpha}=\frac{1}{2E}\frac{N}{l\nu_{\max}+L(N-\nu_{\min})}$
	gives the exponential convergence guarantee 
	\begin{align*}
	\mathbb{E}F(\overline{w}_{t}) & \leq L(1-c\frac{N}{E\kappa})^{t}\|w_{0}-w^{\ast}\|^{2}
	\end{align*}
	where $\kappa=L/\mu$ and $c$ is some universal constant. 
\end{theorem}

Next we show FedAvg with MaSS has exponential convergence but with an improved rate. 

\subsection{Notation and Definitions}

Following Liu\&Belkin and Jain et al., we define some condition number
related quantities. Let $H^{k}=\frac{1}{n_{k}}\sum_{j=1}^{n_{k}}x_{k,j}x_{k,j}$
be the Hessian matrix of $F_{k}$. Let $L^{k}$ and $\mu^{k}$ be
any upper bound and lower bound of the non-zero eigenvalues of the
Hessians $H^{k}$. For a mini-batch $\{\tilde{x}_{j}\}_{j=1}^{m}$
of $m$ samples from device $k$, let $\tilde{H}_{m}^{k}=\frac{1}{m}\sum_{j=1}^{m}\tilde{x}_{j}\tilde{x}_{j}^{T}$
be the unbiased mini-batch estimate of $H^{k}$. Let $L_{1}^{k}$
be the smallest positive number such that 
\begin{align*}
\mathbb{E}\left[\|\tilde{x}\|^{2}\tilde{x}\tilde{x}^{T}\right] & \preceq L_{1}^{k}H^{k}
\end{align*}
and define 
\begin{align*}
L_{m}^{k} & =L_{1}^{k}/m+(m-1)L^{k}/m
\end{align*}

Define the $m$-stochastic condition number as $\kappa_{m}^{k}:=L_{m}^{k}/\mu$.
Define $L_{m}=\max_{k}L_{m}^{k}$ and $\kappa_{m}=\max_{k}\kappa_{m}^{k}$.
Define the statistical condition number $\tilde{\kappa}^{k}$ as the
smallest positive real number such that 
\begin{align*}
\mathbb{E}\left[\langle\tilde{x}(H^{k})^{-1},\tilde{x}\rangle\tilde{x}\tilde{x}^{T}\right] & \preceq\tilde{\kappa}^{k}H^{k}
\end{align*}

In order to use hyperparameters that are universal across devices,
we further define $L=\max_{k}L^{k}$, $\mu=\min_{k}\mu^{k}$, $L_{m}=\max_{k}L_{m}^{k}$
and $\kappa_{m}=\max_{k}\kappa_{m}^{k}$. 

\subsection{FedAvg with MaSS Updates}

The FedAvg algorithm with MaSS follows the updates
\begin{align*}
w_{t+1}^{k} & =\begin{cases}
u_{t}^{k}-\eta_{1}^{k}g_{t,k} & \text{if }t+1\notin\mathcal{I}_{E}\\
\sum_{k=1}^{N}p_{k}\left[u_{t}^{k}-\eta_{1}^{k}g_{t,k}\right] & \text{if }t+1\in\mathcal{I}_{E}
\end{cases}\\
u_{t+1}^{k} & =w_{t+1}^{k}+\gamma^{k}(w_{t+1}^{k}-w_{t}^{k})+\eta_{2}^{k}g_{t,k}
\end{align*}
where we note that the natural parameter is $w_{t}$, while $u_{t}$
is an auxiliary parameter, which we initialize to be $u_{0}^{k}$,
and 
\begin{align*}
g_{t,k} & :=\nabla F_{k}(u_{t}^{k},\xi_{t}^{k})
\end{align*}
is the stochastic gradient and 
\begin{align*}
g_{t} & =\sum_{k=1}^{N}p_{k}g_{t,k}
\end{align*}
is the averaged stochastic gradient. When $\eta_{2}^{k}\equiv0$,
this reduces to the FedAvg algorithm with Nesterov updates.

We note that the update can equivalently be written as 
\begin{align*}
v_{t+1}^{k} & =(1-\alpha^{k})v_{t}^{k}+\alpha^{k}u_{t}^{k}-\delta^{k}g_{t,k}\\
w_{t+1}^{k} & =\begin{cases}
u_{t}^{k}-\eta^{k}g_{t,k} & \text{if }t+1\notin\mathcal{I}_{E}\\
\sum_{k=1}^{N}p_{k}\left[u_{t}^{k}-\eta^{k}g_{t,k}\right] & \text{if }t+1\in\mathcal{I}_{E}
\end{cases}\\
u_{t+1}^{k} & =\frac{\alpha^{k}}{1+\alpha^{k}}v_{t+1}^{k}+\frac{1}{1+\alpha^{k}}w_{t+1}^{k}
\end{align*}
where there is a bijection between the parameters 
\begin{align*}
\frac{1-\alpha^{k}}{1+\alpha^{k}} & =\gamma^{k}\\
\eta^{k} & =\eta_{1}^{k}\\
\frac{\eta^{k}-\alpha^{k}\delta^{k}}{1+\alpha^{k}} & =\eta_{2}^{k}
\end{align*}
and we further introduce an auxiliary parameter $v_{t}^{k}$, which
is initialized at $v_{0}^{k}$. We also note that when $\delta^{k}=\frac{\eta^{k}}{\alpha^{k}}$,
the update reduces to the Nesterov accelerated SGD. This version of
the FedAvg with MaSS algorithm is used for analyzing the exponential
convergence. 

\subsection{Exponential Convergence of FedAvg with MaSS}

We now present the exponential convergence result in linear regression
using FedAvg with MaSS updates. On each device, local data is stored
and mini-batch gradient descent with batch size $m$ is performed.
We assume that the batch size is the same across devices. 
\begin{theorem}
	(FedAvg with MaSS, Linear Regression) For the overparamterized linear
	regression problem, FedAvg with local MaSS updates and communication
	every $E$ iterations with constant step size $\overline{\alpha}=\frac{1}{2E}\frac{N}{l\nu_{\max}+L(N-\nu_{\min})}$
	gives the exponential convergence guarantee 
	\begin{align*}
	\mathbb{E}F(\overline{w}_{t}) & \leq L(1-c\frac{N}{E\sqrt{\kappa\tilde{\kappa}}})^{t}\|w_{0}-w^{\ast}\|^{2}
	\end{align*}
	where $\kappa=L/\mu$, $c$ is some universal constant, and $\tilde{\kappa}$
	is the smallest positive number such that $\mathbb{E}\|\tilde{x}\|_{H^{-1}}^{2}\tilde{x}\tilde{x}^{T}\preceq\tilde{\kappa}H$. 
	\label{th:fedmasslr}
\end{theorem}

We immediately see that since $\tilde{\kappa}\leq\kappa$, FedAvg with MaSS provides an improvement over FedAvg with SGD in the the interpolation setting, while enjoying the same speedup factor in the number of workers. 