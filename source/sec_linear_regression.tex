% !TEX ROOT=./main.tex



\section{Linear Regression}

In contrast to the gradient descent setting, in the stochastic gradient
setting, Nesterov update is known to fail to accelerate over stochastic
gradient. Thus in general we cannot hope to obtain acceleration results
for the FedAvg algorithm with Nesterov updates. However, in special
problem settings, there have been a number of works showing that accelerated
stochastic gradient descent methods achieves better rates than stochastic
gradient descent. One such case is the so-called interpolation setting,
where the objective has an optimal value of 0, e.g. due to overparameterization.
The works of Belkin et.al have shown that under interpolation, SGD
achieves exponential convergence under constant step-size, thanks
to the automatic variance reduction property that is not available
in generic optimization problems. In this section, we extend their
result to the federated learning problem and prove the exponential
convergence of SGD for a class of convex objectives in the interpolation
setting.

In the non-federated learning setting, {[}TODO: CITE{]} have provided
negative results on the acceleration of Nesterov and Heavy Ball updates
over plain SGD for certain problems. In {[}Liu\&Belkin{]}, the authors
show that in the interpolation setting, stochastic Nesterov update
is not able to achieve accleration over vanilla SGD. They introduce
the MaSS algorithm, which is a variant of the Nesterov acceleration,
where in the update a non-negative multiple of the gradient is added
to the Nesterov parameter update to correct for the ``over-descent''
of the Nesterov update. They show that the MaSS algorithm is able
to achieve acceleration over the exponential convergence of SGD both
theoretically and empirically.

The next natural question is then whether similar conclusions can
be drawn in the federated learning setting. We demonstrate that this
is possible by extending the results of Belkin et al. to show that
for the federated learning problem in the interpolation setting, Nesterov
SGD cannot in general achieve acceleration over plain SGD. We then
adapt the MaSS algorithm to the federated learning setting, and prove
that for certain class of convex objectives with 0 global objective
value, the FedAvg algorithm with MaSS updates achieves exponential
convergence and acceleration of convergence rate over SGD. 

We first show that FedAvg with MaSS has exponential convergence for
linear regression when the global objective has 0 as its global minimum.

\subsection{Notation and Definitions}

Following Liu\&Belkin and Jain et al., we define some condition number
related quantities. Let $H^{k}=\frac{1}{n_{k}}\sum_{j=1}^{n_{k}}x_{k,j}x_{k,j}$
be the Hessian matrix of $F_{k}$. Let $L^{k}$ and $\mu^{k}$ be
any upper bound and lower bound of the non-zero eigenvalues of the
Hessians $H^{k}$. For a mini-batch $\{\tilde{x}_{j}\}_{j=1}^{m}$
of $m$ samples from device $k$, let $\tilde{H}_{m}^{k}=\frac{1}{m}\sum_{j=1}^{m}\tilde{x}_{j}\tilde{x}_{j}^{T}$
be the unbiased mini-batch estimate of $H^{k}$. Let $L_{1}^{k}$
be the smallest positive number such that 
\begin{align*}
\mathbb{E}\left[\|\tilde{x}\|^{2}\tilde{x}\tilde{x}^{T}\right] & \preceq L_{1}^{k}H^{k}
\end{align*}
and define 
\begin{align*}
L_{m}^{k} & =L_{1}^{k}/m+(m-1)L^{k}/m
\end{align*}

Define the $m$-stochastic condition number as $\kappa_{m}^{k}:=L_{m}^{k}/\mu$.
Define $L_{m}=\max_{k}L_{m}^{k}$ and $\kappa_{m}=\max_{k}\kappa_{m}^{k}$.
Define the statistical condition number $\tilde{\kappa}^{k}$ as the
smallest positive real number such that 
\begin{align*}
\mathbb{E}\left[\langle\tilde{x}(H^{k})^{-1},\tilde{x}\rangle\tilde{x}\tilde{x}^{T}\right] & \preceq\tilde{\kappa}^{k}H^{k}
\end{align*}

In order to use hyperparameters that are universal across devices,
we further define $L=\max_{k}L^{k}$, $\mu=\min_{k}\mu^{k}$, $L_{m}=\max_{k}L_{m}^{k}$
and $\kappa_{m}=\max_{k}\kappa_{m}^{k}$. 

\subsection{FedAvg with MaSS Updates}

The FedAvg algorithm with MaSS follows the updates
\begin{align*}
w_{t+1}^{k} & =\begin{cases}
u_{t}^{k}-\eta_{1}^{k}g_{t,k} & \text{if }t+1\notin\mathcal{I}_{E}\\
\sum_{k=1}^{N}p_{k}\left[u_{t}^{k}-\eta_{1}^{k}g_{t,k}\right] & \text{if }t+1\in\mathcal{I}_{E}
\end{cases}\\
u_{t+1}^{k} & =w_{t+1}^{k}+\gamma^{k}(w_{t+1}^{k}-w_{t}^{k})+\eta_{2}^{k}g_{t,k}
\end{align*}
where we note that the natural parameter is $w_{t}$, while $u_{t}$
is an auxiliary parameter, which we initialize to be $u_{0}^{k}$,
and 
\begin{align*}
g_{t,k} & :=\nabla F_{k}(u_{t}^{k},\xi_{t}^{k})
\end{align*}
is the stochastic gradient and 
\begin{align*}
g_{t} & =\sum_{k=1}^{N}p_{k}g_{t,k}
\end{align*}
is the averaged stochastic gradient. When $\eta_{2}^{k}\equiv0$,
this reduces to the FedAvg algorithm with Nesterov updates.

We note that the update can equivalently be written as 
\begin{align*}
v_{t+1}^{k} & =(1-\alpha^{k})v_{t}^{k}+\alpha^{k}u_{t}^{k}-\delta^{k}g_{t,k}\\
w_{t+1}^{k} & =\begin{cases}
u_{t}^{k}-\eta^{k}g_{t,k} & \text{if }t+1\notin\mathcal{I}_{E}\\
\sum_{k=1}^{N}p_{k}\left[u_{t}^{k}-\eta^{k}g_{t,k}\right] & \text{if }t+1\in\mathcal{I}_{E}
\end{cases}\\
u_{t+1}^{k} & =\frac{\alpha^{k}}{1+\alpha^{k}}v_{t+1}^{k}+\frac{1}{1+\alpha^{k}}w_{t+1}^{k}
\end{align*}
where there is a bijection between the parameters 
\begin{align*}
\frac{1-\alpha^{k}}{1+\alpha^{k}} & =\gamma^{k}\\
\eta^{k} & =\eta_{1}^{k}\\
\frac{\eta^{k}-\alpha^{k}\delta^{k}}{1+\alpha^{k}} & =\eta_{2}^{k}
\end{align*}
and we further introduce an auxiliary parameter $v_{t}^{k}$, which
is initialized at $v_{0}^{k}$. We also note that when $\delta^{k}=\frac{\eta^{k}}{\alpha^{k}}$,
the update reduces to the Nesterov accelerated SGD. This version of
the FedAvg with MaSS algorithm is used for analyzing the exponential
convergence. 

\subsection{Exponential Convergence of FedAvg with MaSS}

We now present the exponential convergence result in linear regression
using FedAvg with MaSS updates. On each device, local data is stored
and mini-batch gradient descent with batch size $m$ is performed.
We assume that the batch size is the same across devices. 
\begin{theorem}
	(FedAvg with MaSS, Linear Regression) Let $\tilde{\kappa}_{m}:=\tilde{\kappa}/m+(m-1)/m$,
	and let the hyperparameters satisfy 
	\begin{align*}
	\eta^{k}(m)=\frac{1}{L_{m}}, & \alpha^{k}(m)=\frac{1}{\sqrt{\kappa_{m}\tilde{\kappa}_{m}}},\delta^{k}(m)=\frac{\eta^{k}}{\alpha^{k}\tilde{\kappa}_{m}}
	\end{align*}
	for all $k$. Let $\mathcal{I}_{E}=\{\ell E\mid\ell\in\mathbb{N}\}$
	be the set of communication rounds, then the FedAvg algorithm with
	MaSS and full participation solves the problem 
	\begin{align*}
	F(w) & =\sum_{k=1}^{N}p_{k}F_{k}(w)\\
	F_{k}(w) & =\frac{1}{2n_{k}}\sum_{j=1}^{n_{k}}(w^{T}x_{k,j}-z_{k,j})^{2}
	\end{align*}
	with exponential convergence
	\begin{align*}
	\mathbb{E}F(\overline{w}_{t})\leq\frac{L}{2}\mathbb{E}\|\overline{w}_{t}-w^{\ast}\|^{2} & \leq C\cdot\exp(-\frac{t}{\sqrt{(\kappa_{N}+(E-1)^{2})(\tilde{\kappa}_{N}+(E-1)^{2})}})
	\end{align*}
	where $w^{\ast}$ is such that $F(w^{\ast})=0$ and $C=\frac{L}{2}\cdot\frac{\alpha}{\delta}\sum_{k=1}^{N}p_{k}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})$. 
	
In particular, this implies that for $E=1$, there is linear speedup, i.e. 
	\begin{align*}
	\|\overline{w}_{t+1}-w^{\ast}\|^{2} & \leq C\cdot\exp(-\frac{t}{\sqrt{(\kappa_{N})(\tilde{\kappa}_{N})}})\leq C\cdot\exp(-\frac{Nt}{\sqrt{(\kappa_{1})(\tilde{\kappa})}})
	\end{align*}

	
	With partial participation of $K$ devices each communication round,
	the convergence rate is also 
	\begin{align*}
	\mathbb{E}F(\overline{w}_{t})\leq\frac{L}{2}\mathbb{E}\|\overline{w}_{t}-w^{\ast}\|^{2} & \leq C\cdot(1+\frac{4}{K}\cdot(\frac{t}{E}-1))\cdot(1-\frac{1}{\sqrt{\kappa_{K}\tilde{\kappa}_{K}}})^{t}=O(\exp(-\frac{t}{\sqrt{(\kappa_{N}+(E-1)^{2})(\tilde{\kappa}_{N}+(E-1)^{2})}}))
	\end{align*}
	

\end{theorem}