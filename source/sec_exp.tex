% !TEX ROOT=./main.tex



\section{Numerical Experiments}

We examine the practical speedup on a linear regression problem, 
$ F(\vw) = \sum_{k=1}^N p_kF_k(\vw)$, the objective function on each local device $k$ is given by, $F_k(\vw) = \frac{1}{N_k} \sum_{i=1}^{N_k} (\vw^T\vx_i^k + b  - y_i^k)^2$, where we generated i.i.d. data $\vx_i^k$ for all devices. At each communication round, all devices participated
in synchronization. 


\begin{figure}
\centering
	\begin{tabular}{ccc}
	\includegraphics[width=0.33\textwidth]{fig/paper-linregressionspeedupEpochsT-min-linearregressionw8a-epsilon002-reg0.pdf} & \includegraphics[width=0.33\textwidth]{fig/paper-stronglycvxsmthspeedupEpochsT-min-w8a-epsilon0131-reg1e-05.pdf} &
	\includegraphics[width=0.33\textwidth]{fig/paper-cvxsmoothspeedupEpochsT-min-w8a-epsilon0134-reg0.pdf}
\\
\includegraphics[width=0.33\textwidth]{fig/paper-linregressionspeedupNodesT-min-linearregressionw8a-epsilon002-reg0.pdf}
&
\includegraphics[width=0.33\textwidth]{fig/paper-stronglycvxsmthspeedupNodesT-min-w8a-epsilon0131-reg1e-05.pdf} &
\includegraphics[width=0.33\textwidth]{fig/paper-cvxsmoothspeedupNodesT-min-w8a-epsilon0134-reg0.pdf} \\
(a) Linear regression  & (b) Strongly convex objective & (c) Convex smooth objective
	\end{tabular}

\end{figure}

\subsection{Strongly Convex}

\begin{figure}[h!]
\centering
\begin{tabular}{cc}
	\includegraphics[width=0.45\textwidth]{fig/paper-stronglycvxsmthspeedupNodesT-min-w8a-epsilon0131-reg1e-05.pdf} & 
\includegraphics[width=0.45\textwidth]{fig/paper-stronglycvxsmthspeedupEpochsT-min-w8a-epsilon0131-reg1e-05.pdf} \\
\end{tabular}
	\caption{Logistic regression with regularization $1e-5$. The linear speed up w.r.t the number of nodes and number of epochs. The synthetic dataset has $N=49749$ samples, evenly distributed on $1, 2, 4, 8, 16, 32$ devices. The figure shows the number of iterations/rounds needed to converge to $\epsilon-$accuracy, where $\epsilon=0.005$. The learning rate is decayed as the $\eta_t = \min(32, \frac{Nc}{1 + t})$, where we extensively search the best learning rate $c \in \{2^{-1}c, 2^{-2}c, c, 2c, 2^{2}c\}$ and for each configuration. Batch size is 4 in this case.}
\end{figure}



\subsection{Convex smooth objective}

\begin{figure}[h!]
\centering
\begin{tabular}{cc}
	\includegraphics[width=0.45\textwidth]{fig/paper-cvxsmoothspeedupNodesT-min-w8a-epsilon0134-reg0.pdf} & 
\includegraphics[width=0.45\textwidth]{fig/paper-cvxsmoothspeedupEpochsT-min-w8a-epsilon0134-reg0.pdf} \\
\end{tabular}
	\caption{Logistic regression without regularization. The linear speed up w.r.t the number of nodes and number of epochs. The w8a dataset has $N=49749$ samples, evenly distributed on $1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024$ devices. The figure shows the number of iterations needed to converge to $\epsilon-$accuracy, where $\epsilon=0.02$. The learning rate is decayed as the $\eta_t = \min(\eta_0, \frac{Nc}{1 + t})$, where we extensively search the best learning rate $c \in \{2^{-1}c, 2^{-2}c, c, 2c, 2^{2}c\}$, $\eta_0 \in \{1, 32\}$ and for each configuration. Batch size is 4 in this case.}
\end{figure}




\subsection{Linear regression}





