% !TEX ROOT=./main.tex


\section{Comparison of Convergence Rates with Related Works}
\label{sec:app:comparison}
In this section, we compare our convergence rate with the best-known results in the literature (see Table~\ref{tb:convergenceratev3}). 
% In Table~\ref{tb:convergenceratev1} and Table~\ref{tb:convergenceratev2}, we summarize 
% the convergence results in terms of participations and accelerations. 
% In Table~\ref{tb:convergenceratev3}, we compare our 
% results with the best known results in the literatures. 
In~\cite{haddadpour2019convergence}, the authors provide $\cO(1/NT)$ convergence rate of non-convex problems under Polyak-Åojasiewicz (PL) condition, which
means their results can directly apply to the strongly convex problems. However, their assumption is based on bounded gradient diversity, defined as follows: 
\begin{align*}
	\Lambda(\vw) = \frac{\sum_{k}p_{k}\|\nabla F_{k}(\mathbf{w})\|_{2}^{2}}{\|\sum_{k}p_{k}\nabla F_{k}(\mathbf{w})\|_{2}^{2}} \leq B
\end{align*} 
This is a more restrictive assumption comparing to assuming bounded gradient under the case of target accuracy $\epsilon \rightarrow 0$ and PL condition.
To see this, consider the gradient diversity at the global optimal $\vw^*$, i.e., $\Lambda(\vw^*) = \frac{\sum_{k}p_{k}\|\nabla F_{k}(\mathbf{w})\|_{2}^{2}}{\|\sum_{k}p_{k}\nabla F_{k}(\mathbf{w})\|_{2}^{2}}$. For $\Lambda(\vw^*)$ to be bounded, it requires $\|\nabla F_{k}(\mathbf{w}^*)\|_{2}^{2} = 0$, $\forall \ k$. This indicates 
$\vw^*$ is also the minimizer of each local objective, which contradicts to the practical setting of heterogeneous data. Therefore, their bound 
is not effective for arbitrary small $\epsilon$-accuracy under general heterogeneous data while our convergence results still hold in this case.
% If we only consider the domain $\vw \in \{\vw | \|\sum_{k}p_{k}\nabla F_{k}(\mathbf{w})\|_{2}^{2} > c\}$, where $c$ is a positive constant. 
% The bounded gradient can imply gradient diversity (see Corollary 10 in ~\cite{li2018federated}). In this case, the gradient diversity is a relaxed 

In~\cite{karimireddy2019scaffold}, the linear speedup convergence rate of FedAvg are provided for strongly convex, general convex, and non-convex problems 
under full participation setting. However, their rate does not enjoy linear speedup for any number of devices while our results apply to any valid $K\leq N$. For example,  they provides an optimality gap of $\cO\left((1-\frac{K}{N})E/T\right)$ for the strongly convex case \cite[Theorem V]{karimireddy2019scaffold}.
With partial participation, and when $K=O(1)$, their convergence rate is $\cO(E/T)$ which does not have linear speedup. 
Under partial participation, the FedAvg analyses in~\cite{karimireddy2019scaffold} requires $E=\cO(1)$. For example, the strongly convex result $\cO((1-\frac{K}{N})E/T)$ in Theorem V is $\cO(E/T)$ when $K=O(1)$ and is $\cO(E/NT)$ when $K=\cO(N)$. In either case, to achieve a $\cO(1/T)$ convergence rate, it requires $E=\cO(1)$ as well. Similar conclusion also holds for the general convex problem. 

% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|c|}\hline 
% 	Algorithm \textbackslash\  Objective function        & Strongly Convex        & Convex \\ \hline \hline
% 	FedAvg                         & $\cO(\frac{1}{NT}+\frac{E^{2}}{T^{2}})$    &  $\mathcal{O}\left(\frac{1}{\sqrt{NT}}+\frac{NE^{2}}{T}\right)$       \\ \hline
% 	Nesterov accelerated FedAvg    & $\cO(\frac{1}{NT}+\frac{E^{2}}{T^{2}})$    & $\mathcal{O}\left(\frac{1}{\sqrt{NT}}+\frac{NE^{2}}{T}\right)$       \\ \hline
% \end{tabular}
% \caption{A high level summary of the convergence results for full participation in this paper. This table only highlights the
% dependence on $T$ (number of iterations), $E$ (the largest number of local steps), $N$ (the total number of devices), and $K\leq N$ the number of participated devices.}
% \label{tb:convergenceratev1}
% \end{table}

% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|c|}\hline 
% 	Participation \textbackslash\ Objective function            & Strongly Convex        & Convex \\ \hline \hline
% 	Full                         & $\cO(\frac{1}{NT}+\frac{E^{2}}{T^{2}})$    &  $\mathcal{O}\left(\frac{1}{\sqrt{NT}}+\frac{NE^{2}}{T}\right)$       \\ \hline
% 	Partial                      &  $\cO\left(\frac{E^{2}}{KT}+\frac{E^{2}}{T^{2}}\right)$   &  $\cO\left(\frac{E^2}{\sqrt{KT}}+\frac{KE^2}{T} \right)$      \\ \hline
% \end{tabular}
% \caption{A high level summary of the convergence results in this paper. This table only highlights the
% dependence on $T$ (number of iterations), $E$ (the largest number of local steps), $N$ (the total number of devices), and $K\leq N$ the number of participated devices.
% These convergence rates applied to both FedAvg and Nesterov accelerated FedAvg.}
% \label{tb:convergenceratev2}
% \end{table}


\begin{table*}[h!]
{
\scriptsize
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline Reference                 & Convergence rate    & E                           			& NonIID & Participation & Extra Assumptions  		  & Setting  \\ \hline\hline 
FedAvg\cite{li2019convergence}         & $\cO(\frac{E^2}{T})$& $\cO(1)$                    		& \cmark & Partial       & Bounded gradient   		  & Strongly convex  \\ \hline
FedAvg\cite{haddadpour2019convergence} & $\cO(\frac{1}{KT})$ & $\cO(K^{-1/3}T^{2/3})^{\dagger}$ & \cmark$^{\ddagger\ddagger}$ & Partial       & Bounded gradient diversity   & Strongly convex$^{\mathsection}$  \\ \hline
FedAvg\cite{koloskova2020unified} & $\cO(\frac{1}{NT})$ & $\cO(N^{-1/2}T^{1/2})$     	& \cmark & Full       & Bounded gradient   & Strongly convex  \\ \hline
FedAvg\cite{karimireddy2019scaffold} & $\cO(\frac{1}{NT})^{\dagger\dagger}$ & $\cO(N^{-1/2}T^{1/2})^{\dagger\dagger}$   	& \cmark & Partial       & Bounded gradient dissimilarity   & Strongly convex  \\ \hline
FedAvg/N-FedAvg (our work)                 & $\cO(\frac{1}{KT})$ & $\cO(N^{-1/2}T^{1/2})^{\ddagger}$ & \cmark	 & Partial       & Bounded gradient             & Strongly convex  \\\hline\hline
FedAvg\cite{khaled2020tighter}  & $\cO(\frac{1}{\sqrt{NT}})$ & $\cO(N^{-3/2}T^{1/2})$     	    & \cmark& Full        & Bounded gradient             & Convex  \\\hline
FedAvg\cite{koloskova2020unified} & $\cO(\frac{1}{\sqrt{NT}})$ & $\cO(N^{-3/4}T^{1/4})$    & \cmark & Full       & Bounded gradient             &  Convex  \\ \hline
FedAvg\cite{karimireddy2019scaffold} & $\cO(\frac{1}{\sqrt{NT}})^{\dagger\dagger}$ & $\cO(N^{-3/4}T^{1/4})^{\dagger\dagger}$  & \cmark & Partial       & Bounded gradient dissimilarity   &  Convex  \\ \hline
FedAvg/N-FedAvg (our work)      & $\cO\left(\frac{1}{\sqrt{KT}}\right)$ & $\cO(N^{-3/4}T^{1/4})^{\ddagger}$& \cmark			& Partial     & Bounded gradient            &  Convex   \\ \hline\hline
FedAvg & $\cO\left(\exp(-\frac{NT}{E\kappa_1})\right)$ & $ \cO(T^{\beta})$                   & \cmark&  Partial     & Bounded gradient    & Overparameterized LR\\ \hline
FedMass & $\cO\left(\exp(-\frac{NT}{E\sqrt{\kappa_1\tilde{\kappa}}})\right)$ & $ \cO(T^{\beta})$ & \cmark &  Partial     & Bounded gradient    & Overparameterized LR \\ \hline
\end{tabular}
}
\caption{A high-level summary of the convergence results in this paper compared to prior state-of-the-art FL algorithms. This table only highlights the
dependence on $T$ (number of iterations), $E$ (the maximal number of local steps), $N$ (the total number of devices), and $K\leq N$ the number of participated devices. 
$\kappa$ is the condition number of the system and $\beta \in (0,1)$. We denote Nesterov accelerated FedAvg as N-FedAvg in this table.}
{\raggedright 
         $^{\dagger}$ This $E$ is obtained under i.i.d. setting. \\
         $^{\ddagger}$ This $E$ is obtained under full participation setting. \\ 
         % $^{\ddagger}$ The convergence of FedAvg for general overparameterized strongly convex and overparameterized linear regression share the same order in terms of $E, T, N$.\\
         $^{\mathsection}$ In~\cite{haddadpour2019convergence}, the convergence rate is for non-convex smooth problems with PL condition, which also applies to strongly convex problems. Therefore, we compare it with our strongly convex results here.\\
         $^{\ddagger\ddagger}$ The bounded gradient diversity assumption is not applicable for general heterogeneous data when converging to arbitrarily small $\epsilon$-accuracy (see discussions in Sec~\ref{sec:app:comparison}).\\
         $^{\dagger\dagger}$ Although the results in~\cite{karimireddy2019scaffold} is applicable for partial participation setting, their results only achieve linear speedup under full participation setting $K=N$ while we show linear speedup convergence for $K\leq N$ (see discussions in Sec~\ref{sec:app:comparison}). The $E$ in the table is obtained under full participation. Under partial participation, the communication complexity is $E=\cO(1)$.
           \par}
\label{tb:convergenceratev3}
\end{table*}



