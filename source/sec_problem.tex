% !TEX ROOT=./main.tex


\section{Problem Setting}

\begin{align}
	\min _{\mathbf{w}}\left\{F(\mathbf{w}) \triangleq \sum_{k=1}^{N} p_{k} F_{k}(\mathbf{w})\right\}
	\label{eq:problem}
\end{align}
where $N$ is the number of devices, and $p_k$ is the weight of the k-th device
such that $p_k \geq 0$ and $\sum_{k=1}^N p_k = 1$. Suppose the k-th device
holds the nk training data: $x_{k,1}, x_{k,2}, \dots, x_{k,n_k}$ . The local
$k \in S_t^K$ objective $F_k(\cdot)$ is defined by

\begin{align}
F_{k}(\mathbf{w}) \triangleq \frac{1}{n_{k}} \sum_{j=1}^{n_{k}} \ell\left(\mathbf{w} ; x_{k, j}\right)	
\label{eq:localloss}
\end{align}

% The summary of all assumptions:

% \begin{assumption}
% $F_{1}, \cdots, F_{N}$ are all $L$-smooth: for all  $\mathbf{v}$  and $\mathbf{w}$, $F_{k}(\mathbf{v}) \leq F_{k}(\mathbf{w})+(\mathbf{v}- \\ \mathbf{w})^{T} \nabla F_{k}(\mathbf{w})+\frac{L}{2}\|\mathbf{v}-\mathbf{w}\|_{2}^{2}$.
% \end{assumption}

% \begin{assumption}
% $F_1,\dots, F_N$ are all convex: for all $\vv$ and $\vw$, 
% $F_k(\vv) \geq F_k(\vw)+(\vv -\vw)^T \grad F_k(\vw)$. \label{ass:cvx}
% \end{assumption}
% \begin{assumption}
% $	F_{1}, \cdots, F_{N} \text { are all } \mu \text { -strongly convex: for all v and } \mathbf{w}, F_{k}(\mathbf{v}) \geq F_{k}(\mathbf{w})+(\mathbf{v}- \\ \mathbf{w})^{T} \nabla F_{k}(\mathbf{w})+\frac{\mu}{2}\|\mathbf{v}-\mathbf{w}\|_{2}^{2}$
% \end{assumption}

% \begin{assumption}
% Let $\xi_{t}^{k}$ be sampled from the k-th device's local data uniformly at random. The variance of stochastic gradients in each device is bounded:
% $\mathbb{E}\left\|\nabla F_{k}\left(\mathbf{w}_{t}^{k}, \xi_{t}^{k}\right)-\nabla F_{k}\left(\mathbf{w}_{t}^{k}\right)\right\|^{2} \leq \sigma_{k}^{2}$, for $k = 1,..., N$.
% \end{assumption}

% \begin{assumption}
% The expected squared norm of stochastic gradients is uniformly bounded. i.e.,
% $\mathbb{E}\left\|\nabla F_{k}\left(\mathbf{w}_{t}^{k}, \xi_{t}^{k}\right)\right\|^{2} \leq G^{2}$, for all $k = 1,..., N$ and $t=0, \dots, T-1$.
% \end{assumption}

\section{Notations}


Let $\vw_t^k$ be the model parameter maintained in the k-th device at the t-th step. $\cI_E$ is a set of global synchronization steps, i.e., $\cI_E = \{n E|n = 1, 2, 3,\dots \}$. If $t+1 \in \cI_E$, which means we communicate the server with (all) clients at time step $t+1$. $\vv^k_{t+1}$ is the immediate result of one step SGD update from $\vw^k_{t}$.
$\xi_{t}^{k}$ denotes the data sampled from k-th deviceâ€™s local data uniformly at random.
Follow the common practice, we define two virtual sequences $\overline{\mathbf{v}}_{t}$ and $\overline{\mathbf{w}}_{t}$. For full device participation and $t \notin \cI_E$,
$\ov{v}_t = \ov{w}_t =\sum_{k=1}^{N} p_{k} \mathbf{v}_{t}^{k}$. In partial participation, $t \in \cI_E$, $\ov{w}_t \neq \ov{v}_t$ since $\ov{v}_t=\sum_{k=1}^{N} p_{k} \mathbf{v}_{t}^{k}$ while $\sum_{k\in \cS_t}\mathbf{w}_{t}^{k}$. However, we can
set unbiased sampling strategy such that $ \EE_{\cS_t} \ov{w}_t = \ov{v}_t$.
$\overline{\mathbf{v}}_{t+1}$ is one-step SGD from $\overline{\mathbf{w}}_{t}$. 
\begin{align}
\overline{\mathbf{v}}_{t+1}=\overline{\mathbf{w}}_{t}-\eta_{t} \mathbf{g}_{t}	\label{eq:vbar}
\end{align}
% $t+1 \in \cI_E$, we can fetch $\overline{\vw}_{t+1}$, we can communicate $\overline{\vw}_{t+1}$ to all devices.
where $\vg_{t} = \sum_{k=1}^{N} p_{k} \vg_{t,k} $ is one-step stochastic gradient, averaged over all devices. 
\begin{align}
\vg_{t,k} \left\{\begin{array}{ll} 
 = \nabla F_{k}\left(\mathbf{w}_{t}^{k},\xi_{t}^{k} \right)  &  \text{smooth}\\
 \in \partial F_{k}\left(w_{t}^{k}, \xi_{t}^{k}\right)  & \text{non-smooth}
 \end{array}\right.
\end{align}
Similarly, we denote the expected one-step gradient $\ov{g}_{t}= \EE_{\xi_t}[\vg_t] = \sum_{k=1}^{N} p_{k} \EE_{\xi_{t}^{k}} \vg_{t,k}$, where
\begin{align}
\EE_{\xi_{t}^{k}} \vg_{t,k}  \left\{\begin{array}{ll} 
 = \nabla F_{k}\left(\mathbf{w}_{t}^{k}\right)  &  \text{smooth}\\
 \in \partial F_{k}\left(w_{t}^{k}\right)  & \text{non-smooth}
 \end{array}\right.
\end{align}
and we use $\xi_t = \{\xi_t^k\}_{k=1}^N$ denotes samples at all devices at time step $t$. 

The updates of FedAve with partial device activation is given by: 
\begin{align} 
\mathbf{v}_{t+1}^{k} &=\mathbf{w}_{t}^{k}-\eta_{t} \nabla F_{k}\left(\mathbf{w}_{t}^{k}, \xi_{t}^{k}\right) \\ \mathbf{w}_{t+1}^{k} &=\left\{\begin{array}{ll}\mathbf{v}_{t+1}^{k} & \text { if } t+1 \notin \mathcal{I}_{E}, \\ 
\sum_{k \in \cS_{t+1}} \mathbf{v}_{t+1}^{k} & \text { if } t+1 \in \mathcal{I}_{E}\end{array}\right.
\end{align}

In~\cite{li2019convergence}, two types of unbiased sampling strategies are considered in Lemma 5. 
The sampling scheme I establishes $\cS_{t+1}$ by i.i.d. sampling the devices with replacement,
in this case the upper bound of expected square norm of $\ov{w}_{t+1} - \ov{v}_{t+1}$ is given by:
\begin{align}
\EE_{\cS_{t+1}}\left\|\ov{w}_{t+1} - \ov{v}_{t+1}\right\|^2	\leq \frac{4}{K} \eta_t^2 E^2G^2
\end{align}
The sampling scheme II establishes $\cS_{t+1}$ by uniformly sampling all devices without
replacement, in which we have the 
\begin{align}
\EE_{\cS_{t+1}}\left\|\ov{w}_{t+1} - \ov{v}_{t+1}\right\|^2	\leq \frac{4(N - K)}{K(N-1)} \eta_t^2 E^2G^2
\end{align}
We denote this upper bound as follows for concise presentation. 
\begin{align}
	\EE_{\cS_{t+1}}\left\|\ov{w}_{t+1} - \ov{v}_{t+1}\right\|^2 \leq  \eta_t^2 C
	\label{eq:partialsample}
\end{align}




