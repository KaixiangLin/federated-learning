In contrast to the gradient descent setting, in the stochastic gradient
setting, Nesterov update is known to fail to accelerate over stochastic
gradient. Thus in general we cannot hope to obtain acceleration results
for the FedAvg algorithm with Nesterov updates. However, in special
problem settings, there have been a number of works showing that accelerated
stochastic gradient descent methods achieves better rates than stochastic
gradient descent. One such case is the so-called interpolation setting,
where the objective has an optimal value of 0, e.g. due to overparameterization.
The works of Belkin et.al have shown that under interpolation, SGD
achieves exponential convergence under constant step-size, thanks
to the automatic variance reduction property that is not available
in generic optimization problems. In this section, we extend their
result to the federated learning problem and prove the exponential
convergence of SGD for a class of convex objectives in the interpolation
setting.

In the non-federated learning setting, {[}TODO: CITE{]} have provided
negative results on the acceleration of Nesterov and Heavy Ball updates
over plain SGD for certain problems. In {[}Liu\&Belkin{]}, the authors
show that in the interpolation setting, stochastic Nesterov update
is not able to achieve accleration over vanilla SGD. They introduce
the MaSS algorithm, which is a variant of the Nesterov acceleration,
where in the update a non-negative multiple of the gradient is added
to the Nesterov parameter update to correct for the ``over-descent''
of the Nesterov update. They show that the MaSS algorithm is able
to achieve acceleration over the exponential convergence of SGD both
theoretically and empirically.

The next natural question is then whether similar conclusions can
be drawn in the federated learning setting. We demonstrate that this
is possible by extending the results of Belkin et al. to show that
for the federated learning problem in the interpolation setting, Nesterov
SGD cannot in general achieve acceleration over plain SGD. We then
adapt the MaSS algorithm to the federated learning setting, and prove
that for certain class of convex objectives with 0 global objective
value, the FedAvg algorithm with MaSS updates achieves exponential
convergence and acceleration of convergence rate over SGD. 

We first show that FedAvg with MaSS has exponential convergence for
linear regression when the global objective has 0 as its global minimum.
Since the global loss is given by 
\begin{align*}
F(w) & =\sum_{k=1}^{N}p_{k}F_{k}(w)\\
F_{k}(w) & =\frac{1}{2n_{k}}\sum_{j=1}^{n_{k}}(w^{T}x_{k,j}-z_{k,j})^{2}
\end{align*}
 and there exists $w^{\ast}$ such that $F(w^{\ast})=0$, in particular
this implies that $F_{k}^{\ast}=F_{k}(w^{\ast})=F^{\ast}=0$ for all
$k$.

\subsection{Notation and Definitions}

Following Liu\&Belkin and Jain et al., we define some condition number
related quantities. Let $H^{k}=\frac{1}{n_{k}}\sum_{j=1}^{n_{k}}x_{k,j}x_{k,j}$
be the Hessian matrix of $F_{k}$. Let $L^{k}$ and $\mu^{k}$ be
any upper bound and lower bound of the non-zero eigenvalues of the
Hessians $H^{k}$. For a mini-batch $\{\tilde{x}_{j}\}_{j=1}^{m}$
of $m$ samples from device $k$, let $\tilde{H}_{m}^{k}=\frac{1}{m}\sum_{j=1}^{m}\tilde{x}_{j}\tilde{x}_{j}^{T}$
be the unbiased mini-batch estimate of $H^{k}$. Let $L_{1}^{k}$
be the smallest positive number such that 
\begin{align*}
\mathbb{E}\left[\|\tilde{x}\|^{2}\tilde{x}\tilde{x}^{T}\right] & \preceq L_{1}^{k}H^{k}
\end{align*}
 and define 
\begin{align*}
L_{m}^{k} & =L_{1}^{k}/m+(m-1)L^{k}/m
\end{align*}

Define the $m$-stochastic condition number as $\kappa_{m}^{k}:=L_{m}^{k}/\mu$.
Define $L_{m}=\max_{k}L_{m}^{k}$ and $\kappa_{m}=\max_{k}\kappa_{m}^{k}$.
Define the statistical condition number $\tilde{\kappa}^{k}$ as the
smallest positive real number such that 
\begin{align*}
\mathbb{E}\left[\langle\tilde{x}(H^{k})^{-1},\tilde{x}\rangle\tilde{x}\tilde{x}^{T}\right] & \preceq\tilde{\kappa}^{k}H^{k}
\end{align*}

In order to use hyperparameters that are universal across devices,
we further define $L=\max_{k}L^{k}$, $\mu=\min_{k}\mu^{k}$, $L_{m}=\max_{k}L_{m}^{k}$
and $\kappa_{m}=\max_{k}\kappa_{m}^{k}$. 

\subsection{FedAvg with Nesterov and MaSS}

The FedAvg algorithm with MaSS follows the updates
\begin{align*}
w_{t+1}^{k} & =\begin{cases}
u_{t}^{k}-\eta_{1}^{k}g_{t,k} & \text{if }t+1\notin\mathcal{I}_{E}\\
\sum_{k=1}^{N}p_{k}\left[u_{t}^{k}-\eta_{1}^{k}g_{t,k}\right] & \text{if }t+1\in\mathcal{I}_{E}
\end{cases}\\
u_{t+1}^{k} & =w_{t+1}^{k}+\gamma^{k}(w_{t+1}^{k}-w_{t}^{k})+\eta_{2}^{k}g_{t,k}
\end{align*}
 where we note that the natural parameter is $w_{t}$, while $u_{t}$
is an auxiliary parameter, which we initialize to be $u_{0}^{k}$,
and 
\begin{align*}
g_{t,k} & :=\nabla F_{k}(u_{t}^{k},\xi_{t}^{k})
\end{align*}
 is the stochastic gradient and 
\begin{align*}
g_{t} & =\sum_{k=1}^{N}p_{k}g_{t,k}
\end{align*}
is the averaged stochastic gradient. When $\eta_{2}^{k}\equiv0$,
this reduces to the FedAvg algorithm with Nesterov updates.

We note that the update can equivalently be written as 
\begin{align*}
v_{t+1}^{k} & =(1-\alpha^{k})v_{t}^{k}+\alpha^{k}w_{t}^{k}-\delta^{k}g_{t,k}\\
w_{t+1}^{k} & =\begin{cases}
u_{t}^{k}-\eta^{k}g_{t,k} & \text{if }t+1\notin\mathcal{I}_{E}\\
\sum_{k=1}^{N}p_{k}\left[u_{t}^{k}-\eta^{k}g_{t,k}\right] & \text{if }t+1\in\mathcal{I}_{E}
\end{cases}\\
u_{t+1}^{k} & =\frac{\alpha^{k}}{1+\alpha^{k}}v_{t+1}^{k}+\frac{1}{1+\alpha^{k}}w_{t+1}^{k}
\end{align*}
 where there is a bijection between the parameters 
\begin{align*}
\frac{1-\alpha^{k}}{1+\alpha^{k}} & =\gamma^{k}\\
\eta^{k} & =\eta_{1}^{k}\\
\frac{\eta^{k}-\alpha^{k}\delta^{k}}{1+\alpha^{k}} & =\eta_{2}^{k}
\end{align*}
 and we further introduce an auxiliary parameter $v_{t}^{k}$, which
is initialized at $v_{0}^{k}$. We also note that when $\delta^{k}=\frac{\eta^{k}}{\alpha^{k}}$,
the update reduces to the Nesterov accelerated SGD. This version of
the FedAvg with MaSS algorithm is used for analyzing the exponential
convergence. 

As before, define the virtual sequences $\overline{w}_{t}=\sum_{k=1}^{N}p_{k}w_{t}^{k}$,
$\overline{v}_{t}=\sum_{k=1}^{N}p_{k}v_{t}^{k}$, $\overline{u}_{t}=\sum_{k=1}^{N}p_{k}u_{t}^{k}$,
and $\overline{g}_{t}=\sum_{k=1}^{N}p_{k}\mathbb{E}g_{t,k}$. We have
$\mathbb{E}g_{t}=\overline{g}_{t}$ and $\overline{w}_{t+1}=\overline{u}_{t}-\eta_{t}g_{t}$,
$\overline{v}_{t+1}=(1-\alpha^{k})\overline{v}_{t}+\alpha^{k}\overline{w}_{t}-\delta^{k}g_{t}$,
and $\overline{u}_{t+1}=\frac{\alpha^{k}}{1+\alpha^{k}}\overline{v}_{t+1}+\frac{1}{1+\alpha^{k}}\overline{w}_{t+1}$. 

For the linear regression problem in the interpolation setting, we
can write 
\begin{align*}
F(w) & =\frac{1}{2}(w-w^{\ast})^{T}H(w-w^{\ast})\\
 & =\frac{1}{2}\|w-w^{\ast}\|_{H}^{2}
\end{align*}
 and similarly $F^{k}(w)=\frac{1}{2}\|w-w^{\ast}\|_{H^{k}}^{2}$,
so that 
\begin{align*}
g_{t,k} & =\tilde{H}_{t}^{k}(w_{t}^{k}-w^{\ast})\\
g_{t} & =\sum_{k=1}^{N}p_{k}\tilde{H}_{t}^{k}(w_{t}^{k}-w^{\ast})
\end{align*}


\subsection{Exponential Convergence of FedAvg with SGD and Nesterov}

\subsection{Exponential Convergence of FedAvg with MaSS}

We now present the exponential convergence result in linear regression
using FedAvg with MaSS updates. On each device, local data is stored
and mini-batch gradient descent with batch size $m$ is performed.
We assume that the batch size is the same across devices. 
\begin{theorem}
(FedAvg with MaSS, Linear Regression) Let $\tilde{\kappa}_{m}:=\tilde{\kappa}/m+(m-1)/m$,
and let the hyperparameters satisfy 
\begin{align*}
\eta^{k}(m)=\frac{1}{L_{m}}, & \alpha^{k}(m)=\frac{1}{\sqrt{\kappa_{m}\tilde{\kappa}_{m}}},\delta^{k}(m)=\frac{\eta^{k}}{\alpha^{k}\tilde{\kappa}_{m}}
\end{align*}
for all $k$. Let $\mathcal{I}_{E}=\{\ell E\mid\ell\in\mathbb{N}\}$
be the set of communication rounds, then the FedAvg algorithm with
MaSS and full participation solves the problem 
\begin{align*}
F(w) & =\sum_{k=1}^{N}p_{k}F_{k}(w)\\
F_{k}(w) & =\frac{1}{2n_{k}}\sum_{j=1}^{n_{k}}(w^{T}x_{k,j}-z_{k,j})^{2}
\end{align*}
with exponential convergence
\begin{align*}
\mathbb{E}F(\overline{w}_{t})\leq\frac{L}{2}\mathbb{E}\|\overline{w}_{t}-w^{\ast}\|^{2} & \leq C\cdot(1-\frac{1}{\sqrt{\kappa_{m}\tilde{\kappa}_{m}}})^{t}=O(\exp(-\frac{t}{\sqrt{\kappa_{m}\tilde{\kappa}_{m}}}))
\end{align*}
 where $w^{\ast}$ is such that $F(w^{\ast})=0$ and $C=\frac{L}{2}\cdot\frac{\alpha}{\delta}\sum_{k=1}^{N}p_{k}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})$.
With partial participation of $K$ devices each communication round,
the convergence rate is also 
\begin{align*}
\mathbb{E}F(\overline{w}_{t})\leq\frac{L}{2}\mathbb{E}\|\overline{w}_{t}-w^{\ast}\|^{2} & \leq C\cdot(1+\frac{4}{K}\cdot(\frac{t}{E}-1))\cdot(1-\frac{1}{\sqrt{\kappa_{m}\tilde{\kappa}_{m}}})^{t}=O(\exp(-\frac{t}{\sqrt{\kappa_{m}\tilde{\kappa}_{m}}}))
\end{align*}
\end{theorem}
\begin{proof}
We first proof the convergence for full device participation. Note
that at each communication round we update the $w_{t+1}^{k}$ parameters
to be the average across devices while fixing $v_{t+1}^{k}$. This
automatically adjusts the $u_{t+1}^{k}$ parameter at each device
by the relation 
\begin{align*}
u_{t+1}^{k} & =\frac{\alpha^{k}}{1+\alpha^{k}}v_{t+1}^{k}+\frac{1}{1+\alpha^{k}}w_{t+1}^{k}
\end{align*}
 valid for all $t\geq0$. Note also that the hyperparameters are chosen
the same for all devices: $\delta_{m}\equiv\delta$, $\alpha_{m}\equiv\alpha$,
and $\eta_{m}\equiv\eta$. 

Theorems 2 and 3 of the Liu\&Belkin paper give the bound 
\begin{align*}
\mathbb{E}\left[\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|u_{E}^{k}-\eta g_{E,k}-w^{\ast}\|^{2}\right] & \leq(1-\alpha)^{E}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}
for all $k$, where $E$ is the first communication round. Note that
$w_{E}^{k}=\overline{w}_{E}^{k}\neq u_{E}^{k}-\eta g_{E,k}$.

It follows from convexity that 
\begin{align*}
\mathbb{E}\left[\sum_{k=1}^{N}p_{k}\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{E}-w^{\ast}\|^{2}\right] & \leq\sum_{k=1}^{N}p_{k}\mathbb{E}\left[\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|u_{E}^{k}-\eta g_{E,k}-w^{\ast}\|^{2}\right]\\
 & \leq\sum_{k=1}^{N}p_{k}(1-\alpha)^{E}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}
Since $w_{E}^{k}=\overline{w}_{E}$ for all devices, applying the
per-device result again starting at $t=E$ instead of $t=0$, for
each device we have the bound
\begin{align*}
\mathbb{E}\left[\|v_{2E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|u_{2E}^{k}-\eta g_{2E,k}-w^{\ast}\|^{2}\right] & \leq(1-\alpha)^{E}\mathbb{E}(\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{E}^{k}-w^{\ast}\|^{2})\\
 & =(1-\alpha)^{E}\mathbb{E}(\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{E}-w^{\ast}\|^{2})
\end{align*}

Here we emphasize that $w_{E}^{k}$ results from broadcasting and
so is the same across all devices, while $v_{E}^{k}$ remains distinct
on each device (and is only auxiliary). Then by convexity and summing
the above inequalities across devices we have 
\begin{align*}
\mathbb{E}\left[\sum_{k=1}^{N}p_{k}\|v_{2E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{2E}-w^{\ast}\|^{2}\right] & \leq\sum_{k=1}^{N}p_{k}\mathbb{E}\left[\|v_{2E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|u_{2E}^{k}-\eta g_{2E,k}-w^{\ast}\|^{2}\right]\\
 & \leq\sum_{k=1}^{N}p_{k}(1-\alpha)^{E}\mathbb{E}(\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{E}-w^{\ast}\|^{2})\\
 & \leq\sum_{k=1}^{N}p_{k}(1-\alpha)^{2E}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}
 and by induction we can show that 
\begin{align*}
\mathbb{E}\left[\sum_{k=1}^{N}p_{k}\|v_{\ell E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{\ell E}-w^{\ast}\|^{2}\right] & \leq\sum_{k=1}^{N}p_{k}(1-\alpha)^{\ell E}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}
 and more generally
\begin{align*}
\mathbb{E}\left[\sum_{k=1}^{N}p_{k}\|v_{t}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{t}-w^{\ast}\|^{2}\right] & \leq\sum_{k=1}^{N}p_{k}(1-\alpha)^{t}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}
 In particular, this implies 
\begin{align*}
\mathbb{E}\|\overline{w}_{t}-w^{\ast}\|^{2} & \leq C\cdot(1-\frac{1}{\sqrt{\kappa_{m}\tilde{\kappa}_{m}}})^{t}=O(\exp(-\frac{t}{\sqrt{\kappa_{m}\tilde{\kappa}_{m}}}))
\end{align*}

Now we prove the result for partial device participation. Note that
now 
\begin{align*}
\mathbb{E}\left[\|v_{2E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|u_{2E}^{k}-\eta g_{2E,k}-w^{\ast}\|^{2}\right] & \leq(1-\alpha)^{E}\mathbb{E}(\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{E}^{k}-w^{\ast}\|^{2})\\
 & =(1-\alpha)^{E}\mathbb{E}(\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\frac{1}{K}\sum_{j}\tilde{w}_{E,j}-w^{\ast}\|^{2})
\end{align*}
 where $\tilde{w}_{E,j}$'s are i.i.d. drawn from the discrete distribution
on $w_{E}^{k}$ with probability $p_{k}$. We have 
\begin{align*}
\mathbb{E}\|\frac{1}{K}\sum_{j}\tilde{w}_{E,j}-w^{\ast}\|^{2} & =\mathbb{E}\|\frac{1}{K}\sum_{j}\tilde{w}_{E,j}-\overline{w}_{E}\|^{2}+\|\overline{w}_{E}-w^{\ast}\|^{2}
\end{align*}
 since $\mathbb{E}_{E}\tilde{w}_{E,j}=\overline{w}_{E}$. Now 
\begin{align*}
\mathbb{E}\|\frac{1}{K}\sum_{j}\tilde{w}_{E,j}-\overline{w}_{E}\|^{2} & =\mathbb{E}\frac{1}{K}\sum_{k=1}^{N}p_{k}\|u_{E}^{k}-\eta g_{E,k}-\overline{w}_{E}\|^{2}
\end{align*}
 Since 
\begin{align*}
\mathbb{E}\|u_{E}^{k}-\eta g_{E,k}-\overline{w}_{E}\|^{2} & \leq\mathbb{E}\|u_{E}^{k}-\eta g_{E,k}-w^{\ast}+w^{\ast}-\overline{w}_{E}\|^{2}\\
 & \leq2\mathbb{E}\|u_{E}^{k}-\eta g_{E,k}-w^{\ast}\|+\sum_{k}p_{k}\|w^{\ast}-u_{E}^{k}-\eta g_{E,k}\|^{2}
\end{align*}
 we have 
\begin{align*}
\mathbb{E}\|\frac{1}{K}\sum_{j}\tilde{w}_{E,j}-\overline{w}_{E}\|^{2} & \leq\frac{4}{K}\mathbb{E}\sum_{k=1}^{N}p_{k}\|w^{\ast}-u_{E}^{k}-\eta g_{E,k}\|^{2}\\
 & \leq\frac{4}{K}\mathbb{E}\sum_{k=1}^{N}p_{k}\|w^{\ast}-u_{E}^{k}-\eta g_{E,k}\|^{2}\\
 & \le\frac{4}{K}(1-\alpha)^{E}\frac{\alpha}{\delta}\sum_{k=1}^{N}p_{k}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}
where we have used
\begin{align*}
\mathbb{E}\left[\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|u_{E}^{k}-\eta g_{E,k}-w^{\ast}\|^{2}\right] & \leq(1-\alpha)^{E}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}
Thus 
\begin{align*}
\mathbb{E}\left[\|v_{2E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|u_{2E}^{k}-\eta g_{2E,k}-w^{\ast}\|^{2}\right] & \leq(1-\alpha)^{E}\mathbb{E}(\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{E}^{k}-w^{\ast}\|^{2})\\
 & =(1-\alpha)^{E}\mathbb{E}(\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\frac{1}{K}\sum_{j}\tilde{w}_{E,j}-w^{\ast}\|^{2})\\
 & \leq(1-\alpha)^{E}\mathbb{E}(\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{E}-w^{\ast}\|^{2})+\frac{4}{K}(1-\alpha)^{2E}\sum_{k=1}^{N}p_{k}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}
 

Summing over devices 
\begin{align*}
\mathbb{E}\left[\sum_{k=1}^{N}p_{k}\|v_{2E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{2E}-w^{\ast}\|^{2}\right] & \leq\sum_{k=1}^{N}p_{k}\mathbb{E}\left[\|v_{2E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|u_{2E}^{k}-\eta g_{2E,k}-w^{\ast}\|^{2}\right]\\
 & \leq\sum_{k=1}^{N}p_{k}(1-\alpha)^{E}\mathbb{E}(\|v_{E}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{E}-w^{\ast}\|^{2})+\frac{4}{K}(1-\alpha)^{2E}\sum_{k=1}^{N}p_{k}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})\\
 & \leq\sum_{k=1}^{N}p_{k}(1-\alpha)^{2E}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})+\frac{4}{K}(1-\alpha)^{2E}\sum_{k=1}^{N}p_{k}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})\\
 & =(1+\frac{4}{K})\cdot(1-\alpha)^{2E}\sum_{k=1}^{N}p_{k}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}

By induction, we can show in general 
\begin{align*}
\mathbb{E}\left[\sum_{k=1}^{N}p_{k}\|v_{t}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|\overline{w}_{t}-w^{\ast}\|^{2}\right] & \leq(1+\frac{4}{K}\cdot(\frac{t}{E}-1))\cdot(1-\alpha)^{t}\sum_{k=1}^{N}p_{k}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{\alpha}\|w_{0}^{k}-w^{\ast}\|^{2})
\end{align*}
\end{proof}
%
A similar result can be extended to a class of strongly convex objectives.
For a general convex function $F_{k}$, define $L_{1}^{k}=\inf\{c\mid\mathbb{E}\|\nabla F_{k}(w,\xi)\|^{2}\leq2c(F_{k}(w)-F_{k}^{\ast}),\forall w\}$.
All other quantities are defined in the same way as before. 
\begin{theorem}
(FedAvg with MaSS, Strongly convex objective) Suppose there exists
a $\frac{1}{L}$-strongly convex and $\frac{1}{\mu}$-smooth non-negative
function $g$ such that $g(w^{\ast})=0$ and $\langle g(x),\nabla f(z)\rangle\geq(1-\epsilon)\langle x-w^{\ast},z-w^{\ast}\rangle$,
$\forall x,z\in\mathbb{R}^{d}$, for some $\epsilon>0$. Let the hyperparameters
satisfy 
\begin{align*}
\eta^{k}(m)=\frac{1}{2L_{m}}, & \alpha^{k}(m)=\frac{1-\epsilon}{2\kappa_{m}},\delta^{k}(m)=\frac{1}{2L_{m}}
\end{align*}
for all $k$. Then the FedAvg algorithm with MaSS and full participation
solves the problem 
\begin{align*}
F(w) & =\sum_{k=1}^{N}p_{k}F_{k}(w)
\end{align*}
with exponential convergence
\begin{align*}
\mathbb{E}F(\overline{w}_{t})\leq\frac{L}{2}\mathbb{E}\|\overline{w}_{t}-w^{\ast}\|^{2} & \leq C\cdot(1-\frac{1-\epsilon}{2\kappa_{m}})^{t}=O(\exp(-\frac{1-\epsilon}{2\kappa_{m}}t))
\end{align*}
 where $w^{\ast}$ is such that $F(w^{\ast})=0$ and $C=\frac{L}{2}\cdot\frac{1}{\frac{\delta}{2\alpha}(1-\epsilon)}\sum_{k=1}^{N}p_{k}(\|v_{0}^{k}-w^{\ast}\|_{H_{k}^{-1}}^{2}+\frac{\delta}{2\alpha}(1-\epsilon)\|w_{0}^{k}-w^{\ast}\|^{2})$.
With partial participation of $K$ devices each communication round,
the convergence rate is also 
\begin{align*}
\mathbb{E}F(\overline{w}_{t})\leq\frac{L}{2}\mathbb{E}\|\overline{w}_{t}-w^{\ast}\|^{2} & \leq C\cdot(1+\frac{4}{K}\cdot(\frac{t}{E}-1))\cdot(1-\frac{1-\epsilon}{2\kappa_{m}})^{t}=O(\exp(-\frac{1-\epsilon}{2\kappa_{m}}t))
\end{align*}
\end{theorem}