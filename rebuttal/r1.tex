{\color{blue}\textbf{Reviewer 1:}} We thank the reviewer for the valuable feedback. Your concerns are addressed as follows: 

\textbf{\textit{1. Improvement compared to [20] and [21].}} Compared to [21], the improvement 
of convergence comes from the improved bound of $2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]$ (as we mentioned in L489, Appendix C.1). Because of this improvement, we are able to cancel square of gradient norm ($\alpha_{t}^{2}\|\ov{g}_{t}\|^{2}$) in the upper bound of A1 (defined in L483, also see explanations in L492) while in [21] this term remains as leading term ($6\eta_t^2L \Gamma$). 

Our work is not an extension of [20], we have discussed that our assumption is
a more relaxed assumption compared to the bounded gradient diversity in [20] (see L44 and Appendix B). 

\textbf{\textit{2. The overparameterized case seem to be out of context here with no experimental results to back them up.}} The overparameterized 
setting is an important setting [29, 30], especially in the era of deep learning
where the neural network can achieve zero training loss. We provide the 
geometric rate with linear speed up in this setting including a general overparameterized setting and overparameterized linear regression. 
Experimentally, we verify the linear speedup results of overparameterized setting in 3rd column, Figure 1 and Figure 2. 

\textbf{\textit{3. The novelty and main results.}}
We would like to emphasize that the techniques used for different settings
to achieve linear speedup are quite different (strongly convex see L489, L492, convex smooth see L553). The main contributions include
the linear speedup results in strongly convex, convex smooth, and overparameterized
setting, for both FedAvg and its accelerated variants (this is the first linear speedup results of accelerated FedAvg). We further propose FedMass,
which is the first algorithm that enjoys an improved convergence rate comparing to
FedAvg. We respectfully disagree that our main contribution is the improvement over [21] as the linear speedup results under other settings are not a trivial extension of the strongly convex case.

