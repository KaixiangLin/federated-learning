{\color{blue}\textbf{Reviewer 1:}} We thank the reviewer for the valuable feedback. Your concerns are addressed as follows: 

\textbf{\textit{1.Difference with [21].}} Compared to [21], the improvement 
of convergence comes from the improved bound of $2\alpha_{t}\sum_{k=1}^{N}p_{k}\left[F_{k}(\vw^{\ast})-F_{k}(\ov{w}_{t})\right]$ (as we mentioned in L489, Appendix C.1). Because of this improvement, we are able to cancel square of gradient norm ($\alpha_{t}^{2}\|\ov{g}_{t}\|^{2}$) in the upper bound of A1 (see L483) while in [21] this term remains as leading term ($6\eta_t^2L \Gamma$).

\textbf{\textit{2. The overparameterized case seem to be out of context here with no experimental results to back them up.}} The overparameterized 
setting is an important setting, especially in the era of deep learning
where the neural network can achieve zero training loss. We provide the 
geometric rate with linear speed up in this setting including a general overparameterized setting and overparameterized linear regression. 
Experimentally, we verify the linear speedup results of overparameterized setting in 3rd column, Figure 1 and Figure 2. 

\textbf{\textit{3. The novelty and main results.}}


