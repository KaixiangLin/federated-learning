% !TEX ROOT=./main.tex
This section provides a unified proof framework for the convergence analysis of FedAvg and its accelerated variants. To facilitate our presentation of the unified framework,
we first outline the FedAvg in Alg~\ref{alg:fedavg} and we will take 
the strongly convex problem as an example to illustrate our analysis. 
The corresponding proof for general convex functions follows the 
same framework.  

% The main roadmap of the convergence proof in this work contain the
% following steps. 
% First, the goal of our analysis is to provide the convergence rate 
% of FedAvg and its accelerated variants, which means the we would like
% to show how does the optimality gap decrease as we conduct more 
% iterations 
First, recall that our goal is to analyze the convergence rate of FedAvg algorithm, i.e., $F(\vw_t) - F^* \leq \cO(1/T)$ in strongly convex 
case. 
To prove this bound, we need to connect the optimality gap with the Frobenius norm
of current model and the optimal model ($\| \vw_t - \vw^*\|^2$). Then, we 
analyze the one step progress of FedAvg. This step rigorously shows how 
much the distance between $\vw_{t+1}$ and $\vw^*$ becomes closer when 
we take one step stochastic gradient descent in FedAvg. More specifically,
we provide an upper bound of $\|\vw_{t+1} - \vw^*\|^2$ using the previous
step $\| \vw_t - \vw^*\|^2$. This will justify a solid theoretical foundation for the step in line 9, Alg~\ref{alg:fedavg}. More importantly, our 
prove also motivate that we should include more clients to accelerate the
convergence as we show $\cO(1/NT)$.  Notice that this is the central step of our analysis, which is the main source of improvement of our results comparing to 
prior arts. 
Using one step progress bound, we can connect $\|\vw_t - \vw^*\|^2$ to 
$\|\vw_0 - \vw^*\|^2$ by induction. Therefore, we have an upper bound
of $\|\vw_t - \vw^*\|^2$ and lastly, we can use the upper bound
of $\|\vw_t - \vw^*\|^2$ to bound the optimality gap. 
For strongly convex problems, we can easily connect the optimality gap
and using L-smooth properties. 

In conclusion, our proof contains two main steps: 1) \textbf{one step progress bound} prove how much progress we have made by taking one step
SGD in FedAvg. 2) \textbf{Iterative proof} bounds the up to date progress using one step progress bound iteratively. 


\begin{algorithm}[h!]\small
\begin{algorithmic}[1]
\STATE \textbf{Server input:} initial model $\vw_0$, initial step size $\alpha_0$, local steps $E$. 
\STATE \textbf{Client input:} 
\FOR {each round $r = 0, 1, ..., R$, where $r = t*E$} 
\STATE  Sample clients $\cS_t \subseteq \{1,...,N\}$
\STATE Broadcast $\vw$ to all clients $k \in \cS_t$
\FOR {each client $k \subseteq \cS_t$}
\STATE initialize local model $\vw_t^k = \vw$
\FOR {$t = r * E + 1, \dots, (r+1)*E$}
\STATE $\vw_{t+1}^{k}  =\vw_{t}^{k}-\alpha_{t}\vg_{t,k}$
\ENDFOR
\ENDFOR
\STATE Average the local models at server end: $\ov{w}_t = \sum_{k\in \cS_t} \vw_t^k$.
\ENDFOR 
\end{algorithmic}
\caption{\textsc{FedAvg}: Federated Averaging}% $S$ = cA3C($\theta, \Theta_v$
\label{alg:fedavg}
\end{algorithm}






