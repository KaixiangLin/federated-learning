% !TEX ROOT=./main.tex


\section{Accelerated Federated Learning}

\subsection{Literature review}

\cite{felbab2019optimization} applies various accleration method
into federated learning and empirically compare the performance, 
while they did not provide any theoretical analysis of convergence.
\cite{liu2019accelerating} discuss momentum gradient descent
in the context of federated learning. They provide
convergence rate $\cO(\frac{1}{T})$ 
for strongly convex, $L$-smooth under gradient
descent algorithm. 
\cite{huo2020faster} provide convergence rate of momentum accelerated 
stochastic gradient descent for non-convex functions. 


\subsection{Framework summarization}

\begin{assumption}
The expected squared norm of stochastic subgradients is uniformly bounded. i.e.,
$\mathbb{E}\|\vg_{t,k}\|^2  \leq G_k^{2}$, for all $k = 1,..., N$ and $t=0, \dots, T-1$.  This also implies $\left\| \mathbb{E}\vg_{t,k}\right\|^2  \leq \mathbb{E}\|\vg_{t,k}\|^2 \leq G_k^2$.
\label{ass:subgrad2}
\end{assumption}

\begin{assumption}[Lipschitz continuous]
	The local function $F_k$ is lipschitz continuous. 
	\label{ass:lipcont}
\end{assumption}

\begin{assumption}[L-smooth]
$F_{1}, \cdots, F_{N}$ are all $L$-smooth: for all  $\mathbf{v}$  and $\mathbf{w}$, $F_{k}(\mathbf{v}) \leq F_{k}(\mathbf{w})+(\mathbf{v}- \\ \mathbf{w})^{T} \nabla F_{k}(\mathbf{w})+\frac{L}{2}\|\mathbf{v}-\mathbf{w}\|_{2}^{2}$.
\label{ass:lsmooth}
\end{assumption}

\begin{assumption}[Polyak-Lojasiewicz (PL)] The global objective functions $f($ ) is differentiable and satisfy the Polyak-Lojasiewicz condition with constant $\mu,$ i.e., $\frac{1}{2}\|\nabla f(\boldsymbol{w})\|_{2}^{2} \geq \mu\left(f(\boldsymbol{w})-f\left(\boldsymbol{w}^{*}\right)\right)$ holds $\forall \boldsymbol{w} \in \mathbb{R}^{d}$ with
$\boldsymbol{w}^{*}$ being the optimal solution of global objective.
\label{ass:pl}
\end{assumption}

\begin{assumption}[Strongly-convex]
$	F_{1}, \cdots, F_{N} \text { are all } \mu \text { -strongly convex: for all v and } \mathbf{w}, F_{k}(\mathbf{v}) \geq F_{k}(\mathbf{w})+(\mathbf{v}-\mathbf{w})^{T} \nabla F_{k}(\mathbf{w})+\frac{\mu}{2}\|\mathbf{v}-\mathbf{w}\|_{2}^{2}$
\label{ass:stroncvx}
\end{assumption}

\begin{assumption}[Bounded Variance]
Let $\xi_{t}^{k}$ be sampled from the k-th device's local data uniformly at random. The variance of stochastic gradients in each device is bounded:
$\mathbb{E}\left\|\nabla F_{k}\left(\mathbf{w}_{t}^{k}, \xi_{t}^{k}\right)-\nabla F_{k}\left(\mathbf{w}_{t}^{k}\right)\right\|^{2} \leq \sigma_{k}^{2}$, for $k = 1,..., N$.
\label{ass:lgradvar}
\end{assumption}

\begin{assumption}[Expected square norm of gradient]
The expected squared norm of stochastic gradients is uniformly bounded. i.e.,
$\mathbb{E}\left\|\nabla F_{k}\left(\mathbf{w}_{t}^{k}, \xi_{t}^{k}\right)\right\|^{2} \leq G^{2}$, for all $k = 1,..., N$ and $t=0, \dots, T-1$.
\label{ass:squaregrad}
\end{assumption}

\begin{assumption}[loss function f]
	Loss function $F$ has lower bound $F_{\inf}$. This assumption is assumed for all the works.  The existence of $F^*$. We didn't add this 
	in the table.
	\label{ass:flb}
\end{assumption}

\begin{assumption}[Bounded Weighted Gradient Diversity]
	The weighted gradient diversity among local objectives is defined as:
$$
\Lambda(\boldsymbol{w}, \boldsymbol{q}) \triangleq \frac{\sum_{k=1}^{N} p_{k}\left\|\nabla f_{k}(\boldsymbol{w})\right\|_{2}^{2}}{\left\|\sum_{k=1}^{N} p_{k} \nabla f_{k}(\boldsymbol{w})\right\|_{2}^{2}} \leq \lambda
$$
\label{ass:bgd}
\end{assumption}

\begin{table}[h!]
\centering
\small
	\begin{tabular}{|c|c|c|c|c|}\hline
		paper         & Noncvx & Cvx & Strongly Cvx\\ \hline
	Accelerated	SGD   & \cite{huo2020faster} A\ref{ass:lsmooth},\ref{ass:lgradvar} &          &             \\\hline
	Accelerated GD    &        &       &     \cite{liu2019accelerating}A\ref{ass:lipcont},\ref{ass:lsmooth}        \\\hline
	SGD    &   \cite{haddadpour2019convergence} A\ref{ass:lsmooth},\ref{ass:lgradvar},(\ref{ass:pl},\ref{ass:bgd}) ; \cite{huo2020faster}A\ref{ass:lsmooth},\ref{ass:lgradvar}  & A\ref{ass:subgrad2}  &   \cite{haddadpour2019convergence} A\ref{ass:lsmooth},\ref{ass:lgradvar},(\ref{ass:bgd}); \cite{li2019convergence} A\ref{ass:lsmooth},\ref{ass:lgradvar},\ref{ass:squaregrad}     \\\hline
	GD     &   \cite{haddadpour2019convergence}A\ref{ass:lsmooth},(\ref{ass:pl},\ref{ass:bgd})    & \cite{khaled2019first} A\ref{ass:lsmooth} &        \cite{haddadpour2019convergence}A\ref{ass:lsmooth},(\ref{ass:pl},\ref{ass:bgd})    \\\hline
	\end{tabular}
	\caption{Summarize of assumptions on federated learning, heterogeneous data. $(\cdot)$ means that the assumption is not necessary.}
	\label{tb:ass2}
\end{table}

\begin{table}[h!]
\centering
\small
	\begin{tabular}{|c|c|c|c|c|}\hline
		paper         & Noncvx & Cvx and Non-smth & Cvx and L-smth & Strongly Cvx and L-smth\\ \hline
	Accelerated	SGD   & \cite{huo2020faster} A\ref{ass:lsmooth},\ref{ass:lgradvar} &    &      &             \\\hline
	Accelerated GD    &        &    &   &     \cite{liu2019accelerating}A\ref{ass:lipcont},\ref{ass:lsmooth}        \\\hline
	SGD    &   \cite{haddadpour2019convergence} A\ref{ass:lsmooth},\ref{ass:lgradvar},(\ref{ass:pl},\ref{ass:bgd}) ; \cite{huo2020faster}A\ref{ass:lsmooth},\ref{ass:lgradvar}  & A\ref{ass:subgrad2}  &  &   \cite{haddadpour2019convergence} A\ref{ass:lsmooth},\ref{ass:lgradvar},(\ref{ass:bgd}); \cite{li2019convergence} A\ref{ass:lsmooth},\ref{ass:lgradvar},\ref{ass:squaregrad}     \\\hline
	GD     &   \cite{haddadpour2019convergence}A\ref{ass:lsmooth},(\ref{ass:pl},\ref{ass:bgd})     &  & \cite{khaled2019first} A\ref{ass:lsmooth} &        \cite{haddadpour2019convergence}A\ref{ass:lsmooth},(\ref{ass:pl},\ref{ass:bgd})    \\\hline
	\end{tabular}
	\caption{Summarize of assumptions on federated learning, heterogeneous data. $(\cdot)$ means that the assumption is not necessary.}
	\label{tb:ass}
\end{table}
\vspace{-1em}
\begin{table}[h!]
\centering
\small
	\begin{tabular}{|c|c|c|c|c|}\hline
		paper         & Noncvx & Cvx and Non-smth & Cvx and L-smth & Strongly Cvx and L-smth\\ \hline
	Accelerated	SGD   & \cite{huo2020faster} &        &    &             \\\hline
	Accelerated GD    &        &    &    &    \cite{liu2019accelerating}        \\\hline
	SGD    &   \cite{haddadpour2019convergence,huo2020faster}    & Ours &     &    \cite{li2019convergence}       \\\hline
	GD     &   \cite{haddadpour2019convergence}     &  & \cite{khaled2019first}     &         \\\hline
	\end{tabular}
	\caption{Summarize of relatedwork on federated learning, heterogeneous data.}
\end{table}


Note for table~\ref{tb:ass}:
\begin{itemize}
	\item \textbf{GD}: \cite{haddadpour2019convergence} the proof of Noncvx can be used directly for Strongly cvx and L-smooth.
	\item \textbf{SGD}: necessary assumption: Bounded gradient variance assumption. 
	\item \textbf{Non Cvx}: necessary assumption:  L-smooth.
	\item \textbf{Convex}:
\end{itemize}


\textbf{GD}
The proof of convergence rate of local GD in~\cite{haddadpour2019convergence} 
is based A~\ref{ass:lsmooth} and A~\ref{ass:pl}, note that PL is a generalization of $\mu$-strongly convexity. $\mu$-strongly convexity
indicates A~\ref{ass:pl} is satisfied. Therefore, the same proof in~\cite{haddadpour2019convergence} for non-convex function 
can be adapted for strongly convex and $L$-smooth function. 




\begin{itemize}
	\item Online Meta learning~\cite{finn2019online} strongly convex case convergence rate~\cite{lin2020collaborative}. 
	\item FL + bandit? 
	\item FL + TD convergence rate.
\end{itemize}